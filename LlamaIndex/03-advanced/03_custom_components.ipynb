{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Custom Components\n",
    "\n",
    "LlamaIndex is highly extensible. This notebook shows how to create custom LLMs, embeddings, retrievers, and other components.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Create custom LLM wrappers\n",
    "2. Build custom embedding models\n",
    "3. Implement custom node parsers\n",
    "4. Design custom response synthesizers\n",
    "5. Create custom callbacks for monitoring\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from typing import Any, List, Optional, Sequence\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom LLM Wrapper\n",
    "\n",
    "Create a wrapper around any LLM API or local model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from pydantic import Field\n",
    "\n",
    "class MockLLM(CustomLLM):\n",
    "    \"\"\"A mock LLM for testing and demonstration.\"\"\"\n",
    "    \n",
    "    model_name: str = Field(default=\"mock-model\")\n",
    "    response_prefix: str = Field(default=\"Mock response: \")\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return LLMMetadata(\n",
    "            context_window=4096,\n",
    "            num_output=256,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        \"\"\"Generate a completion.\"\"\"\n",
    "        # In real implementation, call your LLM API here\n",
    "        response_text = f\"{self.response_prefix}Based on your prompt about '{prompt[:50]}...', here is my response.\"\n",
    "        return CompletionResponse(text=response_text)\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        \"\"\"Stream a completion.\"\"\"\n",
    "        response_text = f\"{self.response_prefix}Streaming response for: {prompt[:30]}...\"\n",
    "        \n",
    "        # Simulate streaming\n",
    "        for word in response_text.split():\n",
    "            yield CompletionResponse(text=word + \" \", delta=word + \" \")\n",
    "\n",
    "print(\"✓ MockLLM defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the mock LLM\n",
    "mock_llm = MockLLM(response_prefix=\"[MOCK] \")\n",
    "\n",
    "print(f\"Model: {mock_llm.metadata.model_name}\")\n",
    "print(f\"Context window: {mock_llm.metadata.context_window}\")\n",
    "\n",
    "# Test completion\n",
    "response = mock_llm.complete(\"What is machine learning?\")\n",
    "print(f\"\\nCompletion: {response.text}\")\n",
    "\n",
    "# Test streaming\n",
    "print(\"\\nStreaming: \", end=\"\")\n",
    "for chunk in mock_llm.stream_complete(\"Explain AI\"):\n",
    "    print(chunk.delta, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Custom LLM Example (with Caching)\n",
    "\n",
    "Here's a more practical example - an LLM wrapper with caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "\n",
    "class CachedLLM(CustomLLM):\n",
    "    \"\"\"LLM wrapper with response caching.\"\"\"\n",
    "    \n",
    "    base_llm: Any = Field(default=None)\n",
    "    cache_size: int = Field(default=100)\n",
    "    _cache: OrderedDict = None\n",
    "    _cache_hits: int = 0\n",
    "    _cache_misses: int = 0\n",
    "    \n",
    "    def __init__(self, base_llm: Any, cache_size: int = 100, **kwargs):\n",
    "        super().__init__(base_llm=base_llm, cache_size=cache_size, **kwargs)\n",
    "        self._cache = OrderedDict()\n",
    "        self._cache_hits = 0\n",
    "        self._cache_misses = 0\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return self.base_llm.metadata\n",
    "    \n",
    "    def _get_cache_key(self, prompt: str) -> str:\n",
    "        return hashlib.md5(prompt.encode()).hexdigest()\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        cache_key = self._get_cache_key(prompt)\n",
    "        \n",
    "        # Check cache\n",
    "        if cache_key in self._cache:\n",
    "            self._cache_hits += 1\n",
    "            # Move to end (LRU)\n",
    "            self._cache.move_to_end(cache_key)\n",
    "            return CompletionResponse(text=self._cache[cache_key])\n",
    "        \n",
    "        # Cache miss - call base LLM\n",
    "        self._cache_misses += 1\n",
    "        response = self.base_llm.complete(prompt, **kwargs)\n",
    "        \n",
    "        # Store in cache\n",
    "        self._cache[cache_key] = response.text\n",
    "        \n",
    "        # Evict if necessary\n",
    "        while len(self._cache) > self.cache_size:\n",
    "            self._cache.popitem(last=False)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        # Streaming doesn't use cache\n",
    "        return self.base_llm.stream_complete(prompt, **kwargs)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        total = self._cache_hits + self._cache_misses\n",
    "        return {\n",
    "            \"cache_hits\": self._cache_hits,\n",
    "            \"cache_misses\": self._cache_misses,\n",
    "            \"hit_rate\": self._cache_hits / total if total > 0 else 0,\n",
    "            \"cache_size\": len(self._cache),\n",
    "        }\n",
    "\n",
    "print(\"✓ CachedLLM defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cached LLM\n",
    "base_llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "cached_llm = CachedLLM(base_llm=base_llm, cache_size=50)\n",
    "\n",
    "# First call (cache miss)\n",
    "print(\"First call (should be cache miss)...\")\n",
    "response1 = cached_llm.complete(\"What is Python?\")\n",
    "print(f\"Stats: {cached_llm.get_stats()}\")\n",
    "\n",
    "# Second call with same prompt (cache hit)\n",
    "print(\"\\nSecond call with same prompt (should be cache hit)...\")\n",
    "response2 = cached_llm.complete(\"What is Python?\")\n",
    "print(f\"Stats: {cached_llm.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Embedding Model\n",
    "\n",
    "Create custom embeddings for specialized use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "import numpy as np\n",
    "\n",
    "class SimpleHashEmbedding(BaseEmbedding):\n",
    "    \"\"\"A simple hash-based embedding for demonstration.\n",
    "    \n",
    "    In practice, you would wrap a real embedding model here.\n",
    "    \"\"\"\n",
    "    \n",
    "    embed_dim: int = Field(default=128)\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 128, **kwargs):\n",
    "        super().__init__(embed_dim=embed_dim, **kwargs)\n",
    "    \n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding for a single text.\"\"\"\n",
    "        # Simple hash-based embedding (for demonstration only!)\n",
    "        # In practice, use a real embedding model\n",
    "        np.random.seed(hash(text) % (2**32))\n",
    "        embedding = np.random.randn(self.embed_dim).tolist()\n",
    "        # Normalize\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        return [x / norm for x in embedding]\n",
    "    \n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        \"\"\"Get embedding for a query.\"\"\"\n",
    "        return self._get_text_embedding(query)\n",
    "    \n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        \"\"\"Async query embedding.\"\"\"\n",
    "        return self._get_query_embedding(query)\n",
    "    \n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Async text embedding.\"\"\"\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "print(\"✓ SimpleHashEmbedding defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom embedding\n",
    "custom_embed = SimpleHashEmbedding(embed_dim=64)\n",
    "\n",
    "text1 = \"Machine learning is fascinating\"\n",
    "text2 = \"Deep learning uses neural networks\"\n",
    "\n",
    "emb1 = custom_embed.get_text_embedding(text1)\n",
    "emb2 = custom_embed.get_text_embedding(text2)\n",
    "\n",
    "print(f\"Embedding dimension: {len(emb1)}\")\n",
    "print(f\"Embedding 1 (first 5): {emb1[:5]}\")\n",
    "print(f\"Embedding 2 (first 5): {emb2[:5]}\")\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = np.dot(emb1, emb2)\n",
    "print(f\"\\nCosine similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Node Parser\n",
    "\n",
    "Create custom chunking strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import NodeParser\n",
    "from llama_index.core.schema import Document, TextNode, BaseNode\n",
    "import re\n",
    "\n",
    "class MarkdownHeaderParser(NodeParser):\n",
    "    \"\"\"Parse documents by markdown headers.\"\"\"\n",
    "    \n",
    "    min_chunk_size: int = Field(default=50)\n",
    "    \n",
    "    def __init__(self, min_chunk_size: int = 50, **kwargs):\n",
    "        super().__init__(min_chunk_size=min_chunk_size, **kwargs)\n",
    "    \n",
    "    def _parse_nodes(\n",
    "        self,\n",
    "        nodes: Sequence[BaseNode],\n",
    "        show_progress: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Parse nodes by markdown headers.\"\"\"\n",
    "        all_nodes = []\n",
    "        \n",
    "        for node in nodes:\n",
    "            if isinstance(node, TextNode):\n",
    "                text = node.text\n",
    "                parsed = self._split_by_headers(text, node.metadata)\n",
    "                all_nodes.extend(parsed)\n",
    "            else:\n",
    "                all_nodes.append(node)\n",
    "        \n",
    "        return all_nodes\n",
    "    \n",
    "    def _split_by_headers(self, text: str, metadata: dict) -> List[TextNode]:\n",
    "        \"\"\"Split text by markdown headers.\"\"\"\n",
    "        # Find all headers\n",
    "        header_pattern = r'^(#{1,6})\\s+(.+)$'\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        current_section = []\n",
    "        current_header = None\n",
    "        current_level = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            match = re.match(header_pattern, line)\n",
    "            if match:\n",
    "                # Save previous section\n",
    "                if current_section:\n",
    "                    section_text = '\\n'.join(current_section)\n",
    "                    if len(section_text.strip()) >= self.min_chunk_size:\n",
    "                        sections.append({\n",
    "                            'header': current_header,\n",
    "                            'level': current_level,\n",
    "                            'text': section_text,\n",
    "                        })\n",
    "                \n",
    "                # Start new section\n",
    "                current_level = len(match.group(1))\n",
    "                current_header = match.group(2)\n",
    "                current_section = [line]\n",
    "            else:\n",
    "                current_section.append(line)\n",
    "        \n",
    "        # Don't forget last section\n",
    "        if current_section:\n",
    "            section_text = '\\n'.join(current_section)\n",
    "            if len(section_text.strip()) >= self.min_chunk_size:\n",
    "                sections.append({\n",
    "                    'header': current_header or 'Introduction',\n",
    "                    'level': current_level,\n",
    "                    'text': section_text,\n",
    "                })\n",
    "        \n",
    "        # Create nodes\n",
    "        nodes = []\n",
    "        for section in sections:\n",
    "            node_metadata = {\n",
    "                **metadata,\n",
    "                'section_header': section['header'],\n",
    "                'header_level': section['level'],\n",
    "            }\n",
    "            nodes.append(TextNode(\n",
    "                text=section['text'],\n",
    "                metadata=node_metadata,\n",
    "            ))\n",
    "        \n",
    "        return nodes if nodes else [TextNode(text=text, metadata=metadata)]\n",
    "\n",
    "print(\"✓ MarkdownHeaderParser defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom parser\n",
    "test_markdown = \"\"\"\n",
    "# Introduction\n",
    "\n",
    "This document explains machine learning concepts.\n",
    "It covers the basics and some advanced topics.\n",
    "\n",
    "## Types of Learning\n",
    "\n",
    "There are several types of machine learning:\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Reinforcement learning\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "Supervised learning uses labeled data to train models.\n",
    "Examples include classification and regression.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Unsupervised learning finds patterns in unlabeled data.\n",
    "Common techniques include clustering and dimensionality reduction.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Machine learning is a powerful tool for data analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Create document and parse\n",
    "doc = Document(text=test_markdown, metadata={\"source\": \"test\"})\n",
    "parser = MarkdownHeaderParser(min_chunk_size=30)\n",
    "\n",
    "nodes = parser.get_nodes_from_documents([doc])\n",
    "\n",
    "print(f\"Parsed into {len(nodes)} sections:\\n\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"Section {i+1}:\")\n",
    "    print(f\"  Header: {node.metadata.get('section_header', 'N/A')}\")\n",
    "    print(f\"  Level: {node.metadata.get('header_level', 0)}\")\n",
    "    print(f\"  Text preview: {node.text[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Callbacks for Monitoring\n",
    "\n",
    "Create callbacks to monitor and log operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.callbacks import CallbackManager, CBEventType, LlamaDebugHandler\n",
    "from llama_index.core.callbacks.base_handler import BaseCallbackHandler\n",
    "from typing import Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "class PerformanceCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Track performance metrics for LlamaIndex operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            event_starts_to_ignore=[],\n",
    "            event_ends_to_ignore=[],\n",
    "        )\n",
    "        self.event_times: Dict[str, list] = {}\n",
    "        self._start_times: Dict[str, float] = {}\n",
    "    \n",
    "    def on_event_start(\n",
    "        self,\n",
    "        event_type: CBEventType,\n",
    "        payload: Optional[Dict[str, Any]] = None,\n",
    "        event_id: str = \"\",\n",
    "        parent_id: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        self._start_times[event_id] = time.time()\n",
    "        return event_id\n",
    "    \n",
    "    def on_event_end(\n",
    "        self,\n",
    "        event_type: CBEventType,\n",
    "        payload: Optional[Dict[str, Any]] = None,\n",
    "        event_id: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if event_id in self._start_times:\n",
    "            elapsed = time.time() - self._start_times[event_id]\n",
    "            event_name = event_type.value\n",
    "            \n",
    "            if event_name not in self.event_times:\n",
    "                self.event_times[event_name] = []\n",
    "            self.event_times[event_name].append(elapsed)\n",
    "            \n",
    "            del self._start_times[event_id]\n",
    "    \n",
    "    def start_trace(self, trace_id: Optional[str] = None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def end_trace(\n",
    "        self,\n",
    "        trace_id: Optional[str] = None,\n",
    "        trace_map: Optional[Dict[str, List[str]]] = None,\n",
    "    ) -> None:\n",
    "        pass\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get statistics for all event types.\"\"\"\n",
    "        stats = {}\n",
    "        for event_name, times in self.event_times.items():\n",
    "            if times:\n",
    "                stats[event_name] = {\n",
    "                    \"count\": len(times),\n",
    "                    \"total_time\": sum(times),\n",
    "                    \"avg_time\": sum(times) / len(times),\n",
    "                    \"min_time\": min(times),\n",
    "                    \"max_time\": max(times),\n",
    "                }\n",
    "        return stats\n",
    "\n",
    "print(\"✓ PerformanceCallbackHandler defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with callback\n",
    "perf_handler = PerformanceCallbackHandler()\n",
    "callback_manager = CallbackManager([perf_handler])\n",
    "\n",
    "# Configure settings with callback\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.callback_manager = callback_manager\n",
    "\n",
    "# Load documents and create index with monitoring\n",
    "documents = SimpleDirectoryReader(\"../data/sample_docs\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "print(\"\\n✓ Index created with monitoring!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some queries to gather metrics\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks.\",\n",
    "    \"How does Python work?\",\n",
    "]\n",
    "\n",
    "print(\"Running queries...\")\n",
    "for q in queries:\n",
    "    response = query_engine.query(q)\n",
    "    print(f\"Q: {q[:30]}... ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View performance statistics\n",
    "print(\"\\n=== Performance Statistics ===\")\n",
    "stats = perf_handler.get_stats()\n",
    "\n",
    "for event_type, metrics in stats.items():\n",
    "    print(f\"\\n{event_type}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.4f}s\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Prompt Templates\n",
    "\n",
    "Create and use custom prompt templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.prompts import PromptType\n",
    "\n",
    "# Custom QA prompt with structured output\n",
    "STRUCTURED_QA_PROMPT = PromptTemplate(\n",
    "    \"\"\"You are an expert assistant. Answer the question using the context provided.\n",
    "\n",
    "CONTEXT:\n",
    "{context_str}\n",
    "\n",
    "QUESTION: {query_str}\n",
    "\n",
    "Provide your answer in the following format:\n",
    "\n",
    "ANSWER: [Your direct answer]\n",
    "\n",
    "CONFIDENCE: [High/Medium/Low]\n",
    "\n",
    "REASONING: [Brief explanation of why you gave this answer]\n",
    "\n",
    "SOURCES: [Which parts of the context you used]\n",
    "\"\"\",\n",
    "    prompt_type=PromptType.QUESTION_ANSWER,\n",
    ")\n",
    "\n",
    "print(\"✓ Custom prompt template defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom prompt\n",
    "query_engine_custom = index.as_query_engine(\n",
    "    text_qa_template=STRUCTURED_QA_PROMPT,\n",
    ")\n",
    "\n",
    "response = query_engine_custom.query(\"What is supervised learning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "You've learned how to create custom components in LlamaIndex:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Component | Base Class | Key Methods |\n",
    "|-----------|------------|-------------|\n",
    "| **Custom LLM** | `CustomLLM` | `complete()`, `stream_complete()` |\n",
    "| **Custom Embedding** | `BaseEmbedding` | `_get_text_embedding()` |\n",
    "| **Custom Parser** | `NodeParser` | `_parse_nodes()` |\n",
    "| **Custom Callback** | `BaseCallbackHandler` | `on_event_start()`, `on_event_end()` |\n",
    "\n",
    "### When to Build Custom Components\n",
    "\n",
    "1. **Custom LLM**: Wrap proprietary APIs, add caching/logging\n",
    "2. **Custom Embedding**: Use specialized embedding models\n",
    "3. **Custom Parser**: Domain-specific document structures\n",
    "4. **Custom Callbacks**: Monitoring, debugging, analytics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the Specialty section, we'll explore multimodal RAG, GraphRAG, and LlamaCloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Rate-limited LLM**: Create an LLM wrapper with rate limiting\n",
    "\n",
    "2. **Semantic chunker**: Build a parser that uses embeddings to find semantic boundaries\n",
    "\n",
    "3. **Cost tracker**: Create a callback that estimates API costs\n",
    "\n",
    "4. **Hybrid embedding**: Combine multiple embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Build your own custom components here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
