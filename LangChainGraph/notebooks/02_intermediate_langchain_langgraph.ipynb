{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2: Intermediate - LCEL Deep Dive & LangGraph Introduction\n",
        "\n",
        "This notebook takes you deeper into LangChain Expression Language (LCEL) and introduces LangGraph for building stateful agent workflows.\n",
        "\n",
        "## Learning Objectives\n",
        "- Master LCEL: RunnablePassthrough, RunnableParallel, RunnableLambda, branching\n",
        "- Understand LangGraph fundamentals: StateGraph, nodes, edges\n",
        "- Build your first graph-based agent from scratch\n",
        "- Work with conditional edges and cycles\n",
        "- Learn state management in graphs\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Notebook 01 (Entry Level)\n",
        "\n",
        "---\n",
        "\n",
        "**References:**\n",
        "- [LangGraph Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart)\n",
        "- [LangGraph: Build Stateful AI Agents](https://realpython.com/langgraph-python)\n",
        "- [Graph API Overview](https://langchain-ai.github.io/langgraph/how-tos/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client initialized  -> model: gpt-4o-mini\n",
            "Google client initialized  -> model: gemini-3-flash-preview\n",
            "Using GPT model:    gpt-4o-mini\n",
            "Using Gemini model: gemini-3-flash-preview\n",
            "\n",
            "Available Models:\n",
            "-------------------------------------------------------\n",
            "  gpt-4o-mini          -> ChatOpenAI(gpt-4o-mini)\n",
            "  gemini-3-flash-preview -> ChatGoogleGenerativeAI(gemini-3-flash-preview)\n",
            "-------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for shared config\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import global model configuration\n",
        "from config import (\n",
        "    GPT_MODEL, GEMINI_MODEL,\n",
        "    GPT_MODEL_NAME, GEMINI_MODEL_NAME,\n",
        "    get_model, list_available_models,\n",
        ")\n",
        "\n",
        "print(f\"Using GPT model:    {GPT_MODEL_NAME}\")\n",
        "print(f\"Using Gemini model: {GEMINI_MODEL_NAME}\")\n",
        "print()\n",
        "list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LCEL Advanced Patterns\n",
        "\n",
        "LangChain Expression Language (LCEL) provides composable primitives for building complex chains.\n",
        "\n",
        "### Key Runnables\n",
        "| Runnable | Purpose |\n",
        "|----------|---------|\n",
        "| `RunnablePassthrough` | Pass input through unchanged (or add fields) |\n",
        "| `RunnableParallel` | Run multiple chains in parallel |\n",
        "| `RunnableLambda` | Wrap any Python function as a runnable |\n",
        "| `RunnableBranch` | Conditional routing based on input |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary (GPT): Quantum computing is an advanced computational paradigm that leverages the principles of quantum mechanics to process information in fundamentally different ways than classical computers, potentially solving complex problems more efficiently.\n",
            "\n",
            "Keywords (Gemini): Qubit, Superposition, Entanglement, Quantum Supremacy, Quantum Algorithms\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# RunnableParallel: run multiple chains simultaneously\n",
        "prompt_summary = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Summarize the following topic in 1 sentence.\"),\n",
        "    (\"human\", \"{topic}\"),\n",
        "])\n",
        "\n",
        "prompt_keywords = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"List 5 keywords for the following topic, comma-separated.\"),\n",
        "    (\"human\", \"{topic}\"),\n",
        "])\n",
        "\n",
        "# Build parallel chains\n",
        "parallel_chain = RunnableParallel(\n",
        "    summary=prompt_summary | GPT_MODEL | StrOutputParser(),\n",
        "    keywords=prompt_keywords | GEMINI_MODEL | StrOutputParser(),\n",
        ")\n",
        "\n",
        "result = parallel_chain.invoke({\"topic\": \"Quantum Computing\"})\n",
        "print(\"Summary (GPT):\", result[\"summary\"])\n",
        "print(\"\\nKeywords (Gemini):\", result[\"keywords\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explanation: Neural networks are a subset of machine learning models inspired by the structure and function of th...\n",
            "Word count: 84\n",
            "Character count: 561\n"
          ]
        }
      ],
      "source": [
        "# RunnableLambda: wrap any function as a chain step\n",
        "def word_count(text: str) -> dict:\n",
        "    \"\"\"Count words and return enriched result.\"\"\"\n",
        "    words = text.split()\n",
        "    return {\"text\": text, \"word_count\": len(words), \"char_count\": len(text)}\n",
        "\n",
        "analyze = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Explain the concept briefly.\"),\n",
        "        (\"human\", \"{concept}\"),\n",
        "    ])\n",
        "    | GPT_MODEL\n",
        "    | StrOutputParser()\n",
        "    | RunnableLambda(word_count)\n",
        ")\n",
        "\n",
        "result = analyze.invoke({\"concept\": \"neural networks\"})\n",
        "print(f\"Explanation: {result['text'][:100]}...\")\n",
        "print(f\"Word count: {result['word_count']}\")\n",
        "print(f\"Character count: {result['char_count']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enriched input: {'topic': 'artificial intelligence', 'upper_topic': 'ARTIFICIAL INTELLIGENCE', 'topic_length': 23}\n"
          ]
        }
      ],
      "source": [
        "# RunnablePassthrough with assign: add computed fields to the input\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "enrich_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        upper_topic=lambda x: x[\"topic\"].upper(),\n",
        "        topic_length=lambda x: len(x[\"topic\"]),\n",
        "    )\n",
        ")\n",
        "\n",
        "result = enrich_chain.invoke({\"topic\": \"artificial intelligence\"})\n",
        "print(\"Enriched input:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Introduction to LangGraph\n",
        "\n",
        "LangGraph lets you build **stateful, graph-based** agent workflows. Instead of linear chains, you define:\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **State** | A TypedDict that flows through the graph |\n",
        "| **Nodes** | Functions that transform the state |\n",
        "| **Edges** | Connections between nodes (can be conditional) |\n",
        "| **START / END** | Special nodes marking graph entry/exit |\n",
        "\n",
        "### Why LangGraph over plain LCEL?\n",
        "- Supports **cycles** (agents that loop until done)\n",
        "- Built-in **state management**\n",
        "- **Persistence** for multi-turn conversations\n",
        "- **Human-in-the-loop** capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final response: As of my last knowledge update in October 2023, LangGraph is not a widely recognized term or concept in mainstream technology or academia. However, it could refer to a specific project, tool, or framework related to language processing, graph theory, or a combination of both. \n",
            "\n",
            "If LangGraph is a recent development or a niche project that emerged after my last update, I would recommend checking the latest resources, official documentation, or community discussions for the most accurate and up-to-date information. If you have more context or details about LangGraph, I would be happy to help clarify or provide insights based on that information!\n"
          ]
        }
      ],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Step 1: Define the State\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Step 2: Define nodes (functions that process state)\n",
        "def chatbot(state: ChatState):\n",
        "    \"\"\"The chatbot node - calls the LLM with current messages.\"\"\"\n",
        "    return {\"messages\": [GPT_MODEL.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Step 3: Build the graph\n",
        "graph_builder = StateGraph(ChatState)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Step 4: Compile\n",
        "simple_graph = graph_builder.compile()\n",
        "\n",
        "# Step 5: Invoke\n",
        "result = simple_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is LangGraph?\"}]})\n",
        "print(\"Final response:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Graph with Tools and Conditional Edges\n",
        "\n",
        "Now let's build a more realistic graph where the agent can **decide** whether to call tools or respond directly. This creates a **cycle**: the agent loops until it has enough information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import ToolMessage\n",
        "import json\n",
        "\n",
        "# Define tools\n",
        "@tool\n",
        "def search_knowledge(query: str) -> str:\n",
        "    \"\"\"Search an internal knowledge base for information.\"\"\"\n",
        "    knowledge = {\n",
        "        \"langchain\": \"LangChain is a framework for building LLM-powered applications with chains, agents, and memory.\",\n",
        "        \"langgraph\": \"LangGraph extends LangChain with graph-based state machines for complex agent workflows.\",\n",
        "        \"rag\": \"RAG combines retrieval with generation to ground LLM responses in factual data.\",\n",
        "        \"lcel\": \"LCEL (LangChain Expression Language) uses the pipe operator to compose chain components.\",\n",
        "    }\n",
        "    for key, value in knowledge.items():\n",
        "        if key in query.lower():\n",
        "            return value\n",
        "    return f\"No information found for: {query}\"\n",
        "\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"Evaluate a mathematical expression safely.\"\"\"\n",
        "    try:\n",
        "        result = eval(expression, {\"__builtins__\": {}})\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "tools = [search_knowledge, calculate]\n",
        "tool_map = {t.name: t for t in tools}\n",
        "\n",
        "# Bind tools to model\n",
        "model_with_tools = GPT_MODEL.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent conversation:\n",
            "  [human] What is LCEL? Also calculate 42 * 17.\n",
            "    -> Calling: search_knowledge({'query': 'LCEL'})\n",
            "    -> Calling: calculate({'expression': '42 * 17'})\n",
            "  [tool] LCEL (LangChain Expression Language) uses the pipe operator to compose chain components.\n",
            "  [tool] 714\n",
            "  [ai] LCEL (LangChain Expression Language) uses the pipe operator to compose chain components. \n",
            "\n",
            "Additionally, the result of \\( 42 \\times 17 \\) is 714.\n"
          ]
        }
      ],
      "source": [
        "# Build a full agent graph with conditional routing\n",
        "\n",
        "def agent_node(state: ChatState):\n",
        "    \"\"\"Call the LLM with tools bound.\"\"\"\n",
        "    return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "def tool_node(state: ChatState):\n",
        "    \"\"\"Execute all pending tool calls.\"\"\"\n",
        "    outputs = []\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    for tool_call in last_message.tool_calls:\n",
        "        tool_result = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
        "        outputs.append(\n",
        "            ToolMessage(\n",
        "                content=str(tool_result),\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "                name=tool_call[\"name\"],\n",
        "            )\n",
        "        )\n",
        "    return {\"messages\": outputs}\n",
        "\n",
        "def should_continue(state: ChatState) -> str:\n",
        "    \"\"\"Decide whether to call tools or end.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# Build the graph\n",
        "agent_graph = StateGraph(ChatState)\n",
        "agent_graph.add_node(\"agent\", agent_node)\n",
        "agent_graph.add_node(\"tools\", tool_node)\n",
        "\n",
        "agent_graph.add_edge(START, \"agent\")\n",
        "agent_graph.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "agent_graph.add_edge(\"tools\", \"agent\")  # Loop back after tool execution\n",
        "\n",
        "# Compile\n",
        "compiled_agent = agent_graph.compile()\n",
        "\n",
        "# Test: this will cause the agent to search, then respond\n",
        "result = compiled_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is LCEL? Also calculate 42 * 17.\"}]\n",
        "})\n",
        "\n",
        "print(\"Agent conversation:\")\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
        "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
        "    if content:\n",
        "        print(f\"  [{role}] {content[:150]}\")\n",
        "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"    -> Calling: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Graph with Gemini Model\n",
        "\n",
        "Let's swap the model to Gemini and see how it handles the same graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini agent conversation:\n",
            "  [human] Search for info about RAG, then calculate 100 / 4.\n",
            "    -> Calling: search_knowledge({'query': 'RAG (Retrieval-Augmented Generation)'})\n",
            "  [tool] RAG combines retrieval with generation to ground LLM responses in factual data.\n",
            "    -> Calling: calculate({'expression': '100 / 4'})\n",
            "  [tool] 25.0\n",
            "  [ai] [{'type': 'text', 'text': '**Retrieval-Augmented Generation (RAG)** is a technique used to improve the accuracy and reliability of large language models (LLMs). It works by combining the generative capabilities of a model with a retrieval system that pulls relevant, up-to-date information from external data sources (like documents or databases). This process \"grounds\" the model\\'s responses in factual data, reducing the likelihood of hallucinations.\\n\\nThe result of **100 / 4** is **25**.'}]\n"
          ]
        }
      ],
      "source": [
        "# Rebuild with Gemini\n",
        "gemini_with_tools = GEMINI_MODEL.bind_tools(tools)\n",
        "\n",
        "def gemini_agent_node(state: ChatState):\n",
        "    return {\"messages\": [gemini_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "gemini_graph = StateGraph(ChatState)\n",
        "gemini_graph.add_node(\"agent\", gemini_agent_node)\n",
        "gemini_graph.add_node(\"tools\", tool_node)  # Reuse tool_node from above\n",
        "gemini_graph.add_edge(START, \"agent\")\n",
        "gemini_graph.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "gemini_graph.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "compiled_gemini = gemini_graph.compile()\n",
        "\n",
        "result = compiled_gemini.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for info about RAG, then calculate 100 / 4.\"}]\n",
        "})\n",
        "\n",
        "print(\"Gemini agent conversation:\")\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
        "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
        "    if content:\n",
        "        print(f\"  [{role}] {content[:150]}\")\n",
        "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"    -> Calling: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizing Your Graph\n",
        "\n",
        "LangGraph can generate a visual representation of your graph structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASCII visualization requires 'grandalf' package: Install grandalf to draw graphs: `pip install grandalf`.\n",
            "\n",
            "Graph structure:\n",
            "  START -> agent -> (tools | END)\n",
            "  tools -> agent (loop back)\n"
          ]
        }
      ],
      "source": [
        "# Visualize the graph as ASCII\n",
        "try:\n",
        "    print(compiled_agent.get_graph().draw_ascii())\n",
        "except Exception as e:\n",
        "    print(f\"ASCII visualization requires 'grandalf' package: {e}\")\n",
        "    print(\"\\nGraph structure:\")\n",
        "    print(\"  START -> agent -> (tools | END)\")\n",
        "    print(\"  tools -> agent (loop back)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| RunnableParallel | Run multiple chains simultaneously |\n",
        "| RunnableLambda | Wrap Python functions as chain steps |\n",
        "| RunnablePassthrough | Pass-through and enrich input data |\n",
        "| StateGraph | Define graph-based agent workflows |\n",
        "| Conditional Edges | Route between nodes based on state |\n",
        "| Tool Integration | Bind tools to models in a graph |\n",
        "| Agent Loop | Create cycles for iterative tool use |\n",
        "\n",
        "**Next:** [03 - Advanced: LangGraph Deep Dive](./03_advanced_langgraph.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
