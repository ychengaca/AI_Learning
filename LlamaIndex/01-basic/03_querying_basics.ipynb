{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Basics: Advanced Query Techniques\n",
    "\n",
    "This notebook explores advanced querying capabilities in LlamaIndex including streaming, async operations, query transformations, and response evaluation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Implement streaming responses for better UX\n",
    "2. Use async queries for performance\n",
    "3. Apply query transformations for better retrieval\n",
    "4. Evaluate response quality\n",
    "5. Handle different query types\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Configure\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create index\n",
    "import os\n",
    "\n",
    "PERSIST_DIR = \"./storage/query_basics_index\"\n",
    "\n",
    "if os.path.exists(PERSIST_DIR):\n",
    "    print(\"Loading existing index...\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "else:\n",
    "    print(\"Building new index...\")\n",
    "    documents = SimpleDirectoryReader(\"../data/sample_docs\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "\n",
    "print(\"✓ Index ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streaming Responses\n",
    "\n",
    "Streaming provides a better user experience by showing the response as it's generated, rather than waiting for the complete response.\n",
    "\n",
    "### Why Streaming?\n",
    "- **Perceived speed**: Users see content immediately\n",
    "- **Early feedback**: Can stop if response is going wrong\n",
    "- **Better UX**: Feels more interactive and responsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming query engine\n",
    "streaming_query_engine = index.as_query_engine(\n",
    "    streaming=True,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "# Query with streaming\n",
    "query = \"Explain the different types of machine learning with examples.\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Response (streaming):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "streaming_response = streaming_query_engine.query(query)\n",
    "\n",
    "# Print tokens as they arrive\n",
    "for text in streaming_response.response_gen:\n",
    "    print(text, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"\\n✓ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access source nodes after streaming completes\n",
    "print(\"\\nSource nodes used:\")\n",
    "for i, node in enumerate(streaming_response.source_nodes):\n",
    "    print(f\"  {i+1}. Score: {node.score:.4f}\")\n",
    "    print(f\"     Preview: {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Async Queries\n",
    "\n",
    "Async queries allow you to run multiple queries concurrently, significantly improving throughput for batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Standard query engine\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# Questions to ask\n",
    "questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"What are the main Python data types?\",\n",
    "    \"Explain supervised learning.\",\n",
    "    \"What is deep learning?\",\n",
    "]\n",
    "\n",
    "# Synchronous approach (sequential)\n",
    "print(\"Running queries SEQUENTIALLY...\")\n",
    "start_time = time.time()\n",
    "\n",
    "sync_results = []\n",
    "for q in questions:\n",
    "    response = query_engine.query(q)\n",
    "    sync_results.append(str(response))\n",
    "\n",
    "sync_time = time.time() - start_time\n",
    "print(f\"Sequential time: {sync_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async approach (concurrent)\n",
    "print(\"\\nRunning queries CONCURRENTLY (async)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "async def run_async_queries():\n",
    "    # Create async tasks for all queries\n",
    "    tasks = [query_engine.aquery(q) for q in questions]\n",
    "    # Run all concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "async_results = asyncio.run(run_async_queries())\n",
    "\n",
    "async_time = time.time() - start_time\n",
    "print(f\"Async time: {async_time:.2f} seconds\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = sync_time / async_time\n",
    "print(f\"\\nSpeedup: {speedup:.2f}x faster with async!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display async results\n",
    "print(\"\\nAsync Query Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q, result in zip(questions, async_results):\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {str(result)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Transformations\n",
    "\n",
    "Query transformations can improve retrieval by reformulating the user's query. Common techniques:\n",
    "\n",
    "| Transformation | Description | Use Case |\n",
    "|---------------|-------------|----------|\n",
    "| HyDE | Generate hypothetical answer, use for retrieval | Complex queries |\n",
    "| Query Decomposition | Break into sub-queries | Multi-part questions |\n",
    "| Query Expansion | Add related terms | Improve recall |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "# HyDE: Hypothetical Document Embeddings\n",
    "# Generates a hypothetical answer, then uses that for retrieval\n",
    "\n",
    "hyde_transform = HyDEQueryTransform(include_original=True)\n",
    "base_query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    query_engine=base_query_engine,\n",
    "    query_transform=hyde_transform,\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "query = \"How do neural networks learn patterns in data?\"\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Without HyDE:\")\n",
    "print(\"=\" * 50)\n",
    "base_response = base_query_engine.query(query)\n",
    "print(base_response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"With HyDE:\")\n",
    "print(\"=\" * 50)\n",
    "hyde_response = hyde_query_engine.query(query)\n",
    "print(hyde_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-Back Prompting\n",
    "\n",
    "Another technique is to first ask a more general question, then use that context for the specific query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import StepDecomposeQueryTransform\n",
    "\n",
    "# Step decomposition breaks complex queries into steps\n",
    "step_decompose_transform = StepDecomposeQueryTransform(\n",
    "    llm=Settings.llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# This shows how the query is transformed\n",
    "complex_query = \"How does Python's object-oriented programming relate to AI development?\"\n",
    "\n",
    "transformed = step_decompose_transform.run(complex_query)\n",
    "print(f\"\\nOriginal: {complex_query}\")\n",
    "print(f\"Transformed: {transformed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Different Query Types\n",
    "\n",
    "LlamaIndex handles different types of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for demonstrations\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# Query Type 1: Factual Questions\n",
    "print(\"=\" * 60)\n",
    "print(\"FACTUAL QUERY\")\n",
    "print(\"=\" * 60)\n",
    "factual_q = \"What year was Python created?\"\n",
    "print(f\"Q: {factual_q}\")\n",
    "print(f\"A: {query_engine.query(factual_q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Type 2: Comparative Questions\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARATIVE QUERY\")\n",
    "print(\"=\" * 60)\n",
    "comparative_q = \"What are the differences between supervised and unsupervised learning?\"\n",
    "print(f\"Q: {comparative_q}\")\n",
    "print(f\"A: {query_engine.query(comparative_q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Type 3: Analytical Questions\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALYTICAL QUERY\")\n",
    "print(\"=\" * 60)\n",
    "analytical_q = \"Why is Python popular for machine learning and data science?\"\n",
    "print(f\"Q: {analytical_q}\")\n",
    "print(f\"A: {query_engine.query(analytical_q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Type 4: Synthesis Questions (combining multiple sources)\n",
    "print(\"=\" * 60)\n",
    "print(\"SYNTHESIS QUERY\")\n",
    "print(\"=\" * 60)\n",
    "synthesis_q = \"How can Python and AI be combined to create intelligent applications?\"\n",
    "print(f\"Q: {synthesis_q}\")\n",
    "print(f\"A: {query_engine.query(synthesis_q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Response Evaluation\n",
    "\n",
    "LlamaIndex provides built-in evaluators to assess response quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=Settings.llm)\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=Settings.llm)\n",
    "\n",
    "print(\"✓ Evaluators ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a query and evaluate the response\n",
    "query = \"What are the key principles of object-oriented programming in Python?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nResponse: {response}\\n\")\n",
    "\n",
    "# Evaluate faithfulness (is the response faithful to the source documents?)\n",
    "print(\"Evaluating response quality...\")\n",
    "faithfulness_result = faithfulness_evaluator.evaluate_response(response=response)\n",
    "relevancy_result = relevancy_evaluator.evaluate_response(\n",
    "    query=query,\n",
    "    response=response,\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Faithfulness: {'PASS' if faithfulness_result.passing else 'FAIL'}\")\n",
    "print(f\"  - Score: {faithfulness_result.score}\")\n",
    "print(f\"  - Feedback: {faithfulness_result.feedback}\")\n",
    "print(f\"\\nRelevancy: {'PASS' if relevancy_result.passing else 'FAIL'}\")\n",
    "print(f\"  - Score: {relevancy_result.score}\")\n",
    "print(f\"  - Feedback: {relevancy_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multiple queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does Python handle errors?\",\n",
    "    \"What are neural networks?\",\n",
    "]\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    response = query_engine.query(query)\n",
    "    \n",
    "    faithfulness = faithfulness_evaluator.evaluate_response(response=response)\n",
    "    relevancy = relevancy_evaluator.evaluate_response(query=query, response=response)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        \"query\": query,\n",
    "        \"faithfulness\": faithfulness.passing,\n",
    "        \"relevancy\": relevancy.passing,\n",
    "        \"response_length\": len(str(response)),\n",
    "    })\n",
    "\n",
    "# Display results as table\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(evaluation_results)\n",
    "print(\"\\n=== Batch Evaluation Results ===\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Faithfulness pass rate: {df['faithfulness'].mean()*100:.0f}%\")\n",
    "print(f\"  Relevancy pass rate: {df['relevancy'].mean()*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling Edge Cases\n",
    "\n",
    "What happens when the query can't be answered from the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about something not in our documents\n",
    "out_of_scope_query = \"What is the capital of France?\"\n",
    "\n",
    "response = query_engine.query(out_of_scope_query)\n",
    "\n",
    "print(f\"Query: {out_of_scope_query}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"\\nSource nodes retrieved: {len(response.source_nodes)}\")\n",
    "\n",
    "# The response may still try to answer from unrelated context\n",
    "# This is where custom prompts help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Better handling of out-of-scope queries with custom prompt\n",
    "careful_prompt = PromptTemplate(\n",
    "    \"\"\"You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "If the context does not contain information to answer the question, respond with:\n",
    "\"I cannot answer this question based on the available documents.\"\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "careful_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    text_qa_template=careful_prompt,\n",
    ")\n",
    "\n",
    "# Try again with careful prompt\n",
    "response = careful_query_engine.query(out_of_scope_query)\n",
    "print(f\"Query: {out_of_scope_query}\")\n",
    "print(f\"Response with careful prompt: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query Logging and Debugging\n",
    "\n",
    "For production systems, logging queries and responses is essential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class QueryLogger:\n",
    "    \"\"\"Simple query logger for debugging and analytics.\"\"\"\n",
    "    \n",
    "    def __init__(self, query_engine):\n",
    "        self.query_engine = query_engine\n",
    "        self.logs = []\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = self.query_engine.query(question)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": question,\n",
    "            \"response_length\": len(str(response)),\n",
    "            \"num_sources\": len(response.source_nodes),\n",
    "            \"top_score\": response.source_nodes[0].score if response.source_nodes else 0,\n",
    "            \"elapsed_seconds\": round(elapsed, 2),\n",
    "        }\n",
    "        \n",
    "        self.logs.append(log_entry)\n",
    "        \n",
    "        return str(response)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        if not self.logs:\n",
    "            return {\"message\": \"No queries logged yet\"}\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.logs),\n",
    "            \"avg_response_time\": sum(l[\"elapsed_seconds\"] for l in self.logs) / len(self.logs),\n",
    "            \"avg_sources_used\": sum(l[\"num_sources\"] for l in self.logs) / len(self.logs),\n",
    "            \"avg_top_score\": sum(l[\"top_score\"] for l in self.logs) / len(self.logs),\n",
    "        }\n",
    "    \n",
    "    def export_logs(self, filepath: str):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.logs, f, indent=2)\n",
    "        print(f\"Exported {len(self.logs)} log entries to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the logger\n",
    "logged_engine = QueryLogger(query_engine)\n",
    "\n",
    "# Run some queries\n",
    "queries = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain machine learning algorithms.\",\n",
    "    \"How does error handling work in Python?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"Q: {q}\")\n",
    "    response = logged_engine.query(q)\n",
    "    print(f\"A: {response[:150]}...\\n\")\n",
    "\n",
    "# View statistics\n",
    "print(\"\\n=== Query Statistics ===\")\n",
    "stats = logged_engine.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "You've learned advanced querying techniques in LlamaIndex:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Technique | When to Use | Benefit |\n",
    "|-----------|-------------|--------|\n",
    "| **Streaming** | User-facing apps | Better perceived performance |\n",
    "| **Async** | Batch processing | Concurrent execution |\n",
    "| **HyDE** | Complex queries | Better retrieval |\n",
    "| **Evaluation** | Quality assurance | Measurable quality |\n",
    "| **Custom Prompts** | Edge cases | Better handling |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always stream** for user-facing applications\n",
    "2. **Use async** for batch operations\n",
    "3. **Evaluate regularly** to catch quality issues\n",
    "4. **Log everything** in production\n",
    "5. **Handle edge cases** with custom prompts\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the Intermediate section, we'll explore:\n",
    "- Different index types (Summary, Tree, Keyword)\n",
    "- Custom retrievers and reranking\n",
    "- Chat engines with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Streaming chat**: Build a simple chat interface that streams responses\n",
    "\n",
    "2. **Async benchmark**: Compare sync vs async with 10+ queries\n",
    "\n",
    "3. **Custom evaluator**: Create an evaluator that checks for specific criteria\n",
    "\n",
    "4. **Query analysis**: Use the logger to analyze query patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Try your own experiments here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
