{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Retrievers and Reranking\n",
    "\n",
    "Retrieval quality is often the bottleneck in RAG systems. This notebook covers advanced retrieval techniques including custom retrievers, reranking, and fusion strategies.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Build custom retrievers for specific needs\n",
    "2. Implement reranking to improve retrieval quality\n",
    "3. Use hybrid search (vector + keyword)\n",
    "4. Apply retrieval fusion strategies\n",
    "5. Evaluate retrieval performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from typing import List\n",
    "\n",
    "# Configure\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and index documents\n",
    "documents = SimpleDirectoryReader(\"../data/sample_docs\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(documents)} documents and built index!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Retriever Interface\n",
    "\n",
    "All retrievers in LlamaIndex inherit from `BaseRetriever` and implement the `_retrieve` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default vector retriever\n",
    "default_retriever = index.as_retriever(\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is machine learning?\"\n",
    "retrieved_nodes = default_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nRetrieved {len(retrieved_nodes)} nodes:\\n\")\n",
    "\n",
    "for i, node in enumerate(retrieved_nodes):\n",
    "    print(f\"Node {i+1}:\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print(f\"  Text: {node.text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Custom Retriever\n",
    "\n",
    "Let's create a custom retriever that filters by metadata and applies custom scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "class MetadataFilterRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever with metadata filtering and score boosting.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        index: VectorStoreIndex,\n",
    "        similarity_top_k: int = 5,\n",
    "        required_keywords: List[str] = None,\n",
    "        score_threshold: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._index = index\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._required_keywords = required_keywords or []\n",
    "        self._score_threshold = score_threshold\n",
    "        \n",
    "        # Create base retriever\n",
    "        self._base_retriever = VectorIndexRetriever(\n",
    "            index=index,\n",
    "            similarity_top_k=similarity_top_k * 2,  # Retrieve more for filtering\n",
    "        )\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        # Get initial results\n",
    "        nodes = self._base_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # Apply filters\n",
    "        filtered_nodes = []\n",
    "        for node in nodes:\n",
    "            # Score threshold filter\n",
    "            if node.score < self._score_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Keyword filter (optional)\n",
    "            if self._required_keywords:\n",
    "                text_lower = node.text.lower()\n",
    "                has_keyword = any(kw.lower() in text_lower for kw in self._required_keywords)\n",
    "                if not has_keyword:\n",
    "                    continue\n",
    "            \n",
    "            filtered_nodes.append(node)\n",
    "        \n",
    "        # Return top k after filtering\n",
    "        return filtered_nodes[:self._similarity_top_k]\n",
    "\n",
    "print(\"✓ Custom retriever class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the custom retriever\n",
    "custom_retriever = MetadataFilterRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    "    required_keywords=[\"learning\"],  # Must contain \"learning\"\n",
    "    score_threshold=0.5,  # Minimum similarity score\n",
    ")\n",
    "\n",
    "query = \"How do systems improve over time?\"\n",
    "filtered_nodes = custom_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Filter: Must contain 'learning', score >= 0.5\")\n",
    "print(f\"\\nRetrieved {len(filtered_nodes)} nodes after filtering:\\n\")\n",
    "\n",
    "for i, node in enumerate(filtered_nodes):\n",
    "    print(f\"Node {i+1}: score={node.score:.4f}\")\n",
    "    print(f\"  {node.text[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reranking Retrieved Results\n",
    "\n",
    "Reranking uses a more sophisticated model to re-score and reorder retrieved results. This often improves quality significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# Create a reranker using a cross-encoder model\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
    "    top_n=3,  # Return top 3 after reranking\n",
    ")\n",
    "\n",
    "print(\"✓ Reranker initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results with and without reranking\n",
    "query = \"What are the ethical concerns with AI systems?\"\n",
    "\n",
    "# Without reranking\n",
    "retriever_no_rerank = index.as_retriever(similarity_top_k=5)\n",
    "nodes_no_rerank = retriever_no_rerank.retrieve(query)\n",
    "\n",
    "print(\"WITHOUT Reranking:\")\n",
    "print(\"-\" * 50)\n",
    "for i, node in enumerate(nodes_no_rerank[:3]):\n",
    "    print(f\"{i+1}. Score: {node.score:.4f}\")\n",
    "    print(f\"   {node.text[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With reranking\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine_with_rerank = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever_no_rerank,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "# Get reranked nodes manually for comparison\n",
    "query_bundle = QueryBundle(query_str=query)\n",
    "reranked_nodes = reranker.postprocess_nodes(\n",
    "    nodes_no_rerank,\n",
    "    query_bundle=query_bundle,\n",
    ")\n",
    "\n",
    "print(\"\\nWITH Reranking:\")\n",
    "print(\"-\" * 50)\n",
    "for i, node in enumerate(reranked_nodes):\n",
    "    print(f\"{i+1}. Score: {node.score:.4f}\")\n",
    "    print(f\"   {node.text[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM-Based Reranking\n",
    "\n",
    "For even better quality, you can use an LLM to rerank results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# LLM-based reranker\n",
    "llm_reranker = LLMRerank(\n",
    "    llm=Settings.llm,\n",
    "    choice_batch_size=5,\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "print(\"✓ LLM Reranker initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LLM reranking\n",
    "query = \"How can machine learning be applied to real-world problems?\"\n",
    "\n",
    "# Get initial results\n",
    "initial_nodes = retriever_no_rerank.retrieve(query)\n",
    "\n",
    "print(\"Initial retrieval (vector similarity):\")\n",
    "for i, node in enumerate(initial_nodes[:3]):\n",
    "    print(f\"  {i+1}. {node.text[:80]}...\")\n",
    "\n",
    "# LLM rerank\n",
    "print(\"\\nAfter LLM Reranking:\")\n",
    "llm_reranked = llm_reranker.postprocess_nodes(\n",
    "    initial_nodes,\n",
    "    query_bundle=QueryBundle(query_str=query),\n",
    ")\n",
    "\n",
    "for i, node in enumerate(llm_reranked):\n",
    "    print(f\"  {i+1}. {node.text[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Search (Vector + BM25)\n",
    "\n",
    "Combining semantic search with traditional keyword search often yields better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BM25Retriever\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "# Extract nodes from the index for BM25\n",
    "docstore = index.docstore\n",
    "nodes = list(docstore.docs.values())\n",
    "\n",
    "# Create BM25 retriever (keyword-based)\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# Create vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"✓ Both retrievers ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval methods\n",
    "query = \"supervised learning algorithms\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "# BM25 (keyword)\n",
    "bm25_nodes = bm25_retriever.retrieve(query)\n",
    "print(\"BM25 (Keyword) Results:\")\n",
    "for i, node in enumerate(bm25_nodes[:3]):\n",
    "    print(f\"  {i+1}. {node.text[:80]}...\")\n",
    "\n",
    "# Vector (semantic)\n",
    "vector_nodes = vector_retriever.retrieve(query)\n",
    "print(\"\\nVector (Semantic) Results:\")\n",
    "for i, node in enumerate(vector_nodes[:3]):\n",
    "    print(f\"  {i+1}. {node.text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "# Create a fusion retriever that combines both\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    retrievers=[\n",
    "        vector_retriever,\n",
    "        bm25_retriever,\n",
    "    ],\n",
    "    similarity_top_k=5,\n",
    "    num_queries=1,  # Don't generate additional queries\n",
    "    mode=\"reciprocal_rerank\",  # Fusion method\n",
    ")\n",
    "\n",
    "print(\"✓ Hybrid retriever created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid retrieval\n",
    "query = \"Python functions and methods\"\n",
    "\n",
    "hybrid_nodes = hybrid_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nHybrid (Vector + BM25 Fusion) Results:\")\n",
    "for i, node in enumerate(hybrid_nodes):\n",
    "    print(f\"  {i+1}. Score: {node.score:.4f}\")\n",
    "    print(f\"     {node.text[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Expansion for Better Recall\n",
    "\n",
    "Generate multiple query variations to improve recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query fusion with query generation\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    retrievers=[vector_retriever],\n",
    "    similarity_top_k=5,\n",
    "    num_queries=4,  # Generate 4 query variations\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Query fusion retriever ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with query expansion\n",
    "query = \"How do AI systems make decisions?\"\n",
    "\n",
    "print(f\"Original Query: '{query}'\\n\")\n",
    "print(\"(Watch for generated query variations...)\\n\")\n",
    "\n",
    "expanded_nodes = fusion_retriever.retrieve(query)\n",
    "\n",
    "print(f\"\\nRetrieved {len(expanded_nodes)} nodes after fusion:\")\n",
    "for i, node in enumerate(expanded_nodes):\n",
    "    print(f\"  {i+1}. Score: {node.score:.4f} - {node.text[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Node Postprocessors\n",
    "\n",
    "Create custom postprocessors for specific filtering logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "class KeywordBoostPostprocessor(BaseNodePostprocessor):\n",
    "    \"\"\"Boost scores for nodes containing specific keywords.\"\"\"\n",
    "    \n",
    "    boost_keywords: List[str] = []\n",
    "    boost_factor: float = 1.5\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        for node in nodes:\n",
    "            text_lower = node.text.lower()\n",
    "            # Check if any boost keyword is present\n",
    "            for keyword in self.boost_keywords:\n",
    "                if keyword.lower() in text_lower:\n",
    "                    node.score *= self.boost_factor\n",
    "                    break\n",
    "        \n",
    "        # Re-sort by new scores\n",
    "        return sorted(nodes, key=lambda x: x.score, reverse=True)\n",
    "\n",
    "print(\"✓ Custom postprocessor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the custom postprocessor\n",
    "keyword_booster = KeywordBoostPostprocessor(\n",
    "    boost_keywords=[\"neural\", \"deep learning\"],\n",
    "    boost_factor=2.0,\n",
    ")\n",
    "\n",
    "# Create query engine with postprocessor\n",
    "boosted_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=vector_retriever,\n",
    "    node_postprocessors=[keyword_booster],\n",
    ")\n",
    "\n",
    "query = \"How do advanced AI systems work?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Boosting nodes with 'neural' or 'deep learning'\\n\")\n",
    "\n",
    "response = boosted_engine.query(query)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrieval Evaluation\n",
    "\n",
    "Measure retrieval quality with metrics like hit rate and MRR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# Generate evaluation questions\n",
    "eval_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does Python handle exceptions?\",\n",
    "    \"What are neural networks?\",\n",
    "    \"What is object-oriented programming?\",\n",
    "]\n",
    "\n",
    "# We'll use a simple evaluation approach\n",
    "def evaluate_retriever(retriever, questions, expected_keywords):\n",
    "    \"\"\"Simple retrieval evaluation based on keyword presence.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question, keywords in zip(questions, expected_keywords):\n",
    "        nodes = retriever.retrieve(question)\n",
    "        \n",
    "        # Check if any retrieved node contains expected keywords\n",
    "        hit = False\n",
    "        for node in nodes:\n",
    "            if any(kw.lower() in node.text.lower() for kw in keywords):\n",
    "                hit = True\n",
    "                break\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"hit\": hit,\n",
    "            \"num_nodes\": len(nodes),\n",
    "            \"top_score\": nodes[0].score if nodes else 0,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Expected keywords for each question\n",
    "expected = [\n",
    "    [\"machine learning\", \"learn\"],\n",
    "    [\"exception\", \"error\", \"try\", \"except\"],\n",
    "    [\"neural\", \"network\", \"deep\"],\n",
    "    [\"object\", \"class\", \"inheritance\"],\n",
    "]\n",
    "\n",
    "print(\"Evaluating retriever...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate different retrievers\n",
    "retrievers_to_eval = {\n",
    "    \"Vector\": vector_retriever,\n",
    "    \"BM25\": bm25_retriever,\n",
    "    \"Hybrid\": hybrid_retriever,\n",
    "}\n",
    "\n",
    "print(\"Retrieval Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, retriever in retrievers_to_eval.items():\n",
    "    results = evaluate_retriever(retriever, eval_questions, expected)\n",
    "    hit_rate = sum(r[\"hit\"] for r in results) / len(results)\n",
    "    avg_score = sum(r[\"top_score\"] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"\\n{name} Retriever:\")\n",
    "    print(f\"  Hit Rate: {hit_rate:.1%}\")\n",
    "    print(f\"  Avg Top Score: {avg_score:.4f}\")\n",
    "    \n",
    "    for r in results:\n",
    "        status = \"✓\" if r[\"hit\"] else \"✗\"\n",
    "        print(f\"    {status} {r['question'][:40]}... (score: {r['top_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "You've learned advanced retrieval techniques in LlamaIndex:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Technique | When to Use | Impact |\n",
    "|-----------|-------------|--------|\n",
    "| **Custom Retriever** | Special filtering logic | Flexibility |\n",
    "| **Reranking** | Improve top results | Quality |\n",
    "| **Hybrid Search** | Balance semantic + keyword | Recall |\n",
    "| **Query Expansion** | Improve recall | Coverage |\n",
    "| **Postprocessors** | Custom scoring logic | Customization |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with vector retrieval** as baseline\n",
    "2. **Add reranking** for quality-critical applications\n",
    "3. **Use hybrid search** when exact terms matter\n",
    "4. **Evaluate systematically** to measure improvements\n",
    "5. **Profile latency** - complex retrieval adds time\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll build chat engines with conversational memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Custom filter**: Create a retriever that filters by document source\n",
    "\n",
    "2. **Ensemble reranking**: Combine multiple reranking strategies\n",
    "\n",
    "3. **Ablation study**: Measure impact of each component on quality\n",
    "\n",
    "4. **Latency analysis**: Profile retrieval time for different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Build your custom retrieval pipeline here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
