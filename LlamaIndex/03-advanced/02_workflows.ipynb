{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Workflows\n",
    "\n",
    "Workflows provide structured control flow for complex AI applications. They allow you to define multi-step processes with branching, looping, and error handling.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the Workflow architecture\n",
    "2. Build sequential workflows\n",
    "3. Implement branching and conditional logic\n",
    "4. Create parallel execution patterns\n",
    "5. Handle events and state management\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Workflows?\n",
    "\n",
    "Workflows are **event-driven** pipelines that:\n",
    "- Define explicit control flow\n",
    "- Handle complex multi-step processes\n",
    "- Support parallel execution\n",
    "- Enable state management between steps\n",
    "\n",
    "### Workflow vs Agent\n",
    "\n",
    "| Aspect | Agent | Workflow |\n",
    "|--------|-------|----------|\n",
    "| Control | LLM decides next step | Explicit code defines flow |\n",
    "| Predictability | Variable | Deterministic |\n",
    "| Debugging | Harder | Easier |\n",
    "| Best for | Open-ended tasks | Structured processes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Event,\n",
    "    step,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Configure\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Workflow Structure\n",
    "\n",
    "Workflows consist of:\n",
    "- **Events**: Messages that flow between steps\n",
    "- **Steps**: Functions that process events\n",
    "- **StartEvent**: Triggers the workflow\n",
    "- **StopEvent**: Ends the workflow with a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple workflow\n",
    "class SimpleWorkflow(Workflow):\n",
    "    \"\"\"A basic workflow that processes text.\"\"\"\n",
    "    \n",
    "    @step\n",
    "    async def process_input(self, ev: StartEvent) -> StopEvent:\n",
    "        \"\"\"Process the input and return result.\"\"\"\n",
    "        input_text = ev.input\n",
    "        \n",
    "        # Simple processing\n",
    "        processed = input_text.upper()\n",
    "        word_count = len(input_text.split())\n",
    "        \n",
    "        result = f\"Processed: {processed}\\nWord count: {word_count}\"\n",
    "        \n",
    "        return StopEvent(result=result)\n",
    "\n",
    "print(\"✓ SimpleWorkflow defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "async def run_simple_workflow():\n",
    "    workflow = SimpleWorkflow()\n",
    "    result = await workflow.run(input=\"Hello world from LlamaIndex workflows!\")\n",
    "    return result\n",
    "\n",
    "result = asyncio.run(run_simple_workflow())\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Step Workflow with Custom Events\n",
    "\n",
    "Define custom events to pass data between steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "# Define custom events\n",
    "class ValidationEvent(Event):\n",
    "    \"\"\"Event after input validation.\"\"\"\n",
    "    text: str\n",
    "    is_valid: bool\n",
    "\n",
    "class ProcessedEvent(Event):\n",
    "    \"\"\"Event after text processing.\"\"\"\n",
    "    original: str\n",
    "    processed: str\n",
    "    word_count: int\n",
    "\n",
    "class EnrichedEvent(Event):\n",
    "    \"\"\"Event after enrichment with LLM.\"\"\"\n",
    "    summary: str\n",
    "    metadata: dict\n",
    "\n",
    "print(\"✓ Custom events defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessingWorkflow(Workflow):\n",
    "    \"\"\"A multi-step workflow for text processing.\"\"\"\n",
    "    \n",
    "    @step\n",
    "    async def validate_input(self, ev: StartEvent) -> ValidationEvent:\n",
    "        \"\"\"Step 1: Validate the input text.\"\"\"\n",
    "        print(\"Step 1: Validating input...\")\n",
    "        \n",
    "        text = ev.input\n",
    "        is_valid = len(text) > 0 and len(text) < 10000\n",
    "        \n",
    "        return ValidationEvent(text=text, is_valid=is_valid)\n",
    "    \n",
    "    @step\n",
    "    async def process_text(self, ev: ValidationEvent) -> ProcessedEvent | StopEvent:\n",
    "        \"\"\"Step 2: Process the text if valid.\"\"\"\n",
    "        print(\"Step 2: Processing text...\")\n",
    "        \n",
    "        if not ev.is_valid:\n",
    "            return StopEvent(result=\"Error: Invalid input\")\n",
    "        \n",
    "        processed = ev.text.strip()\n",
    "        word_count = len(processed.split())\n",
    "        \n",
    "        return ProcessedEvent(\n",
    "            original=ev.text,\n",
    "            processed=processed,\n",
    "            word_count=word_count,\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def enrich_with_llm(self, ev: ProcessedEvent) -> EnrichedEvent:\n",
    "        \"\"\"Step 3: Enrich with LLM analysis.\"\"\"\n",
    "        print(\"Step 3: Enriching with LLM...\")\n",
    "        \n",
    "        # Use LLM to generate summary\n",
    "        llm = Settings.llm\n",
    "        response = await llm.acomplete(\n",
    "            f\"Summarize this text in one sentence: {ev.processed}\"\n",
    "        )\n",
    "        \n",
    "        return EnrichedEvent(\n",
    "            summary=str(response),\n",
    "            metadata={\n",
    "                \"word_count\": ev.word_count,\n",
    "                \"char_count\": len(ev.processed),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def format_output(self, ev: EnrichedEvent) -> StopEvent:\n",
    "        \"\"\"Step 4: Format final output.\"\"\"\n",
    "        print(\"Step 4: Formatting output...\")\n",
    "        \n",
    "        result = f\"\"\"\n",
    "=== Processing Complete ===\n",
    "Summary: {ev.summary}\n",
    "Metadata: {ev.metadata}\n",
    "\"\"\"\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "print(\"✓ TextProcessingWorkflow defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the multi-step workflow\n",
    "async def run_text_workflow():\n",
    "    workflow = TextProcessingWorkflow(timeout=60)\n",
    "    \n",
    "    input_text = \"\"\"\n",
    "    Artificial intelligence is transforming how we work and live. \n",
    "    Machine learning algorithms can now recognize patterns in data \n",
    "    that humans might miss. This technology is being applied in \n",
    "    healthcare, finance, and many other industries.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = await workflow.run(input=input_text)\n",
    "    return result\n",
    "\n",
    "print(\"Running multi-step workflow...\\n\")\n",
    "result = asyncio.run(run_text_workflow())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Workflow with RAG\n",
    "\n",
    "Integrate RAG into a workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents and create index\n",
    "documents = SimpleDirectoryReader(\"../data/sample_docs\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "print(\"\\n✓ Index ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG workflow events\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RetrievalEvent(Event):\n",
    "    query: str\n",
    "    contexts: list\n",
    "    \n",
    "class AnswerEvent(Event):\n",
    "    answer: str\n",
    "    sources: list\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    \"\"\"A RAG workflow with explicit steps.\"\"\"\n",
    "    \n",
    "    def __init__(self, query_engine, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.query_engine = query_engine\n",
    "    \n",
    "    @step\n",
    "    async def parse_query(self, ev: StartEvent) -> QueryEvent:\n",
    "        \"\"\"Parse and validate the query.\"\"\"\n",
    "        print(\"Step 1: Parsing query...\")\n",
    "        query = ev.input.strip()\n",
    "        return QueryEvent(query=query)\n",
    "    \n",
    "    @step\n",
    "    async def retrieve(self, ev: QueryEvent) -> RetrievalEvent:\n",
    "        \"\"\"Retrieve relevant documents.\"\"\"\n",
    "        print(\"Step 2: Retrieving documents...\")\n",
    "        \n",
    "        # Get retriever from query engine\n",
    "        retriever = self.query_engine._retriever\n",
    "        nodes = retriever.retrieve(ev.query)\n",
    "        \n",
    "        contexts = [node.text for node in nodes]\n",
    "        \n",
    "        print(f\"  Retrieved {len(contexts)} contexts\")\n",
    "        return RetrievalEvent(query=ev.query, contexts=contexts)\n",
    "    \n",
    "    @step\n",
    "    async def generate(self, ev: RetrievalEvent) -> AnswerEvent:\n",
    "        \"\"\"Generate answer using LLM.\"\"\"\n",
    "        print(\"Step 3: Generating answer...\")\n",
    "        \n",
    "        # Build prompt with context\n",
    "        context_str = \"\\n\\n\".join(ev.contexts)\n",
    "        prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {ev.query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        llm = Settings.llm\n",
    "        response = await llm.acomplete(prompt)\n",
    "        \n",
    "        return AnswerEvent(\n",
    "            answer=str(response),\n",
    "            sources=ev.contexts[:2],  # Include top 2 sources\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def format_response(self, ev: AnswerEvent) -> StopEvent:\n",
    "        \"\"\"Format the final response.\"\"\"\n",
    "        print(\"Step 4: Formatting response...\")\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": ev.answer,\n",
    "            \"sources\": [s[:100] + \"...\" for s in ev.sources],\n",
    "        }\n",
    "        \n",
    "        return StopEvent(result=result)\n",
    "\n",
    "print(\"✓ RAGWorkflow defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG workflow\n",
    "async def run_rag_workflow():\n",
    "    workflow = RAGWorkflow(query_engine=query_engine, timeout=60)\n",
    "    result = await workflow.run(input=\"What is machine learning?\")\n",
    "    return result\n",
    "\n",
    "print(\"Running RAG workflow...\\n\")\n",
    "result = asyncio.run(run_rag_workflow())\n",
    "\n",
    "print(\"\\n=== Result ===\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\\nSources used: {len(result['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Branching Workflow\n",
    "\n",
    "Workflows can have conditional branching based on events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define events for branching\n",
    "class ClassifyEvent(Event):\n",
    "    query: str\n",
    "    query_type: str  # \"factual\", \"analytical\", \"creative\"\n",
    "\n",
    "class FactualEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class AnalyticalEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class CreativeEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class BranchingWorkflow(Workflow):\n",
    "    \"\"\"Workflow with conditional branching based on query type.\"\"\"\n",
    "    \n",
    "    @step\n",
    "    async def classify_query(self, ev: StartEvent) -> ClassifyEvent:\n",
    "        \"\"\"Classify the query type using LLM.\"\"\"\n",
    "        print(\"Classifying query...\")\n",
    "        \n",
    "        llm = Settings.llm\n",
    "        prompt = f\"\"\"Classify this query into one of: factual, analytical, creative.\n",
    "Just respond with one word.\n",
    "\n",
    "Query: {ev.input}\n",
    "\n",
    "Classification:\"\"\"\n",
    "        \n",
    "        response = await llm.acomplete(prompt)\n",
    "        query_type = str(response).strip().lower()\n",
    "        \n",
    "        # Normalize\n",
    "        if \"fact\" in query_type:\n",
    "            query_type = \"factual\"\n",
    "        elif \"analy\" in query_type:\n",
    "            query_type = \"analytical\"\n",
    "        else:\n",
    "            query_type = \"creative\"\n",
    "        \n",
    "        print(f\"  Query type: {query_type}\")\n",
    "        return ClassifyEvent(query=ev.input, query_type=query_type)\n",
    "    \n",
    "    @step\n",
    "    async def route_query(self, ev: ClassifyEvent) -> FactualEvent | AnalyticalEvent | CreativeEvent:\n",
    "        \"\"\"Route to appropriate handler.\"\"\"\n",
    "        print(f\"Routing to {ev.query_type} handler...\")\n",
    "        \n",
    "        if ev.query_type == \"factual\":\n",
    "            return FactualEvent(query=ev.query)\n",
    "        elif ev.query_type == \"analytical\":\n",
    "            return AnalyticalEvent(query=ev.query)\n",
    "        else:\n",
    "            return CreativeEvent(query=ev.query)\n",
    "    \n",
    "    @step\n",
    "    async def handle_factual(self, ev: FactualEvent) -> StopEvent:\n",
    "        \"\"\"Handle factual queries - focus on accuracy.\"\"\"\n",
    "        print(\"Handling as FACTUAL query...\")\n",
    "        \n",
    "        llm = Settings.llm\n",
    "        response = await llm.acomplete(\n",
    "            f\"Provide a concise, factual answer: {ev.query}\"\n",
    "        )\n",
    "        \n",
    "        return StopEvent(result={\n",
    "            \"type\": \"factual\",\n",
    "            \"answer\": str(response),\n",
    "        })\n",
    "    \n",
    "    @step\n",
    "    async def handle_analytical(self, ev: AnalyticalEvent) -> StopEvent:\n",
    "        \"\"\"Handle analytical queries - provide analysis.\"\"\"\n",
    "        print(\"Handling as ANALYTICAL query...\")\n",
    "        \n",
    "        llm = Settings.llm\n",
    "        response = await llm.acomplete(\n",
    "            f\"Provide a detailed analysis with multiple perspectives: {ev.query}\"\n",
    "        )\n",
    "        \n",
    "        return StopEvent(result={\n",
    "            \"type\": \"analytical\",\n",
    "            \"answer\": str(response),\n",
    "        })\n",
    "    \n",
    "    @step\n",
    "    async def handle_creative(self, ev: CreativeEvent) -> StopEvent:\n",
    "        \"\"\"Handle creative queries - be imaginative.\"\"\"\n",
    "        print(\"Handling as CREATIVE query...\")\n",
    "        \n",
    "        llm = Settings.llm\n",
    "        response = await llm.acomplete(\n",
    "            f\"Provide a creative and engaging response: {ev.query}\"\n",
    "        )\n",
    "        \n",
    "        return StopEvent(result={\n",
    "            \"type\": \"creative\",\n",
    "            \"answer\": str(response),\n",
    "        })\n",
    "\n",
    "print(\"✓ BranchingWorkflow defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test branching workflow with different query types\n",
    "async def test_branching():\n",
    "    workflow = BranchingWorkflow(timeout=60)\n",
    "    \n",
    "    queries = [\n",
    "        \"What is the capital of France?\",  # Factual\n",
    "        \"Why do some companies succeed while others fail?\",  # Analytical\n",
    "        \"Write a haiku about programming.\",  # Creative\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        result = await workflow.run(input=query)\n",
    "        print(f\"\\nType: {result['type']}\")\n",
    "        print(f\"Answer: {result['answer'][:200]}...\")\n",
    "\n",
    "asyncio.run(test_branching())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling in Workflows\n",
    "\n",
    "Handle errors gracefully within workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorEvent(Event):\n",
    "    error_message: str\n",
    "    step_name: str\n",
    "\n",
    "class RobustWorkflow(Workflow):\n",
    "    \"\"\"Workflow with error handling.\"\"\"\n",
    "    \n",
    "    @step\n",
    "    async def risky_step(self, ev: StartEvent) -> ProcessedEvent | ErrorEvent:\n",
    "        \"\"\"A step that might fail.\"\"\"\n",
    "        try:\n",
    "            value = int(ev.input)\n",
    "            if value < 0:\n",
    "                raise ValueError(\"Negative values not allowed\")\n",
    "            \n",
    "            return ProcessedEvent(\n",
    "                original=ev.input,\n",
    "                processed=str(value * 2),\n",
    "                word_count=1,\n",
    "            )\n",
    "        except (ValueError, TypeError) as e:\n",
    "            return ErrorEvent(\n",
    "                error_message=str(e),\n",
    "                step_name=\"risky_step\",\n",
    "            )\n",
    "    \n",
    "    @step\n",
    "    async def handle_success(self, ev: ProcessedEvent) -> StopEvent:\n",
    "        \"\"\"Handle successful processing.\"\"\"\n",
    "        return StopEvent(result={\n",
    "            \"status\": \"success\",\n",
    "            \"result\": ev.processed,\n",
    "        })\n",
    "    \n",
    "    @step\n",
    "    async def handle_error(self, ev: ErrorEvent) -> StopEvent:\n",
    "        \"\"\"Handle errors gracefully.\"\"\"\n",
    "        return StopEvent(result={\n",
    "            \"status\": \"error\",\n",
    "            \"error\": ev.error_message,\n",
    "            \"step\": ev.step_name,\n",
    "        })\n",
    "\n",
    "print(\"✓ RobustWorkflow defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling\n",
    "async def test_error_handling():\n",
    "    workflow = RobustWorkflow(timeout=30)\n",
    "    \n",
    "    test_inputs = [\"42\", \"-5\", \"not a number\"]\n",
    "    \n",
    "    for inp in test_inputs:\n",
    "        print(f\"\\nInput: '{inp}'\")\n",
    "        result = await workflow.run(input=inp)\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "asyncio.run(test_error_handling())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "You've learned how to build workflows with LlamaIndex:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Events** | Messages between workflow steps |\n",
    "| **Steps** | Async functions that process events |\n",
    "| **Branching** | Conditional routing based on data |\n",
    "| **Error Handling** | Graceful failure recovery |\n",
    "\n",
    "### When to Use Workflows\n",
    "\n",
    "- **Use Workflows** when you need predictable, explicit control flow\n",
    "- **Use Agents** when you want LLM-driven decision making\n",
    "- **Combine both** for complex applications\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore creating custom components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Document processing**: Create a workflow that processes documents through multiple stages\n",
    "\n",
    "2. **Retry logic**: Add retry capability for failed steps\n",
    "\n",
    "3. **Parallel steps**: Implement steps that run in parallel\n",
    "\n",
    "4. **State management**: Add global state that persists across steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Build your own workflow here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
