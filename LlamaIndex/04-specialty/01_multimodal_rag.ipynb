{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG with LlamaIndex\n",
    "\n",
    "Multimodal RAG extends traditional RAG to handle images, tables, and other non-text content. This notebook covers techniques for building multimodal AI applications.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand multimodal RAG architecture\n",
    "2. Process documents with images and tables\n",
    "3. Use vision-language models for retrieval\n",
    "4. Build applications that understand visual content\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Multimodal RAG?\n",
    "\n",
    "Traditional RAG only handles text. **Multimodal RAG** extends this to:\n",
    "- **Images**: Photos, diagrams, charts\n",
    "- **Tables**: Structured data in documents\n",
    "- **Mixed content**: PDFs with text, images, and tables\n",
    "\n",
    "### Approaches to Multimodal RAG\n",
    "\n",
    "| Approach | Description | Best For |\n",
    "|----------|-------------|----------|\n",
    "| **Text extraction** | Extract text from images/tables | Simple documents |\n",
    "| **Image embeddings** | Embed images alongside text | Visual similarity |\n",
    "| **Vision LLM** | Use GPT-4V or similar | Complex visual reasoning |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "# Configure\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vision Language Models\n",
    "\n",
    "First, let's understand how to use vision-capable LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multimodal LLM (GPT-4 Vision)\n",
    "multimodal_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4o\",  # Vision-capable model\n",
    "    max_new_tokens=500,\n",
    ")\n",
    "\n",
    "print(\"✓ Multimodal LLM initialized!\")\n",
    "print(f\"Model: {multimodal_llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import ImageDocument\n",
    "import base64\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Helper function to create image document from URL\n",
    "def create_image_doc_from_url(url: str) -> ImageDocument:\n",
    "    \"\"\"Create an ImageDocument from a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    image_data = base64.b64encode(response.content).decode()\n",
    "    return ImageDocument(\n",
    "        image=image_data,\n",
    "        image_url=url,\n",
    "    )\n",
    "\n",
    "print(\"✓ Helper function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze an image with vision LLM\n",
    "# Using a sample image from the web\n",
    "sample_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/200px-Python-logo-notext.svg.png\"\n",
    "\n",
    "try:\n",
    "    image_doc = create_image_doc_from_url(sample_image_url)\n",
    "    \n",
    "    # Ask the vision LLM about the image\n",
    "    response = multimodal_llm.complete(\n",
    "        prompt=\"What is shown in this image? Describe it in detail.\",\n",
    "        image_documents=[image_doc],\n",
    "    )\n",
    "    \n",
    "    print(f\"Image analysis:\\n{response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Image analysis requires a vision-capable model and valid image URL.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multimodal Document Processing\n",
    "\n",
    "Process documents that contain both text and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode, ImageNode\n",
    "from typing import List\n",
    "\n",
    "class MultimodalDocument:\n",
    "    \"\"\"A document with both text and image content.\"\"\"\n",
    "    \n",
    "    def __init__(self, doc_id: str):\n",
    "        self.doc_id = doc_id\n",
    "        self.text_nodes: List[TextNode] = []\n",
    "        self.image_nodes: List[ImageNode] = []\n",
    "    \n",
    "    def add_text(self, text: str, metadata: dict = None):\n",
    "        \"\"\"Add a text section.\"\"\"\n",
    "        node = TextNode(\n",
    "            text=text,\n",
    "            metadata={\n",
    "                \"doc_id\": self.doc_id,\n",
    "                \"type\": \"text\",\n",
    "                **(metadata or {}),\n",
    "            }\n",
    "        )\n",
    "        self.text_nodes.append(node)\n",
    "    \n",
    "    def add_image_description(self, description: str, image_ref: str, metadata: dict = None):\n",
    "        \"\"\"Add an image with its description.\"\"\"\n",
    "        # For retrieval, we index the description\n",
    "        node = TextNode(\n",
    "            text=f\"[IMAGE: {image_ref}]\\n{description}\",\n",
    "            metadata={\n",
    "                \"doc_id\": self.doc_id,\n",
    "                \"type\": \"image\",\n",
    "                \"image_ref\": image_ref,\n",
    "                **(metadata or {}),\n",
    "            }\n",
    "        )\n",
    "        self.text_nodes.append(node)\n",
    "    \n",
    "    def get_all_nodes(self) -> List[TextNode]:\n",
    "        \"\"\"Get all nodes for indexing.\"\"\"\n",
    "        return self.text_nodes\n",
    "\n",
    "print(\"✓ MultimodalDocument class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample multimodal document\n",
    "doc = MultimodalDocument(doc_id=\"ai_overview\")\n",
    "\n",
    "# Add text content\n",
    "doc.add_text(\"\"\"\n",
    "# Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create \n",
    "intelligent machines that can perform tasks typically requiring human intelligence.\n",
    "\"\"\")\n",
    "\n",
    "# Add image description (simulating OCR/vision analysis)\n",
    "doc.add_image_description(\n",
    "    description=\"A diagram showing the relationship between AI, Machine Learning, \"\n",
    "                \"and Deep Learning as nested circles. AI is the outermost circle, \"\n",
    "                \"containing Machine Learning, which contains Deep Learning.\",\n",
    "    image_ref=\"ai_ml_dl_diagram.png\"\n",
    ")\n",
    "\n",
    "doc.add_text(\"\"\"\n",
    "## Machine Learning\n",
    "\n",
    "Machine Learning is a subset of AI that enables systems to learn from data \n",
    "and improve their performance without explicit programming.\n",
    "\"\"\")\n",
    "\n",
    "# Add another image description\n",
    "doc.add_image_description(\n",
    "    description=\"A flowchart showing the machine learning pipeline: Data Collection → \"\n",
    "                \"Data Preprocessing → Model Training → Model Evaluation → Deployment.\",\n",
    "    image_ref=\"ml_pipeline.png\"\n",
    ")\n",
    "\n",
    "doc.add_text(\"\"\"\n",
    "## Deep Learning\n",
    "\n",
    "Deep Learning uses neural networks with many layers to learn hierarchical \n",
    "representations of data. It excels at tasks like image recognition and NLP.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Created document with {len(doc.get_all_nodes())} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the multimodal document\n",
    "nodes = doc.get_all_nodes()\n",
    "\n",
    "# Create index from nodes\n",
    "mm_index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "# Create query engine\n",
    "mm_query_engine = mm_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "print(\"✓ Multimodal index created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about visual content\n",
    "queries = [\n",
    "    \"What does the AI/ML diagram show?\",\n",
    "    \"Describe the machine learning pipeline.\",\n",
    "    \"What is the relationship between AI and deep learning?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQ: {query}\")\n",
    "    response = mm_query_engine.query(query)\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Table Understanding\n",
    "\n",
    "Handle tabular data in documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample tables\n",
    "ml_algorithms = pd.DataFrame({\n",
    "    \"Algorithm\": [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\", \"Neural Network\"],\n",
    "    \"Type\": [\"Regression\", \"Both\", \"Both\", \"Both\", \"Both\"],\n",
    "    \"Complexity\": [\"Low\", \"Medium\", \"High\", \"High\", \"Very High\"],\n",
    "    \"Interpretability\": [\"High\", \"High\", \"Low\", \"Low\", \"Very Low\"],\n",
    "})\n",
    "\n",
    "print(\"Sample table:\")\n",
    "print(ml_algorithms.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_text(df: pd.DataFrame, table_name: str) -> str:\n",
    "    \"\"\"Convert a pandas DataFrame to a searchable text representation.\"\"\"\n",
    "    lines = [f\"TABLE: {table_name}\"]\n",
    "    lines.append(f\"Columns: {', '.join(df.columns)}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Add markdown representation\n",
    "    lines.append(df.to_markdown(index=False))\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Add row-by-row natural language description\n",
    "    lines.append(\"Row descriptions:\")\n",
    "    for _, row in df.iterrows():\n",
    "        desc = \", \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
    "        lines.append(f\"- {desc}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Convert table to text\n",
    "table_text = table_to_text(ml_algorithms, \"Machine Learning Algorithms Comparison\")\n",
    "print(table_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document with table content\n",
    "table_doc = MultimodalDocument(doc_id=\"ml_comparison\")\n",
    "\n",
    "table_doc.add_text(\"\"\"\n",
    "# Comparison of Machine Learning Algorithms\n",
    "\n",
    "This document compares different machine learning algorithms based on \n",
    "their type, complexity, and interpretability.\n",
    "\"\"\")\n",
    "\n",
    "table_doc.add_text(table_text, metadata={\"type\": \"table\", \"table_name\": \"ML Algorithms\"})\n",
    "\n",
    "table_doc.add_text(\"\"\"\n",
    "## Key Insights\n",
    "\n",
    "- Linear Regression is the simplest and most interpretable\n",
    "- Neural Networks are the most complex but least interpretable\n",
    "- Random Forest offers good performance with moderate complexity\n",
    "\"\"\")\n",
    "\n",
    "# Index\n",
    "table_nodes = table_doc.get_all_nodes()\n",
    "table_index = VectorStoreIndex(nodes=table_nodes)\n",
    "table_engine = table_index.as_query_engine()\n",
    "\n",
    "print(\"✓ Table-aware index created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about table content\n",
    "table_queries = [\n",
    "    \"Which algorithm has the highest interpretability?\",\n",
    "    \"Compare Random Forest and SVM complexity.\",\n",
    "    \"Which algorithms can be used for both classification and regression?\",\n",
    "]\n",
    "\n",
    "for query in table_queries:\n",
    "    print(f\"\\nQ: {query}\")\n",
    "    response = table_engine.query(query)\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multimodal Pipeline Class\n",
    "\n",
    "A complete pipeline for handling multimodal documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MultimodalContent:\n",
    "    \"\"\"Container for multimodal content.\"\"\"\n",
    "    content_type: str  # 'text', 'image', 'table'\n",
    "    content: Any\n",
    "    description: Optional[str] = None\n",
    "    metadata: Optional[Dict] = None\n",
    "\n",
    "class MultimodalRAGPipeline:\n",
    "    \"\"\"A complete pipeline for multimodal RAG.\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_llm=None):\n",
    "        self.vision_llm = vision_llm\n",
    "        self.nodes = []\n",
    "        self.index = None\n",
    "        self.query_engine = None\n",
    "    \n",
    "    def add_text(self, text: str, metadata: dict = None):\n",
    "        \"\"\"Add text content.\"\"\"\n",
    "        node = TextNode(\n",
    "            text=text,\n",
    "            metadata={\"type\": \"text\", **(metadata or {})}\n",
    "        )\n",
    "        self.nodes.append(node)\n",
    "    \n",
    "    def add_image(self, image_description: str, image_path: str = None, metadata: dict = None):\n",
    "        \"\"\"Add image content (via description).\"\"\"\n",
    "        text = f\"[IMAGE: {image_path or 'embedded'}]\\nDescription: {image_description}\"\n",
    "        node = TextNode(\n",
    "            text=text,\n",
    "            metadata={\n",
    "                \"type\": \"image\",\n",
    "                \"image_path\": image_path,\n",
    "                **(metadata or {})\n",
    "            }\n",
    "        )\n",
    "        self.nodes.append(node)\n",
    "    \n",
    "    def add_table(self, df: pd.DataFrame, table_name: str, metadata: dict = None):\n",
    "        \"\"\"Add table content.\"\"\"\n",
    "        text = table_to_text(df, table_name)\n",
    "        node = TextNode(\n",
    "            text=text,\n",
    "            metadata={\n",
    "                \"type\": \"table\",\n",
    "                \"table_name\": table_name,\n",
    "                **(metadata or {})\n",
    "            }\n",
    "        )\n",
    "        self.nodes.append(node)\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build the vector index.\"\"\"\n",
    "        if not self.nodes:\n",
    "            raise ValueError(\"No content added yet\")\n",
    "        \n",
    "        self.index = VectorStoreIndex(nodes=self.nodes)\n",
    "        self.query_engine = self.index.as_query_engine(similarity_top_k=5)\n",
    "        print(f\"✓ Index built with {len(self.nodes)} nodes\")\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Query the multimodal index.\"\"\"\n",
    "        if not self.query_engine:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        response = self.query_engine.query(question)\n",
    "        return str(response)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get statistics about the content.\"\"\"\n",
    "        stats = {\n",
    "            \"total_nodes\": len(self.nodes),\n",
    "            \"text_nodes\": len([n for n in self.nodes if n.metadata.get(\"type\") == \"text\"]),\n",
    "            \"image_nodes\": len([n for n in self.nodes if n.metadata.get(\"type\") == \"image\"]),\n",
    "            \"table_nodes\": len([n for n in self.nodes if n.metadata.get(\"type\") == \"table\"]),\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "print(\"✓ MultimodalRAGPipeline defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pipeline\n",
    "pipeline = MultimodalRAGPipeline()\n",
    "\n",
    "# Add various content types\n",
    "pipeline.add_text(\"\"\"\n",
    "# Deep Learning Architectures\n",
    "\n",
    "Deep learning uses various neural network architectures for different tasks.\n",
    "Common architectures include CNNs, RNNs, and Transformers.\n",
    "\"\"\")\n",
    "\n",
    "pipeline.add_image(\n",
    "    image_description=\"A CNN architecture diagram showing input layer, \"\n",
    "                      \"convolutional layers with pooling, fully connected layers, \"\n",
    "                      \"and output layer for image classification.\",\n",
    "    image_path=\"cnn_architecture.png\"\n",
    ")\n",
    "\n",
    "# Add architecture comparison table\n",
    "architectures = pd.DataFrame({\n",
    "    \"Architecture\": [\"CNN\", \"RNN\", \"LSTM\", \"Transformer\"],\n",
    "    \"Best For\": [\"Images\", \"Sequences\", \"Long sequences\", \"Everything\"],\n",
    "    \"Key Feature\": [\"Convolution\", \"Recurrence\", \"Memory gates\", \"Attention\"],\n",
    "    \"Year Introduced\": [1998, 1986, 1997, 2017],\n",
    "})\n",
    "\n",
    "pipeline.add_table(architectures, \"Deep Learning Architectures\")\n",
    "\n",
    "pipeline.add_text(\"\"\"\n",
    "## Transformers Revolution\n",
    "\n",
    "Transformers, introduced in the \"Attention is All You Need\" paper, \n",
    "have revolutionized NLP and are now being applied to vision tasks.\n",
    "\"\"\")\n",
    "\n",
    "# Build and query\n",
    "pipeline.build_index()\n",
    "print(f\"\\nStats: {pipeline.get_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multimodal queries\n",
    "test_queries = [\n",
    "    \"What does the CNN diagram show?\",\n",
    "    \"When was the Transformer architecture introduced?\",\n",
    "    \"Which architecture is best for image processing?\",\n",
    "    \"What is the key feature of LSTM?\",\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {pipeline.query(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "You've learned multimodal RAG techniques:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Technique | Description | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| **Vision LLM** | GPT-4V for image understanding | Complex visual reasoning |\n",
    "| **Image descriptions** | Index image descriptions as text | Visual content retrieval |\n",
    "| **Table processing** | Convert tables to searchable text | Structured data Q&A |\n",
    "| **Multimodal pipeline** | Unified handling of mixed content | Complete documents |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Describe images thoroughly** for better retrieval\n",
    "2. **Include table structure** in text representation\n",
    "3. **Use metadata** to track content types\n",
    "4. **Consider LlamaParse** for complex PDFs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore GraphRAG for entity relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **PDF processing**: Process a PDF with mixed content using LlamaParse\n",
    "\n",
    "2. **Chart understanding**: Create a pipeline that describes chart images\n",
    "\n",
    "3. **Table Q&A**: Build a specialized engine for tabular data\n",
    "\n",
    "4. **Image similarity**: Implement image-to-image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Build your multimodal application here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
