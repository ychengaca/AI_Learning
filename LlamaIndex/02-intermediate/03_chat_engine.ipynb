{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Engines with Memory\n",
    "\n",
    "Chat engines enable conversational interactions with your data, maintaining context across multiple turns. This is essential for building chatbots and conversational assistants.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the difference between query engines and chat engines\n",
    "2. Implement different chat modes\n",
    "3. Customize conversation memory\n",
    "4. Build a complete chatbot with RAG\n",
    "5. Handle conversation state and context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine vs Chat Engine\n",
    "\n",
    "| Feature | Query Engine | Chat Engine |\n",
    "|---------|--------------|-------------|\n",
    "| Memory | None | Maintains conversation history |\n",
    "| Context | Single query | Multi-turn context |\n",
    "| Use Case | One-off questions | Conversations |\n",
    "| Pronouns | Can't resolve \"it\", \"that\" | Understands references |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Configure\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and index documents\n",
    "documents = SimpleDirectoryReader(\"../data/sample_docs\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "print(f\"\\n✓ Index ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Chat Engine\n",
    "\n",
    "The simplest way to create a chat engine from an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic chat engine\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\",  # Reformulates follow-ups\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Chat engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "print(\"=\" * 60)\n",
    "print(\"CONVERSATION START\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First message\n",
    "response1 = chat_engine.chat(\"What is machine learning?\")\n",
    "print(f\"\\nUser: What is machine learning?\")\n",
    "print(f\"Assistant: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up that references previous context\n",
    "response2 = chat_engine.chat(\"What are its main types?\")\n",
    "print(f\"\\nUser: What are its main types?\")\n",
    "print(f\"Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another follow-up\n",
    "response3 = chat_engine.chat(\"Can you give an example of the first type?\")\n",
    "print(f\"\\nUser: Can you give an example of the first type?\")\n",
    "print(f\"Assistant: {response3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View conversation history\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONVERSATION HISTORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for msg in chat_engine.chat_history:\n",
    "    role = msg.role.value.upper()\n",
    "    print(f\"\\n{role}: {str(msg.content)[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset conversation\n",
    "chat_engine.reset()\n",
    "print(\"✓ Conversation reset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chat Modes\n",
    "\n",
    "LlamaIndex offers different chat modes for different use cases:\n",
    "\n",
    "| Mode | Description | Best For |\n",
    "|------|-------------|----------|\n",
    "| `condense_question` | Reformulates follow-ups into standalone queries | General RAG chat |\n",
    "| `context` | Retrieves context for every message | Simple Q&A |\n",
    "| `condense_plus_context` | Combines both approaches | Complex conversations |\n",
    "| `simple` | Direct LLM chat (no retrieval) | General chat |\n",
    "| `react` | Agent-style with reasoning | Tool use |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different chat modes\n",
    "def test_chat_mode(mode, messages):\n",
    "    \"\"\"Test a chat mode with a series of messages.\"\"\"\n",
    "    engine = index.as_chat_engine(chat_mode=mode)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Chat Mode: {mode}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for msg in messages:\n",
    "        response = engine.chat(msg)\n",
    "        print(f\"\\nUser: {msg}\")\n",
    "        print(f\"Assistant: {str(response)[:200]}...\")\n",
    "    \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test messages\n",
    "test_messages = [\n",
    "    \"What is Python?\",\n",
    "    \"What are its main uses?\",\n",
    "    \"How does it compare to other languages?\",\n",
    "]\n",
    "\n",
    "# Test condense_question mode\n",
    "engine1 = test_chat_mode(\"condense_question\", test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test context mode\n",
    "engine2 = test_chat_mode(\"context\", test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Memory Configuration\n",
    "\n",
    "Control how conversation history is managed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom memory buffer\n",
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    token_limit=3000,  # Maximum tokens to keep in memory\n",
    ")\n",
    "\n",
    "# Create chat engine with custom memory\n",
    "custom_chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\",\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Chat engine with custom memory ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long conversation to test memory\n",
    "conversation = [\n",
    "    \"Tell me about artificial intelligence.\",\n",
    "    \"What about machine learning specifically?\",\n",
    "    \"How does deep learning fit into this?\",\n",
    "    \"What are some practical applications?\",\n",
    "    \"Can you summarize what we've discussed?\",\n",
    "]\n",
    "\n",
    "print(\"Testing long conversation with memory...\\n\")\n",
    "\n",
    "for msg in conversation:\n",
    "    response = custom_chat_engine.chat(msg)\n",
    "    print(f\"User: {msg}\")\n",
    "    print(f\"Assistant: {str(response)[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory status\n",
    "print(f\"Messages in memory: {len(memory.get_all())}\")\n",
    "print(f\"\\nMemory contents:\")\n",
    "for i, msg in enumerate(memory.get_all()):\n",
    "    print(f\"  {i+1}. {msg.role}: {str(msg.content)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Chat\n",
    "\n",
    "Stream chat responses for better user experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming chat engine\n",
    "streaming_chat = index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Streaming chat engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream a response\n",
    "query = \"Explain the key concepts of object-oriented programming.\"\n",
    "\n",
    "print(f\"User: {query}\\n\")\n",
    "print(\"Assistant: \", end=\"\")\n",
    "\n",
    "streaming_response = streaming_chat.stream_chat(query)\n",
    "\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom System Prompt\n",
    "\n",
    "Customize the chat assistant's personality and behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "# Custom system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful AI programming tutor specializing in Python and AI.\n",
    "Your responses should be:\n",
    "- Educational and encouraging\n",
    "- Include code examples when relevant\n",
    "- Explain concepts step by step\n",
    "- Ask clarifying questions if the user's question is unclear\n",
    "\n",
    "If asked about topics outside your knowledge base, politely redirect\n",
    "the conversation back to programming and AI topics.\n",
    "\"\"\"\n",
    "\n",
    "# Create chat engine with custom system prompt\n",
    "tutor_chat = index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\",\n",
    "    system_prompt=system_prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"✓ Tutor chat engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tutor\n",
    "tutor_questions = [\n",
    "    \"I'm new to programming. What should I learn first?\",\n",
    "    \"Can you show me how to write a simple function?\",\n",
    "    \"What's the difference between a list and a tuple?\",\n",
    "]\n",
    "\n",
    "print(\"Programming Tutor Session\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in tutor_questions:\n",
    "    response = tutor_chat.chat(q)\n",
    "    print(f\"\\nStudent: {q}\")\n",
    "    print(f\"\\nTutor: {response}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Complete Chatbot\n",
    "\n",
    "Let's create a reusable chatbot class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Generator\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class ChatMessage:\n",
    "    role: str\n",
    "    content: str\n",
    "    timestamp: datetime\n",
    "\n",
    "class RAGChatbot:\n",
    "    \"\"\"A complete RAG-powered chatbot with conversation management.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        index: VectorStoreIndex,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        chat_mode: str = \"condense_question\",\n",
    "        memory_token_limit: int = 3000,\n",
    "    ):\n",
    "        self.index = index\n",
    "        self.system_prompt = system_prompt\n",
    "        self.chat_mode = chat_mode\n",
    "        self.memory_token_limit = memory_token_limit\n",
    "        \n",
    "        self._init_chat_engine()\n",
    "        self.conversation_log: List[ChatMessage] = []\n",
    "    \n",
    "    def _init_chat_engine(self):\n",
    "        \"\"\"Initialize the chat engine.\"\"\"\n",
    "        memory = ChatMemoryBuffer.from_defaults(\n",
    "            token_limit=self.memory_token_limit\n",
    "        )\n",
    "        \n",
    "        kwargs = {\n",
    "            \"chat_mode\": self.chat_mode,\n",
    "            \"memory\": memory,\n",
    "        }\n",
    "        \n",
    "        if self.system_prompt:\n",
    "            kwargs[\"system_prompt\"] = self.system_prompt\n",
    "        \n",
    "        self.chat_engine = self.index.as_chat_engine(**kwargs)\n",
    "    \n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        # Log user message\n",
    "        self.conversation_log.append(ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=message,\n",
    "            timestamp=datetime.now(),\n",
    "        ))\n",
    "        \n",
    "        # Get response\n",
    "        response = self.chat_engine.chat(message)\n",
    "        response_text = str(response)\n",
    "        \n",
    "        # Log assistant response\n",
    "        self.conversation_log.append(ChatMessage(\n",
    "            role=\"assistant\",\n",
    "            content=response_text,\n",
    "            timestamp=datetime.now(),\n",
    "        ))\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "    def stream_chat(self, message: str) -> Generator[str, None, None]:\n",
    "        \"\"\"Stream a response token by token.\"\"\"\n",
    "        # Create streaming engine temporarily\n",
    "        streaming_engine = self.index.as_chat_engine(\n",
    "            chat_mode=self.chat_mode,\n",
    "            memory=self.chat_engine.memory,\n",
    "            system_prompt=self.system_prompt,\n",
    "            streaming=True,\n",
    "        )\n",
    "        \n",
    "        self.conversation_log.append(ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=message,\n",
    "            timestamp=datetime.now(),\n",
    "        ))\n",
    "        \n",
    "        response = streaming_engine.stream_chat(message)\n",
    "        full_response = \"\"\n",
    "        \n",
    "        for token in response.response_gen:\n",
    "            full_response += token\n",
    "            yield token\n",
    "        \n",
    "        self.conversation_log.append(ChatMessage(\n",
    "            role=\"assistant\",\n",
    "            content=full_response,\n",
    "            timestamp=datetime.now(),\n",
    "        ))\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the conversation.\"\"\"\n",
    "        self.chat_engine.reset()\n",
    "        self.conversation_log.clear()\n",
    "    \n",
    "    def get_history(self) -> List[dict]:\n",
    "        \"\"\"Get conversation history.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"role\": msg.role,\n",
    "                \"content\": msg.content,\n",
    "                \"timestamp\": msg.timestamp.isoformat(),\n",
    "            }\n",
    "            for msg in self.conversation_log\n",
    "        ]\n",
    "    \n",
    "    def export_conversation(self) -> str:\n",
    "        \"\"\"Export conversation as formatted text.\"\"\"\n",
    "        lines = [\"=== Conversation Export ===\", \"\"]\n",
    "        \n",
    "        for msg in self.conversation_log:\n",
    "            role = msg.role.upper()\n",
    "            time = msg.timestamp.strftime(\"%H:%M:%S\")\n",
    "            lines.append(f\"[{time}] {role}:\")\n",
    "            lines.append(msg.content)\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "print(\"✓ RAGChatbot class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and use the chatbot\n",
    "chatbot = RAGChatbot(\n",
    "    index=index,\n",
    "    system_prompt=\"You are a helpful AI assistant that explains technical concepts clearly.\",\n",
    "    chat_mode=\"condense_question\",\n",
    "    memory_token_limit=4000,\n",
    ")\n",
    "\n",
    "print(\"✓ Chatbot initialized!\\n\")\n",
    "\n",
    "# Have a conversation\n",
    "questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How is it different from machine learning?\",\n",
    "    \"What role does Python play in this field?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"You: {q}\")\n",
    "    response = chatbot.chat(q)\n",
    "    print(f\"\\nBot: {response[:300]}...\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export conversation\n",
    "print(chatbot.export_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Context Window Limits\n",
    "\n",
    "When conversations get long, you need to manage the context window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import (\n",
    "    ChatMemoryBuffer,\n",
    "    VectorMemory,\n",
    ")\n",
    "\n",
    "# Vector-based memory for long conversations\n",
    "# Retrieves relevant past messages instead of keeping all\n",
    "vector_memory = VectorMemory.from_defaults(\n",
    "    vector_store=None,  # Uses in-memory store\n",
    "    embed_model=Settings.embed_model,\n",
    "    retriever_kwargs={\"similarity_top_k\": 3},\n",
    ")\n",
    "\n",
    "# Create chat engine with vector memory\n",
    "long_chat = index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\",\n",
    "    memory=vector_memory,\n",
    ")\n",
    "\n",
    "print(\"✓ Chat engine with vector memory ready!\")\n",
    "print(\"(Retrieves relevant past messages instead of keeping all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "You've learned how to build conversational interfaces with LlamaIndex:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Chat Modes** | Different strategies for handling conversation context |\n",
    "| **Memory** | Maintains conversation history across turns |\n",
    "| **Streaming** | Better UX with token-by-token output |\n",
    "| **System Prompt** | Customize assistant personality and behavior |\n",
    "| **Vector Memory** | Efficient handling of long conversations |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use `condense_question`** for most RAG chat applications\n",
    "2. **Set appropriate memory limits** based on your LLM's context window\n",
    "3. **Always stream** for user-facing applications\n",
    "4. **Customize system prompts** for your use case\n",
    "5. **Log conversations** for debugging and improvement\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the Advanced section, we'll explore:\n",
    "- Building agents with tools\n",
    "- Complex workflows\n",
    "- Custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Custom persona**: Create a chat engine with a unique personality\n",
    "\n",
    "2. **Conversation analytics**: Add metrics tracking to the chatbot class\n",
    "\n",
    "3. **Multi-turn evaluation**: Test how well context is maintained over many turns\n",
    "\n",
    "4. **Hybrid memory**: Combine buffer and vector memory strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Build your custom chatbot here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
