{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI API - Middle Level Demo\n",
        "\n",
        "Welcome to the **Middle Level** OpenAI API tutorial! Building on the fundamentals, this notebook covers:\n",
        "\n",
        "1. **Streaming Responses** - Real-time token-by-token output\n",
        "2. **Function Calling / Tools** - Let the model call your functions\n",
        "3. **Structured Outputs** - Get consistent JSON responses\n",
        "4. **Vision Capabilities** - Analyze images with GPT-4o\n",
        "5. **Advanced Conversation Patterns** - Context management and optimization\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Entry Level notebook\n",
        "- Familiarity with basic Chat Completions API\n",
        "- OpenAI API key configured\n",
        "\n",
        "---\n",
        "\n",
        "## Reference Documentation\n",
        "- [Streaming](https://platform.openai.com/docs/api-reference/streaming)\n",
        "- [Function Calling](https://platform.openai.com/docs/guides/function-calling)\n",
        "- [Vision](https://platform.openai.com/docs/guides/vision)\n",
        "- [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ OpenAI client initialized!\n",
            "✓ Using model: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# Setup - Import libraries and initialize client\n",
        "import os\n",
        "import json\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# =============================================================================\n",
        "# GLOBAL CONFIGURATION\n",
        "# =============================================================================\n",
        "# Set the model to use throughout this notebook\n",
        "MODEL = \"gpt-4o-mini\"  # Change to \"gpt-4o\" for more capable model\n",
        "# =============================================================================\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "print(\"✓ OpenAI client initialized!\")\n",
        "print(f\"✓ Using model: {MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Streaming Responses\n",
        "\n",
        "Streaming allows you to receive the response token-by-token as it's generated, rather than waiting for the complete response. This is essential for:\n",
        "- Better user experience (immediate feedback)\n",
        "- Long-form content generation\n",
        "- Chat applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "--------------------------------------------------\n",
            "Lines of code unfold,  \n",
            "Logic dances in the screen,  \n",
            "Dreams in syntax gleam.\n",
            "--------------------------------------------------\n",
            "\n",
            "Full response collected: Lines of code unfold,  \n",
            "Logic dances in the screen,  \n",
            "Dreams in syntax gleam.\n"
          ]
        }
      ],
      "source": [
        "# Basic streaming example\n",
        "print(\"Streaming response:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a haiku about programming.\"}\n",
        "    ],\n",
        "    stream=True  # Enable streaming\n",
        ")\n",
        "\n",
        "# Process the stream\n",
        "full_response = \"\"\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        print(content, end=\"\", flush=True)  # Print each token as it arrives\n",
        "        full_response += content\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(f\"\\nFull response collected: {full_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming with usage tracking:\n",
            "--------------------------------------------------\n",
            "Recursion is a programming technique where a function calls itself to solve a problem by breaking it down into smaller, more manageable subproblems.\n",
            "\n",
            "Usage: 13 prompt + 28 completion = 41 total tokens\n"
          ]
        }
      ],
      "source": [
        "# Streaming with usage statistics\n",
        "# Note: You need to request usage stats explicitly with stream_options\n",
        "print(\"Streaming with usage tracking:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain recursion in one sentence.\"}\n",
        "    ],\n",
        "    stream=True,\n",
        "    stream_options={\"include_usage\": True}  # Request usage stats\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    # Content chunks\n",
        "    if chunk.choices and chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "    \n",
        "    # Final chunk contains usage stats\n",
        "    if chunk.usage:\n",
        "        print(f\"\\n\\nUsage: {chunk.usage.prompt_tokens} prompt + {chunk.usage.completion_tokens} completion = {chunk.usage.total_tokens} total tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Function Calling / Tools\n",
        "\n",
        "Function calling allows the model to intelligently choose to call functions you define. The model doesn't execute code - it returns structured arguments for YOU to execute.\n",
        "\n",
        "**Use cases:**\n",
        "- Fetching real-time data (weather, stocks, databases)\n",
        "- Performing calculations\n",
        "- Interacting with external APIs\n",
        "- Taking actions in your application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tools defined: get_weather, calculate\n"
          ]
        }
      ],
      "source": [
        "# Define your tools (functions the model can call)\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the current weather for a location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"City and state, e.g., San Francisco, CA\"\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"Temperature unit\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calculate\",\n",
        "            \"description\": \"Perform a mathematical calculation\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"expression\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Mathematical expression to evaluate, e.g., '2 + 2 * 3'\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"expression\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Tools defined: get_weather, calculate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function implementations ready!\n"
          ]
        }
      ],
      "source": [
        "# Simulated function implementations\n",
        "def get_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Simulated weather API call\"\"\"\n",
        "    # In real applications, you'd call an actual weather API\n",
        "    weather_data = {\n",
        "        \"San Francisco, CA\": {\"temp\": 65, \"condition\": \"Foggy\"},\n",
        "        \"New York, NY\": {\"temp\": 45, \"condition\": \"Cloudy\"},\n",
        "        \"Miami, FL\": {\"temp\": 82, \"condition\": \"Sunny\"},\n",
        "    }\n",
        "    data = weather_data.get(location, {\"temp\": 70, \"condition\": \"Clear\"})\n",
        "    if unit == \"celsius\":\n",
        "        data[\"temp\"] = round((data[\"temp\"] - 32) * 5/9)\n",
        "    return json.dumps({\"location\": location, \"temperature\": data[\"temp\"], \"unit\": unit, \"condition\": data[\"condition\"]})\n",
        "\n",
        "def calculate(expression):\n",
        "    \"\"\"Safe mathematical calculation\"\"\"\n",
        "    try:\n",
        "        # Warning: eval is used here for demo only. Use a safe parser in production!\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
        "        return json.dumps({\"expression\": expression, \"result\": result})\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "# Map function names to implementations\n",
        "available_functions = {\n",
        "    \"get_weather\": get_weather,\n",
        "    \"calculate\": calculate\n",
        "}\n",
        "\n",
        "print(\"Function implementations ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's response:\n",
            "  Content: None\n",
            "  Tool calls: 2\n"
          ]
        }
      ],
      "source": [
        "# Make a request that triggers function calling\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the weather in San Francisco? Also, what's 15% of 89?\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\"  # Let the model decide when to use tools\n",
        ")\n",
        "\n",
        "# Check if the model wants to call functions\n",
        "assistant_message = response.choices[0].message\n",
        "print(\"Model's response:\")\n",
        "print(f\"  Content: {assistant_message.content}\")\n",
        "print(f\"  Tool calls: {len(assistant_message.tool_calls) if assistant_message.tool_calls else 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Executing: get_weather({'location': 'San Francisco, CA'})\n",
            "Result: {\"location\": \"San Francisco, CA\", \"temperature\": 65, \"unit\": \"fahrenheit\", \"condition\": \"Foggy\"}\n",
            "\n",
            "Executing: calculate({'expression': '0.15 * 89'})\n",
            "Result: {\"expression\": \"0.15 * 89\", \"result\": 13.35}\n",
            "\n",
            "==================================================\n",
            "Final Response:\n",
            "The weather in San Francisco is currently 65°F and foggy. \n",
            "\n",
            "As for your calculation, 15% of 89 is 13.35.\n"
          ]
        }
      ],
      "source": [
        "# Complete function calling flow - execute functions and return results\n",
        "if assistant_message.tool_calls:\n",
        "    # Add assistant's message (with tool calls) to history\n",
        "    messages.append(assistant_message)\n",
        "    \n",
        "    # Process each tool call\n",
        "    for tool_call in assistant_message.tool_calls:\n",
        "        function_name = tool_call.function.name\n",
        "        function_args = json.loads(tool_call.function.arguments)\n",
        "        \n",
        "        print(f\"\\nExecuting: {function_name}({function_args})\")\n",
        "        \n",
        "        # Call the actual function\n",
        "        function_to_call = available_functions[function_name]\n",
        "        function_response = function_to_call(**function_args)\n",
        "        \n",
        "        print(f\"Result: {function_response}\")\n",
        "        \n",
        "        # Add function result to messages\n",
        "        messages.append({\n",
        "            \"role\": \"tool\",\n",
        "            \"tool_call_id\": tool_call.id,\n",
        "            \"content\": function_response\n",
        "        })\n",
        "    \n",
        "    # Get final response with function results\n",
        "    final_response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Final Response:\")\n",
        "    print(final_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Structured Outputs (JSON Mode)\n",
        "\n",
        "Get consistent, parseable JSON responses from the model. This is crucial for:\n",
        "- API integrations\n",
        "- Data extraction\n",
        "- Consistent formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted JSON:\n",
            "{\n",
            "  \"name\": \"John Smith\",\n",
            "  \"age\": 32,\n",
            "  \"occupation\": \"software engineer\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Simple JSON mode\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You extract information and return it as JSON.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Extract the person's name, age, and occupation from: 'John Smith is a 32-year-old software engineer from Seattle.'\"\n",
        "        }\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"}  # Enable JSON mode\n",
        ")\n",
        "\n",
        "result = json.loads(response.choices[0].message.content)\n",
        "print(\"Extracted JSON:\")\n",
        "print(json.dumps(result, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: Maria Garcia\n",
            "Age: 28\n",
            "Occupation: data scientist\n",
            "City: None\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Structured Outputs with JSON Schema (more reliable)\n",
        "# This ensures the output strictly follows your schema\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "\n",
        "class PersonInfo(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    occupation: str\n",
        "    city: Optional[str] = None\n",
        "\n",
        "response = client.beta.chat.completions.parse(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Extract person information from the text.\"\n",
        "        },\n",
        "        # {\n",
        "        #     \"role\": \"user\",\n",
        "        #     \"content\": \"Maria Garcia is a 28-year-old data scientist living in Boston.\"\n",
        "        # }\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Maria Garcia is a 28-year-old data scientist.\"\n",
        "        }\n",
        "    ],\n",
        "    response_format=PersonInfo\n",
        ")\n",
        "\n",
        "person = response.choices[0].message.parsed\n",
        "print(f\"Name: {person.name}\")\n",
        "print(f\"Age: {person.age}\")\n",
        "print(f\"Occupation: {person.occupation}\")\n",
        "print(f\"City: {person.city}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Vision Capabilities\n",
        "\n",
        "GPT-4o and GPT-4o-mini can understand images! You can:\n",
        "- Analyze images\n",
        "- Describe visual content\n",
        "- Extract text from images (OCR)\n",
        "- Answer questions about images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image Analysis:\n",
            "The image features a serene outdoor setting. A wooden boardwalk, which appears to be well-maintained, winds through a lush landscape of tall, green grass. The boardwalk stretches towards the horizon, inviting viewers to explore further into the scene.\n",
            "\n",
            "On either side of the boardwalk, the grass is vibrant and appears to sway gently in the breeze. There are also some bushes and trees in the background, suggesting a diverse ecosystem. The sky above is expansive and painted with a soft gradient of blue, punctuated by scattered fluffy white clouds that create a sense of tranquility. The overall lighting is warm, perhaps indicating either morning or late afternoon, adding to the peaceful ambiance of the setting.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Vision: Analyze an image from URL\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Describe this image in detail. What do you see?\"\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": image_url,\n",
        "                        \"detail\": \"auto\"  # Options: \"low\", \"high\", \"auto\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=300\n",
        ")\n",
        "\n",
        "print(\"Image Analysis:\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base64 encoding function ready for local images!\n",
            "The image features a serene lakeside scene at sunset. There are two wooden benches positioned on a grassy area near the water, with mountains in the background. The sky is painted in vibrant hues of orange, blue, and purple, reflecting on the calm surface of the lake. The overall atmosphere appears peaceful and picturesque.\n"
          ]
        }
      ],
      "source": [
        "# Vision: Analyze a local image (base64 encoded)\n",
        "def encode_image_to_base64(image_path):\n",
        "    \"\"\"Encode a local image file to base64.\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Example usage (uncomment when you have a local image):\n",
        "image_path = \"sunset.jpg\"\n",
        "base64_image = encode_image_to_base64(image_path)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Base64 encoding function ready for local images!\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Advanced Conversation Patterns\n",
        "\n",
        "Managing conversation context effectively is crucial for production applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Messages in context: 4\n",
            "Ready for API call with: [{'role': 'system', 'content': 'You are a tech support assistant.'}, {'role': 'user', 'content': \"My computer won't start\"}, {'role': 'assistant', 'content': \"Let's troubleshoot. Is it plugged in?\"}, {'role': 'user', 'content': 'Yes it is'}]\n"
          ]
        }
      ],
      "source": [
        "# Context Window Management - Sliding window approach\n",
        "class ConversationManager:\n",
        "    def __init__(self, max_messages=20, system_prompt=\"You are a helpful assistant.\"):\n",
        "        self.system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
        "        self.messages = []\n",
        "        self.max_messages = max_messages\n",
        "    \n",
        "    def add_message(self, role, content):\n",
        "        \"\"\"Add a message and trim if needed.\"\"\"\n",
        "        self.messages.append({\"role\": role, \"content\": content})\n",
        "        \n",
        "        # Keep only the last N messages (sliding window)\n",
        "        if len(self.messages) > self.max_messages:\n",
        "            self.messages = self.messages[-self.max_messages:]\n",
        "    \n",
        "    def get_messages(self):\n",
        "        \"\"\"Get full message list for API call.\"\"\"\n",
        "        return [self.system_message] + self.messages\n",
        "    \n",
        "    def summarize_and_reset(self, client):\n",
        "        \"\"\"Summarize conversation and start fresh with context.\"\"\"\n",
        "        if len(self.messages) < 4:\n",
        "            return\n",
        "        \n",
        "        # Ask AI to summarize the conversation\n",
        "        summary_request = self.get_messages() + [\n",
        "            {\"role\": \"user\", \"content\": \"Summarize our conversation so far in 2-3 sentences.\"}\n",
        "        ]\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=summary_request,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        \n",
        "        summary = response.choices[0].message.content\n",
        "        \n",
        "        # Reset with summary as context\n",
        "        self.messages = [\n",
        "            {\"role\": \"assistant\", \"content\": f\"[Previous conversation summary: {summary}]\"}\n",
        "        ]\n",
        "        \n",
        "        return summary\n",
        "\n",
        "# Demo the conversation manager\n",
        "conv = ConversationManager(max_messages=10, system_prompt=\"You are a tech support assistant.\")\n",
        "conv.add_message(\"user\", \"My computer won't start\")\n",
        "conv.add_message(\"assistant\", \"Let's troubleshoot. Is it plugged in?\")\n",
        "conv.add_message(\"user\", \"Yes it is\")\n",
        "\n",
        "print(\"Messages in context:\", len(conv.get_messages()))\n",
        "print(\"Ready for API call with:\", conv.get_messages())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model requested 2 parallel function calls:\n",
            "  - get_weather: {\"location\": \"New York, NY\"}\n",
            "  - get_weather: {\"location\": \"Miami, FL\"}\n"
          ]
        }
      ],
      "source": [
        "# Parallel function calls for efficiency\n",
        "# When a user asks multiple questions, the model can call multiple functions at once\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather in New York and Miami?\"}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        "    parallel_tool_calls=True  # Enable parallel function calls (default)\n",
        ")\n",
        "\n",
        "if response.choices[0].message.tool_calls:\n",
        "    print(f\"Model requested {len(response.choices[0].message.tool_calls)} parallel function calls:\")\n",
        "    for tc in response.choices[0].message.tool_calls:\n",
        "        print(f\"  - {tc.function.name}: {tc.function.arguments}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "You've now learned intermediate OpenAI API techniques:\n",
        "\n",
        "1. **Streaming**: Real-time token-by-token responses for better UX\n",
        "2. **Function Calling**: Let the model invoke your functions with structured arguments\n",
        "3. **Structured Outputs**: Get consistent JSON responses with schemas\n",
        "4. **Vision**: Analyze and understand images\n",
        "5. **Conversation Management**: Handle context windows and message history\n",
        "\n",
        "### Key Takeaways\n",
        "- Streaming is essential for chat applications\n",
        "- Function calling enables powerful integrations with external tools\n",
        "- Structured outputs ensure reliable data extraction\n",
        "- Vision capabilities open up multimodal applications\n",
        "- Proper context management prevents token limit issues\n",
        "\n",
        "### Next Steps\n",
        "Move on to **03_advanced_level_openai_api.ipynb** to learn:\n",
        "- Building AI agents\n",
        "- Embeddings and semantic search\n",
        "- Batch processing\n",
        "- Production patterns and optimization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
