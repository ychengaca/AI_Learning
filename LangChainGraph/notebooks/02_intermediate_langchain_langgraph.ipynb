{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2: Intermediate - LCEL Deep Dive & LangGraph Introduction\n",
        "\n",
        "This notebook takes you deeper into LangChain Expression Language (LCEL) and introduces LangGraph for building stateful agent workflows.\n",
        "\n",
        "## Learning Objectives\n",
        "- Master LCEL: RunnablePassthrough, RunnableParallel, RunnableLambda, branching\n",
        "- Understand LangGraph fundamentals: StateGraph, nodes, edges\n",
        "- Build your first graph-based agent from scratch\n",
        "- Work with conditional edges and cycles\n",
        "- Learn state management in graphs\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Notebook 01 (Entry Level)\n",
        "\n",
        "---\n",
        "\n",
        "**References:**\n",
        "- [LangGraph Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart)\n",
        "- [LangGraph: Build Stateful AI Agents](https://realpython.com/langgraph-python)\n",
        "- [Graph API Overview](https://langchain-ai.github.io/langgraph/how-tos/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for shared config\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import global model configuration\n",
        "from config import (\n",
        "    GPT_MODEL, GEMINI_MODEL,\n",
        "    GPT_MODEL_NAME, GEMINI_MODEL_NAME,\n",
        "    get_model, list_available_models,\n",
        ")\n",
        "\n",
        "print(f\"Using GPT model:    {GPT_MODEL_NAME}\")\n",
        "print(f\"Using Gemini model: {GEMINI_MODEL_NAME}\")\n",
        "print()\n",
        "list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LCEL Advanced Patterns\n",
        "\n",
        "LangChain Expression Language (LCEL) provides composable primitives for building complex chains.\n",
        "\n",
        "### Key Runnables\n",
        "| Runnable | Purpose |\n",
        "|----------|---------|\n",
        "| `RunnablePassthrough` | Pass input through unchanged (or add fields) |\n",
        "| `RunnableParallel` | Run multiple chains in parallel |\n",
        "| `RunnableLambda` | Wrap any Python function as a runnable |\n",
        "| `RunnableBranch` | Conditional routing based on input |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# RunnableParallel: run multiple chains simultaneously\n",
        "prompt_summary = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Summarize the following topic in 1 sentence.\"),\n",
        "    (\"human\", \"{topic}\"),\n",
        "])\n",
        "\n",
        "prompt_keywords = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"List 5 keywords for the following topic, comma-separated.\"),\n",
        "    (\"human\", \"{topic}\"),\n",
        "])\n",
        "\n",
        "# Build parallel chains\n",
        "parallel_chain = RunnableParallel(\n",
        "    summary=prompt_summary | GPT_MODEL | StrOutputParser(),\n",
        "    keywords=prompt_keywords | GEMINI_MODEL | StrOutputParser(),\n",
        ")\n",
        "\n",
        "result = parallel_chain.invoke({\"topic\": \"Quantum Computing\"})\n",
        "print(\"Summary (GPT):\", result[\"summary\"])\n",
        "print(\"\\nKeywords (Gemini):\", result[\"keywords\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunnableLambda: wrap any function as a chain step\n",
        "def word_count(text: str) -> dict:\n",
        "    \"\"\"Count words and return enriched result.\"\"\"\n",
        "    words = text.split()\n",
        "    return {\"text\": text, \"word_count\": len(words), \"char_count\": len(text)}\n",
        "\n",
        "analyze = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Explain the concept briefly.\"),\n",
        "        (\"human\", \"{concept}\"),\n",
        "    ])\n",
        "    | GPT_MODEL\n",
        "    | StrOutputParser()\n",
        "    | RunnableLambda(word_count)\n",
        ")\n",
        "\n",
        "result = analyze.invoke({\"concept\": \"neural networks\"})\n",
        "print(f\"Explanation: {result['text'][:100]}...\")\n",
        "print(f\"Word count: {result['word_count']}\")\n",
        "print(f\"Character count: {result['char_count']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunnablePassthrough with assign: add computed fields to the input\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "enrich_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        upper_topic=lambda x: x[\"topic\"].upper(),\n",
        "        topic_length=lambda x: len(x[\"topic\"]),\n",
        "    )\n",
        ")\n",
        "\n",
        "result = enrich_chain.invoke({\"topic\": \"artificial intelligence\"})\n",
        "print(\"Enriched input:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Introduction to LangGraph\n",
        "\n",
        "LangGraph lets you build **stateful, graph-based** agent workflows. Instead of linear chains, you define:\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **State** | A TypedDict that flows through the graph |\n",
        "| **Nodes** | Functions that transform the state |\n",
        "| **Edges** | Connections between nodes (can be conditional) |\n",
        "| **START / END** | Special nodes marking graph entry/exit |\n",
        "\n",
        "### Why LangGraph over plain LCEL?\n",
        "- Supports **cycles** (agents that loop until done)\n",
        "- Built-in **state management**\n",
        "- **Persistence** for multi-turn conversations\n",
        "- **Human-in-the-loop** capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Step 1: Define the State\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Step 2: Define nodes (functions that process state)\n",
        "def chatbot(state: ChatState):\n",
        "    \"\"\"The chatbot node - calls the LLM with current messages.\"\"\"\n",
        "    return {\"messages\": [GPT_MODEL.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Step 3: Build the graph\n",
        "graph_builder = StateGraph(ChatState)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Step 4: Compile\n",
        "simple_graph = graph_builder.compile()\n",
        "\n",
        "# Step 5: Invoke\n",
        "result = simple_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is LangGraph?\"}]})\n",
        "print(\"Final response:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Graph with Tools and Conditional Edges\n",
        "\n",
        "Now let's build a more realistic graph where the agent can **decide** whether to call tools or respond directly. This creates a **cycle**: the agent loops until it has enough information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import ToolMessage\n",
        "import json\n",
        "\n",
        "# Define tools\n",
        "@tool\n",
        "def search_knowledge(query: str) -> str:\n",
        "    \"\"\"Search an internal knowledge base for information.\"\"\"\n",
        "    knowledge = {\n",
        "        \"langchain\": \"LangChain is a framework for building LLM-powered applications with chains, agents, and memory.\",\n",
        "        \"langgraph\": \"LangGraph extends LangChain with graph-based state machines for complex agent workflows.\",\n",
        "        \"rag\": \"RAG combines retrieval with generation to ground LLM responses in factual data.\",\n",
        "        \"lcel\": \"LCEL (LangChain Expression Language) uses the pipe operator to compose chain components.\",\n",
        "    }\n",
        "    for key, value in knowledge.items():\n",
        "        if key in query.lower():\n",
        "            return value\n",
        "    return f\"No information found for: {query}\"\n",
        "\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"Evaluate a mathematical expression safely.\"\"\"\n",
        "    try:\n",
        "        result = eval(expression, {\"__builtins__\": {}})\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "tools = [search_knowledge, calculate]\n",
        "tool_map = {t.name: t for t in tools}\n",
        "\n",
        "# Bind tools to model\n",
        "model_with_tools = GPT_MODEL.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a full agent graph with conditional routing\n",
        "\n",
        "def agent_node(state: ChatState):\n",
        "    \"\"\"Call the LLM with tools bound.\"\"\"\n",
        "    return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "def tool_node(state: ChatState):\n",
        "    \"\"\"Execute all pending tool calls.\"\"\"\n",
        "    outputs = []\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    for tool_call in last_message.tool_calls:\n",
        "        tool_result = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
        "        outputs.append(\n",
        "            ToolMessage(\n",
        "                content=str(tool_result),\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "                name=tool_call[\"name\"],\n",
        "            )\n",
        "        )\n",
        "    return {\"messages\": outputs}\n",
        "\n",
        "def should_continue(state: ChatState) -> str:\n",
        "    \"\"\"Decide whether to call tools or end.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# Build the graph\n",
        "agent_graph = StateGraph(ChatState)\n",
        "agent_graph.add_node(\"agent\", agent_node)\n",
        "agent_graph.add_node(\"tools\", tool_node)\n",
        "\n",
        "agent_graph.add_edge(START, \"agent\")\n",
        "agent_graph.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "agent_graph.add_edge(\"tools\", \"agent\")  # Loop back after tool execution\n",
        "\n",
        "# Compile\n",
        "compiled_agent = agent_graph.compile()\n",
        "\n",
        "# Test: this will cause the agent to search, then respond\n",
        "result = compiled_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is LCEL? Also calculate 42 * 17.\"}]\n",
        "})\n",
        "\n",
        "print(\"Agent conversation:\")\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
        "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
        "    if content:\n",
        "        print(f\"  [{role}] {content[:150]}\")\n",
        "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"    -> Calling: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Graph with Gemini Model\n",
        "\n",
        "Let's swap the model to Gemini and see how it handles the same graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rebuild with Gemini\n",
        "gemini_with_tools = GEMINI_MODEL.bind_tools(tools)\n",
        "\n",
        "def gemini_agent_node(state: ChatState):\n",
        "    return {\"messages\": [gemini_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "gemini_graph = StateGraph(ChatState)\n",
        "gemini_graph.add_node(\"agent\", gemini_agent_node)\n",
        "gemini_graph.add_node(\"tools\", tool_node)  # Reuse tool_node from above\n",
        "gemini_graph.add_edge(START, \"agent\")\n",
        "gemini_graph.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "gemini_graph.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "compiled_gemini = gemini_graph.compile()\n",
        "\n",
        "result = compiled_gemini.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for info about RAG, then calculate 100 / 4.\"}]\n",
        "})\n",
        "\n",
        "print(\"Gemini agent conversation:\")\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
        "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
        "    if content:\n",
        "        print(f\"  [{role}] {content[:150]}\")\n",
        "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"    -> Calling: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizing Your Graph\n",
        "\n",
        "LangGraph can generate a visual representation of your graph structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the graph as ASCII\n",
        "try:\n",
        "    print(compiled_agent.get_graph().draw_ascii())\n",
        "except Exception as e:\n",
        "    print(f\"ASCII visualization requires 'grandalf' package: {e}\")\n",
        "    print(\"\\nGraph structure:\")\n",
        "    print(\"  START -> agent -> (tools | END)\")\n",
        "    print(\"  tools -> agent (loop back)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| RunnableParallel | Run multiple chains simultaneously |\n",
        "| RunnableLambda | Wrap Python functions as chain steps |\n",
        "| RunnablePassthrough | Pass-through and enrich input data |\n",
        "| StateGraph | Define graph-based agent workflows |\n",
        "| Conditional Edges | Route between nodes based on state |\n",
        "| Tool Integration | Bind tools to models in a graph |\n",
        "| Agent Loop | Create cycles for iterative tool use |\n",
        "\n",
        "**Next:** [03 - Advanced: LangGraph Deep Dive](./03_advanced_langgraph.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
