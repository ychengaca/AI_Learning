{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI API - Advanced Level Demo\n",
        "\n",
        "Welcome to the **Advanced Level** OpenAI API tutorial! This notebook covers production-ready patterns:\n",
        "\n",
        "1. **AI Agents** - Autonomous agents with tool loops\n",
        "2. **Embeddings & Semantic Search** - Vector representations for similarity\n",
        "3. **Batch Processing** - Efficient large-scale API usage\n",
        "4. **Async Operations** - Concurrent API calls\n",
        "5. **Production Patterns** - Error handling, retries, cost optimization\n",
        "6. **Audio Capabilities** - Speech-to-text and text-to-speech\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Entry and Middle Level notebooks\n",
        "- Understanding of function calling and streaming\n",
        "- OpenAI API key configured\n",
        "\n",
        "---\n",
        "\n",
        "## Reference Documentation\n",
        "- [Agents Overview](https://platform.openai.com/docs/guides/agents)\n",
        "- [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
        "- [Batch API](https://platform.openai.com/docs/guides/batch)\n",
        "- [Best Practices](https://platform.openai.com/docs/guides/production-best-practices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ OpenAI clients initialized (sync + async)!\n",
            "✓ Using model: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# Setup - Import all required libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# =============================================================================\n",
        "# GLOBAL CONFIGURATION\n",
        "# =============================================================================\n",
        "# Set the model to use throughout this notebook\n",
        "MODEL = \"gpt-4o-mini\"  # Change to \"gpt-4o\" for more capable model\n",
        "# =============================================================================\n",
        "\n",
        "# Synchronous client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Async client for concurrent operations\n",
        "async_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "print(\"✓ OpenAI clients initialized (sync + async)!\")\n",
        "print(f\"✓ Using model: {MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Building AI Agents\n",
        "\n",
        "An **agent** is an AI system that can:\n",
        "- Use tools to gather information\n",
        "- Make decisions based on results\n",
        "- Loop until the task is complete\n",
        "\n",
        "The key pattern: **Agentic Loop** - repeatedly call the model until it stops requesting tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined 4 tools for the agent\n"
          ]
        }
      ],
      "source": [
        "# Define a comprehensive set of tools for our agent\n",
        "agent_tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_database\",\n",
        "            \"description\": \"Search a product database for items matching a query\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
        "                    \"category\": {\"type\": \"string\", \"enum\": [\"electronics\", \"clothing\", \"books\", \"all\"]},\n",
        "                    \"max_results\": {\"type\": \"integer\", \"description\": \"Maximum results to return\"}\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_product_details\",\n",
        "            \"description\": \"Get detailed information about a specific product\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"product_id\": {\"type\": \"string\", \"description\": \"The product ID\"}\n",
        "                },\n",
        "                \"required\": [\"product_id\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"check_inventory\",\n",
        "            \"description\": \"Check if a product is in stock\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"product_id\": {\"type\": \"string\", \"description\": \"The product ID\"},\n",
        "                    \"quantity\": {\"type\": \"integer\", \"description\": \"Quantity needed\"}\n",
        "                },\n",
        "                \"required\": [\"product_id\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"place_order\",\n",
        "            \"description\": \"Place an order for a product\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"product_id\": {\"type\": \"string\", \"description\": \"The product ID\"},\n",
        "                    \"quantity\": {\"type\": \"integer\", \"description\": \"Quantity to order\"},\n",
        "                    \"shipping_address\": {\"type\": \"string\", \"description\": \"Delivery address\"}\n",
        "                },\n",
        "                \"required\": [\"product_id\", \"quantity\", \"shipping_address\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(agent_tools)} tools for the agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool implementations ready!\n"
          ]
        }
      ],
      "source": [
        "# Simulated tool implementations (in production, these would call real systems)\n",
        "def search_database(query: str, category: str = \"all\", max_results: int = 5) -> str:\n",
        "    \"\"\"Simulated product search.\"\"\"\n",
        "    products = [\n",
        "        {\"id\": \"ELEC001\", \"name\": \"Wireless Headphones\", \"category\": \"electronics\", \"price\": 79.99},\n",
        "        {\"id\": \"ELEC002\", \"name\": \"Bluetooth Speaker\", \"category\": \"electronics\", \"price\": 49.99},\n",
        "        {\"id\": \"BOOK001\", \"name\": \"Python Programming\", \"category\": \"books\", \"price\": 34.99},\n",
        "        {\"id\": \"CLTH001\", \"name\": \"Running Shoes\", \"category\": \"clothing\", \"price\": 89.99},\n",
        "    ]\n",
        "    \n",
        "    # Filter by category and query\n",
        "    results = [p for p in products if category == \"all\" or p[\"category\"] == category]\n",
        "    results = [p for p in results if query.lower() in p[\"name\"].lower()][:max_results]\n",
        "    \n",
        "    return json.dumps({\"results\": results, \"total_found\": len(results)})\n",
        "\n",
        "def get_product_details(product_id: str) -> str:\n",
        "    \"\"\"Get detailed product info.\"\"\"\n",
        "    details = {\n",
        "        \"ELEC001\": {\"name\": \"Wireless Headphones\", \"price\": 79.99, \"description\": \"Premium noise-canceling headphones\", \"rating\": 4.5},\n",
        "        \"ELEC002\": {\"name\": \"Bluetooth Speaker\", \"price\": 49.99, \"description\": \"Portable waterproof speaker\", \"rating\": 4.2},\n",
        "        \"BOOK001\": {\"name\": \"Python Programming\", \"price\": 34.99, \"description\": \"Complete guide to Python\", \"rating\": 4.8},\n",
        "    }\n",
        "    return json.dumps(details.get(product_id, {\"error\": \"Product not found\"}))\n",
        "\n",
        "def check_inventory(product_id: str, quantity: int = 1) -> str:\n",
        "    \"\"\"Check product availability.\"\"\"\n",
        "    inventory = {\"ELEC001\": 50, \"ELEC002\": 0, \"BOOK001\": 100}\n",
        "    stock = inventory.get(product_id, 0)\n",
        "    return json.dumps({\"product_id\": product_id, \"in_stock\": stock >= quantity, \"available\": stock})\n",
        "\n",
        "def place_order(product_id: str, quantity: int, shipping_address: str) -> str:\n",
        "    \"\"\"Place an order (simulated).\"\"\"\n",
        "    order_id = f\"ORD-{int(time.time())}\"\n",
        "    return json.dumps({\"success\": True, \"order_id\": order_id, \"product_id\": product_id, \"quantity\": quantity, \"address\": shipping_address})\n",
        "\n",
        "# Map function names to implementations\n",
        "tool_implementations = {\n",
        "    \"search_database\": search_database,\n",
        "    \"get_product_details\": get_product_details,\n",
        "    \"check_inventory\": check_inventory,\n",
        "    \"place_order\": place_order\n",
        "}\n",
        "\n",
        "print(\"Tool implementations ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent function defined!\n"
          ]
        }
      ],
      "source": [
        "# The Agentic Loop - the core pattern for building agents\n",
        "def run_agent(user_request: str, max_iterations: int = 10, verbose: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Run an agent that can use tools to complete a task.\n",
        "    \n",
        "    The agent loop:\n",
        "    1. Send request to model with tools\n",
        "    2. If model wants to use tools, execute them\n",
        "    3. Add tool results to conversation\n",
        "    4. Repeat until model gives final answer\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a helpful shopping assistant. You can search for products, \n",
        "            get details, check inventory, and place orders. Always verify availability before \n",
        "            placing orders. Be thorough and provide helpful information to the user.\"\"\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": user_request}\n",
        "    ]\n",
        "    \n",
        "    iteration = 0\n",
        "    while iteration < max_iterations:\n",
        "        iteration += 1\n",
        "        if verbose:\n",
        "            print(f\"\\n--- Agent Iteration {iteration} ---\")\n",
        "        \n",
        "        # Call the model\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            tools=agent_tools,\n",
        "            tool_choice=\"auto\"\n",
        "        )\n",
        "        \n",
        "        assistant_message = response.choices[0].message\n",
        "        \n",
        "        # Check if we're done (no tool calls = final answer)\n",
        "        if not assistant_message.tool_calls:\n",
        "            if verbose:\n",
        "                print(\"Agent finished with final response\")\n",
        "            return assistant_message.content\n",
        "        \n",
        "        # Process tool calls\n",
        "        messages.append(assistant_message)\n",
        "        \n",
        "        for tool_call in assistant_message.tool_calls:\n",
        "            func_name = tool_call.function.name\n",
        "            func_args = json.loads(tool_call.function.arguments)\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"  Calling: {func_name}({func_args})\")\n",
        "            \n",
        "            # Execute the function\n",
        "            func_to_call = tool_implementations[func_name]\n",
        "            result = func_to_call(**func_args)\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"  Result: {result[:100]}...\" if len(result) > 100 else f\"  Result: {result}\")\n",
        "            \n",
        "            # Add result to messages\n",
        "            messages.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "                \"content\": result\n",
        "            })\n",
        "    \n",
        "    return \"Agent reached maximum iterations without completing the task.\"\n",
        "\n",
        "print(\"Agent function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Agent Iteration 1 ---\n",
            "  Calling: search_database({'query': 'headphones'})\n",
            "  Result: {\"results\": [{\"id\": \"ELEC001\", \"name\": \"Wireless Headphones\", \"category\": \"electronics\", \"price\": 79...\n",
            "\n",
            "--- Agent Iteration 2 ---\n",
            "  Calling: get_product_details({'product_id': 'ELEC001'})\n",
            "  Result: {\"name\": \"Wireless Headphones\", \"price\": 79.99, \"description\": \"Premium noise-canceling headphones\",...\n",
            "  Calling: check_inventory({'product_id': 'ELEC001'})\n",
            "  Result: {\"product_id\": \"ELEC001\", \"in_stock\": true, \"available\": 50}\n",
            "\n",
            "--- Agent Iteration 3 ---\n",
            "Agent finished with final response\n",
            "\n",
            "============================================================\n",
            "FINAL AGENT RESPONSE:\n",
            "============================================================\n",
            "I found a pair of headphones that might interest you:\n",
            "\n",
            "### Wireless Headphones\n",
            "- **Price:** $79.99\n",
            "- **Description:** Premium noise-canceling headphones\n",
            "- **Rating:** 4.5 out of 5\n",
            "\n",
            "#### Availability\n",
            "These headphones are currently in stock, with 50 units available.\n",
            "\n",
            "Would you like to place an order for them or need more information?\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with a complex multi-step request\n",
        "result = run_agent(\"I'm looking for headphones. Can you find some, check if they're in stock, and tell me about them?\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL AGENT RESPONSE:\")\n",
        "print(\"=\" * 60)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Embeddings & Semantic Search\n",
        "\n",
        "**Embeddings** convert text into numerical vectors that capture semantic meaning. Similar texts have similar vectors, enabling:\n",
        "- Semantic search (find related content)\n",
        "- Clustering and classification\n",
        "- Recommendation systems\n",
        "- RAG (Retrieval-Augmented Generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'The quick brown fox jumps over the lazy dog.'\n",
            "Embedding dimensions: 1536\n",
            "First 10 values: [-0.018449828028678894, -0.007201016414910555, 0.0036607810761779547, -0.05420747399330139, -0.022751403972506523, 0.036975789815187454, 0.029032466933131218, 0.023918794468045235, 0.011191711761057377, -0.020645027980208397]\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings for text\n",
        "def get_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
        "    \"\"\"Get embedding vector for a text string.\"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=model\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Example: Get embedding for a sentence\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "embedding = get_embedding(sample_text)\n",
        "\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"Embedding dimensions: {len(embedding)}\")\n",
        "print(f\"First 10 values: {embedding[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building knowledge base embeddings...\n",
            "Created 6 embeddings\n",
            "\n",
            "Query: 'How can I build AI applications?'\n",
            "\n",
            "Top results:\n",
            "  [0.417] APIs allow different software applications to communicate with each other.\n",
            "  [0.327] Machine learning models can recognize patterns in large datasets.\n",
            "  [0.272] Python is a popular programming language for data science and machine learning.\n"
          ]
        }
      ],
      "source": [
        "# Semantic Search Implementation\n",
        "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# Build a simple knowledge base\n",
        "knowledge_base = [\n",
        "    \"Python is a popular programming language for data science and machine learning.\",\n",
        "    \"JavaScript is primarily used for web development and runs in browsers.\",\n",
        "    \"Machine learning models can recognize patterns in large datasets.\",\n",
        "    \"Cloud computing provides on-demand access to computing resources.\",\n",
        "    \"APIs allow different software applications to communicate with each other.\",\n",
        "    \"Database optimization improves query performance and reduces latency.\",\n",
        "]\n",
        "\n",
        "# Pre-compute embeddings for the knowledge base\n",
        "print(\"Building knowledge base embeddings...\")\n",
        "kb_embeddings = [get_embedding(text) for text in knowledge_base]\n",
        "print(f\"Created {len(kb_embeddings)} embeddings\")\n",
        "\n",
        "def semantic_search(query: str, top_k: int = 3) -> List[tuple]:\n",
        "    \"\"\"Search the knowledge base for relevant content.\"\"\"\n",
        "    query_embedding = get_embedding(query)\n",
        "    \n",
        "    # Calculate similarities\n",
        "    similarities = []\n",
        "    for i, kb_emb in enumerate(kb_embeddings):\n",
        "        sim = cosine_similarity(query_embedding, kb_emb)\n",
        "        similarities.append((sim, knowledge_base[i]))\n",
        "    \n",
        "    # Sort by similarity and return top results\n",
        "    similarities.sort(reverse=True)\n",
        "    return similarities[:top_k]\n",
        "\n",
        "# Test semantic search\n",
        "query = \"How can I build AI applications?\"\n",
        "results = semantic_search(query)\n",
        "\n",
        "print(f\"\\nQuery: '{query}'\")\n",
        "print(\"\\nTop results:\")\n",
        "for score, text in results:\n",
        "    print(f\"  [{score:.3f}] {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG Answer:\n",
            "Based on the context, Python is a popular programming language for machine learning. It is widely used in the field and has many libraries and frameworks that support machine learning tasks.\n"
          ]
        }
      ],
      "source": [
        "# RAG (Retrieval-Augmented Generation) Pattern\n",
        "def rag_query(user_question: str) -> str:\n",
        "    \"\"\"Answer a question using RAG - retrieve relevant context, then generate.\"\"\"\n",
        "    # Step 1: Retrieve relevant context\n",
        "    relevant_docs = semantic_search(user_question, top_k=2)\n",
        "    context = \"\\n\".join([doc for _, doc in relevant_docs])\n",
        "    \n",
        "    # Step 2: Generate answer with context\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"Answer the user's question based on the following context. \n",
        "                If the context doesn't contain relevant information, say so.\n",
        "                \n",
        "                Context:\n",
        "                {context}\"\"\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": user_question}\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test RAG\n",
        "answer = rag_query(\"What programming language should I learn for machine learning?\")\n",
        "print(\"RAG Answer:\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Async Operations for Concurrency\n",
        "\n",
        "Use async operations to make multiple API calls concurrently, dramatically improving throughput for batch operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 5 prompts concurrently...\n",
            "Completed in 1.22 seconds\n",
            "\n",
            "Results:\n",
            "  [0] What is 2+2?... -> 2 + 2 equals 4....\n",
            "  [1] Name a color.... -> Azure....\n",
            "  [2] What is the capital of France?... -> The capital of France is Paris....\n",
            "  [3] Name a fruit.... -> Apple....\n",
            "  [4] What day comes after Monday?... -> The day that comes after Monday is Tuesday....\n"
          ]
        }
      ],
      "source": [
        "# Async API calls for concurrent processing\n",
        "async def async_chat_completion(prompt: str, request_id: int) -> dict:\n",
        "    \"\"\"Make an async chat completion request.\"\"\"\n",
        "    response = await async_client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return {\n",
        "        \"id\": request_id,\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response.choices[0].message.content\n",
        "    }\n",
        "\n",
        "async def process_batch_async(prompts: List[str]) -> List[dict]:\n",
        "    \"\"\"Process multiple prompts concurrently.\"\"\"\n",
        "    tasks = [\n",
        "        async_chat_completion(prompt, i) \n",
        "        for i, prompt in enumerate(prompts)\n",
        "    ]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return results\n",
        "\n",
        "# Test async batch processing\n",
        "test_prompts = [\n",
        "    \"What is 2+2?\",\n",
        "    \"Name a color.\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Name a fruit.\",\n",
        "    \"What day comes after Monday?\"\n",
        "]\n",
        "\n",
        "print(f\"Processing {len(test_prompts)} prompts concurrently...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Run async function (use await in Jupyter, asyncio.run() in scripts)\n",
        "results = await process_batch_async(test_prompts)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Completed in {elapsed:.2f} seconds\")\n",
        "print(\"\\nResults:\")\n",
        "for r in results:\n",
        "    print(f\"  [{r['id']}] {r['prompt'][:30]}... -> {r['response'][:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Production Patterns\n",
        "\n",
        "Essential patterns for production-grade applications: retries, rate limiting, cost tracking, and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: The speed of light in a vacuum is approximately \\( 299,792,458 \\) meters per second, commonly rounded to \\( 300,000 \\) kilometers per second (or about \\( 186,282 \\) miles per second). This constant is denoted by the symbol \\( c \\) and is a fundamental constant in physics, forming the basis of Einstein's theory of relativity.\n",
            "\n",
            "API Usage Summary:\n",
            "  Requests: 1 (0 failed)\n",
            "  Prompt tokens: 14\n",
            "  Completion tokens: 80\n",
            "  Estimated cost: $0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Production-ready API wrapper with retries and cost tracking\n",
        "from openai import APIError, RateLimitError, APIConnectionError\n",
        "import random\n",
        "\n",
        "@dataclass\n",
        "class APIUsageTracker:\n",
        "    \"\"\"Track API usage and costs.\"\"\"\n",
        "    total_prompt_tokens: int = 0\n",
        "    total_completion_tokens: int = 0\n",
        "    total_requests: int = 0\n",
        "    failed_requests: int = 0\n",
        "    \n",
        "    # Approximate costs per 1M tokens (update based on current pricing)\n",
        "    COST_PER_1M_PROMPT = 0.15  # for gpt-4o-mini\n",
        "    COST_PER_1M_COMPLETION = 0.60\n",
        "    \n",
        "    def add_usage(self, prompt_tokens: int, completion_tokens: int):\n",
        "        self.total_prompt_tokens += prompt_tokens\n",
        "        self.total_completion_tokens += completion_tokens\n",
        "        self.total_requests += 1\n",
        "    \n",
        "    def add_failure(self):\n",
        "        self.failed_requests += 1\n",
        "    \n",
        "    @property\n",
        "    def estimated_cost(self) -> float:\n",
        "        prompt_cost = (self.total_prompt_tokens / 1_000_000) * self.COST_PER_1M_PROMPT\n",
        "        completion_cost = (self.total_completion_tokens / 1_000_000) * self.COST_PER_1M_COMPLETION\n",
        "        return prompt_cost + completion_cost\n",
        "    \n",
        "    def summary(self) -> str:\n",
        "        return f\"\"\"\n",
        "API Usage Summary:\n",
        "  Requests: {self.total_requests} ({self.failed_requests} failed)\n",
        "  Prompt tokens: {self.total_prompt_tokens:,}\n",
        "  Completion tokens: {self.total_completion_tokens:,}\n",
        "  Estimated cost: ${self.estimated_cost:.4f}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize tracker\n",
        "usage_tracker = APIUsageTracker()\n",
        "\n",
        "def robust_chat_completion(\n",
        "    messages: List[dict],\n",
        "    max_retries: int = 3,\n",
        "    base_delay: float = 1.0,\n",
        "    track_usage: bool = True\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Production-ready chat completion with:\n",
        "    - Exponential backoff retry\n",
        "    - Rate limit handling\n",
        "    - Usage tracking\n",
        "    - Error logging\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=messages\n",
        "            )\n",
        "            \n",
        "            # Track usage\n",
        "            if track_usage and response.usage:\n",
        "                usage_tracker.add_usage(\n",
        "                    response.usage.prompt_tokens,\n",
        "                    response.usage.completion_tokens\n",
        "                )\n",
        "            \n",
        "            return response.choices[0].message.content\n",
        "            \n",
        "        except RateLimitError as e:\n",
        "            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
        "            print(f\"Rate limited. Waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}\")\n",
        "            time.sleep(delay)\n",
        "            \n",
        "        except APIConnectionError as e:\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"Connection error. Waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}\")\n",
        "            time.sleep(delay)\n",
        "            \n",
        "        except APIError as e:\n",
        "            print(f\"API error: {e}\")\n",
        "            usage_tracker.add_failure()\n",
        "            return None\n",
        "    \n",
        "    usage_tracker.add_failure()\n",
        "    return None\n",
        "\n",
        "# Test the robust function\n",
        "result = robust_chat_completion([\n",
        "    {\"role\": \"user\", \"content\": \"What is the speed of light?\"}\n",
        "])\n",
        "print(\"Response:\", result)\n",
        "print(usage_tracker.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing with rate limiting...\n",
            "  Count to 3 -> 1, 2, 3....\n",
            "  Name a planet -> Mars....\n",
            "  What color is grass? -> Grass is typically green, although its color can v...\n"
          ]
        }
      ],
      "source": [
        "# Rate Limiter for controlled throughput\n",
        "class RateLimiter:\n",
        "    \"\"\"Simple rate limiter using token bucket algorithm.\"\"\"\n",
        "    def __init__(self, requests_per_minute: int = 60):\n",
        "        self.requests_per_minute = requests_per_minute\n",
        "        self.min_interval = 60.0 / requests_per_minute\n",
        "        self.last_request_time = 0\n",
        "    \n",
        "    def wait(self):\n",
        "        \"\"\"Wait if necessary to respect rate limits.\"\"\"\n",
        "        elapsed = time.time() - self.last_request_time\n",
        "        if elapsed < self.min_interval:\n",
        "            time.sleep(self.min_interval - elapsed)\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "# Initialize rate limiter (adjust based on your tier)\n",
        "rate_limiter = RateLimiter(requests_per_minute=60)\n",
        "\n",
        "def rate_limited_completion(messages: List[dict]) -> str:\n",
        "    \"\"\"Make a rate-limited API call.\"\"\"\n",
        "    rate_limiter.wait()\n",
        "    return robust_chat_completion(messages)\n",
        "\n",
        "# Process multiple requests with rate limiting\n",
        "prompts = [\"Count to 3\", \"Name a planet\", \"What color is grass?\"]\n",
        "print(\"Processing with rate limiting...\")\n",
        "for prompt in prompts:\n",
        "    result = rate_limited_completion([{\"role\": \"user\", \"content\": prompt}])\n",
        "    print(f\"  {prompt} -> {result[:50] if result else 'Failed'}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Audio Capabilities\n",
        "\n",
        "OpenAI provides both **Text-to-Speech (TTS)** and **Speech-to-Text (STT/Whisper)** APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TTS function defined!\n",
            "Available voices: alloy, echo, fable, onyx, nova, shimmer\n"
          ]
        }
      ],
      "source": [
        "# Text-to-Speech (TTS)\n",
        "def text_to_speech(text: str, output_file: str = \"output.mp3\", voice: str = \"alloy\"):\n",
        "    \"\"\"\n",
        "    Convert text to speech.\n",
        "    \n",
        "    Available voices: alloy, echo, fable, onyx, nova, shimmer\n",
        "    \"\"\"\n",
        "    response = client.audio.speech.create(\n",
        "        model=\"tts-1\",  # or \"tts-1-hd\" for higher quality\n",
        "        voice=voice,\n",
        "        input=text\n",
        "    )\n",
        "    \n",
        "    # Save to file\n",
        "    response.stream_to_file(output_file)\n",
        "    return output_file\n",
        "\n",
        "# Generate speech (uncomment to run)\n",
        "# output = text_to_speech(\n",
        "#     \"Hello! This is a demonstration of OpenAI's text to speech capabilities.\",\n",
        "#     \"demo_speech.mp3\",\n",
        "#     voice=\"nova\"\n",
        "# )\n",
        "# print(f\"Audio saved to: {output}\")\n",
        "\n",
        "print(\"TTS function defined!\")\n",
        "print(\"Available voices: alloy, echo, fable, onyx, nova, shimmer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STT (Whisper) function defined!\n",
            "Supported formats: mp3, mp4, mpeg, mpga, m4a, wav, webm\n"
          ]
        }
      ],
      "source": [
        "# Speech-to-Text (Whisper)\n",
        "def speech_to_text(audio_file: str, language: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Transcribe audio to text using Whisper.\n",
        "    \n",
        "    Supports: mp3, mp4, mpeg, mpga, m4a, wav, webm\n",
        "    \"\"\"\n",
        "    with open(audio_file, \"rb\") as f:\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=f,\n",
        "            language=language,  # Optional: specify language code (e.g., \"en\", \"es\")\n",
        "            response_format=\"verbose_json\"  # Get detailed output with timestamps\n",
        "        )\n",
        "    return transcript\n",
        "\n",
        "# Transcribe audio (uncomment when you have an audio file)\n",
        "# result = speech_to_text(\"your_audio.mp3\")\n",
        "# print(f\"Transcription: {result.text}\")\n",
        "# print(f\"Language: {result.language}\")\n",
        "# print(f\"Duration: {result.duration}s\")\n",
        "\n",
        "print(\"STT (Whisper) function defined!\")\n",
        "print(\"Supported formats: mp3, mp4, mpeg, mpga, m4a, wav, webm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Batch API (For Large-Scale Processing)\n",
        "\n",
        "The Batch API is designed for large-scale, non-time-sensitive workloads with 50% cost savings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch submitted: batch_6988eaf2cd508190a0dd7023047fedac\n",
            "Status: validating\n",
            "Batch API functions defined!\n",
            "Use for large-scale processing with 50% cost savings\n"
          ]
        }
      ],
      "source": [
        "# Batch API - For processing large numbers of requests asynchronously\n",
        "# Batch API provides 50% cost savings for non-time-sensitive workloads\n",
        "\n",
        "def create_batch_file(requests: List[dict], filename: str = \"batch_requests.jsonl\"):\n",
        "    \"\"\"Create a JSONL file for batch processing.\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for i, req in enumerate(requests):\n",
        "            batch_request = {\n",
        "                \"custom_id\": f\"request-{i}\",\n",
        "                \"method\": \"POST\",\n",
        "                \"url\": \"/v1/chat/completions\",\n",
        "                \"body\": {\n",
        "                    \"model\": MODEL,\n",
        "                    \"messages\": req[\"messages\"],\n",
        "                    \"max_tokens\": req.get(\"max_tokens\", 100)\n",
        "                }\n",
        "            }\n",
        "            f.write(json.dumps(batch_request) + \"\\n\")\n",
        "    return filename\n",
        "\n",
        "def submit_batch(filename: str):\n",
        "    \"\"\"Submit a batch job to OpenAI.\"\"\"\n",
        "    # Upload the file\n",
        "    with open(filename, \"rb\") as f:\n",
        "        batch_file = client.files.create(file=f, purpose=\"batch\")\n",
        "    \n",
        "    # Create the batch\n",
        "    batch = client.batches.create(\n",
        "        input_file_id=batch_file.id,\n",
        "        endpoint=\"/v1/chat/completions\",\n",
        "        completion_window=\"24h\"\n",
        "    )\n",
        "    \n",
        "    return batch\n",
        "\n",
        "# Example batch requests (uncomment to run)\n",
        "batch_requests = [\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 1+1?\"}]},\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]},\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 3+3?\"}]},\n",
        "]\n",
        "\n",
        "filename = create_batch_file(batch_requests)\n",
        "batch = submit_batch(filename)\n",
        "print(f\"Batch submitted: {batch.id}\")\n",
        "print(f\"Status: {batch.status}\")\n",
        "\n",
        "print(\"Batch API functions defined!\")\n",
        "print(\"Use for large-scale processing with 50% cost savings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SESSION SUMMARY\n",
            "============================================================\n",
            "\n",
            "API Usage Summary:\n",
            "  Requests: 1 (0 failed)\n",
            "  Prompt tokens: 14\n",
            "  Completion tokens: 80\n",
            "  Estimated cost: $0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Final usage summary\n",
        "print(\"=\" * 60)\n",
        "print(\"SESSION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(usage_tracker.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "Congratulations! You've completed the Advanced Level tutorial and learned:\n",
        "\n",
        "1. **AI Agents**: Building autonomous agents with agentic loops that use tools iteratively\n",
        "2. **Embeddings & RAG**: Semantic search, vector similarity, and retrieval-augmented generation\n",
        "3. **Async Operations**: Concurrent API calls for improved throughput\n",
        "4. **Production Patterns**: Retries, rate limiting, cost tracking, and error handling\n",
        "5. **Audio APIs**: Text-to-speech and speech-to-text capabilities\n",
        "6. **Batch API**: Large-scale processing with cost savings\n",
        "\n",
        "### Best Practices Checklist\n",
        "\n",
        "- Always implement retry logic with exponential backoff\n",
        "- Track token usage and costs in production\n",
        "- Use async for concurrent operations\n",
        "- Pre-compute embeddings for static content\n",
        "- Use Batch API for bulk processing (50% savings)\n",
        "- Handle rate limits gracefully\n",
        "\n",
        "### Further Learning\n",
        "- OpenAI Cookbook: https://cookbook.openai.com/\n",
        "- API Reference: https://platform.openai.com/docs/api-reference\n",
        "- Best Practices: https://platform.openai.com/docs/guides/production-best-practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
