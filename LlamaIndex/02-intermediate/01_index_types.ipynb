{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Index Types in LlamaIndex\n",
    "\n",
    "LlamaIndex offers multiple index types, each optimized for different use cases. Understanding when to use each type is crucial for building effective RAG systems.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the different index types and their trade-offs\n",
    "2. Know when to use each index type\n",
    "3. Be able to combine multiple indexes\n",
    "4. Implement hybrid retrieval strategies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Index Types\n",
    "\n",
    "| Index Type | Description | Best For |\n",
    "|------------|-------------|----------|\n",
    "| **VectorStoreIndex** | Semantic similarity search | General Q&A, semantic matching |\n",
    "| **SummaryIndex** | Sequential traversal of all nodes | Summarization, complete coverage |\n",
    "| **TreeIndex** | Hierarchical tree structure | Large documents, drilling down |\n",
    "| **KeywordTableIndex** | Keyword-based retrieval | Exact term matching |\n",
    "| **KnowledgeGraphIndex** | Graph-based relationships | Entity relationships |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    TreeIndex,\n",
    "    KeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Configure\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"../data/sample_docs\").load_data()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VectorStoreIndex (Default)\n",
    "\n",
    "The most common index type. Uses embeddings for semantic similarity search.\n",
    "\n",
    "**How it works:**\n",
    "1. Documents are chunked into nodes\n",
    "2. Each node is converted to an embedding vector\n",
    "3. Queries are also converted to embeddings\n",
    "4. Most similar vectors are retrieved\n",
    "\n",
    "**Best for:**\n",
    "- Semantic similarity questions\n",
    "- When exact keywords might not be in the document\n",
    "- General-purpose Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VectorStoreIndex\n",
    "print(\"Building VectorStoreIndex...\")\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "print(\"✓ VectorStoreIndex ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic query - works well with paraphrasing\n",
    "query = \"How do computers learn from data?\"  # No exact match, but semantically similar\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "response = vector_query_engine.query(query)\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SummaryIndex (formerly ListIndex)\n",
    "\n",
    "Processes ALL nodes sequentially. Guarantees complete coverage but is slower and more expensive.\n",
    "\n",
    "**How it works:**\n",
    "1. Documents are chunked into nodes\n",
    "2. At query time, ALL nodes are sent to the LLM\n",
    "3. Response is synthesized from all content\n",
    "\n",
    "**Best for:**\n",
    "- Summarization tasks\n",
    "- When you need to consider ALL information\n",
    "- Small document collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SummaryIndex\n",
    "print(\"Building SummaryIndex...\")\n",
    "summary_index = SummaryIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",  # Good for summarization\n",
    ")\n",
    "\n",
    "print(\"✓ SummaryIndex ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization query - SummaryIndex excels at this\n",
    "query = \"Provide a comprehensive summary of all the topics covered in these documents.\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\n(This may take a moment as it processes all nodes...)\\n\")\n",
    "\n",
    "response = summary_query_engine.query(query)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TreeIndex\n",
    "\n",
    "Builds a hierarchical tree structure for multi-level summarization and retrieval.\n",
    "\n",
    "**How it works:**\n",
    "1. Documents are chunked into leaf nodes\n",
    "2. Leaf nodes are summarized into parent nodes\n",
    "3. Process repeats until reaching the root\n",
    "4. Query traverses the tree to find relevant branches\n",
    "\n",
    "**Best for:**\n",
    "- Large documents where you want to drill down\n",
    "- Hierarchical content (books, manuals)\n",
    "- When you need both overview and detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TreeIndex\n",
    "print(\"Building TreeIndex...\")\n",
    "print(\"(This creates a hierarchical summary structure)\\n\")\n",
    "\n",
    "tree_index = TreeIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    "    num_children=3,  # Number of children per parent node\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "tree_query_engine = tree_index.as_query_engine(\n",
    "    child_branch_factor=2,  # How many branches to explore\n",
    ")\n",
    "\n",
    "print(\"\\n✓ TreeIndex ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree traversal query\n",
    "query = \"What are the main programming concepts discussed?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "response = tree_query_engine.query(query)\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KeywordTableIndex\n",
    "\n",
    "Uses keyword extraction for retrieval. Good when exact terminology matters.\n",
    "\n",
    "**How it works:**\n",
    "1. Keywords are extracted from each node (using LLM)\n",
    "2. A keyword-to-node mapping table is created\n",
    "3. Query keywords are extracted and matched\n",
    "4. Matching nodes are retrieved\n",
    "\n",
    "**Best for:**\n",
    "- Technical documentation with specific terms\n",
    "- When exact keyword matching is important\n",
    "- Glossary-style lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KeywordTableIndex\n",
    "print(\"Building KeywordTableIndex...\")\n",
    "print(\"(This extracts keywords from each node)\\n\")\n",
    "\n",
    "keyword_index = KeywordTableIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "keyword_query_engine = keyword_index.as_query_engine()\n",
    "\n",
    "print(\"\\n✓ KeywordTableIndex ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword-based query - exact terms work best\n",
    "query = \"What is supervised learning?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "response = keyword_query_engine.query(query)\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Index Types\n",
    "\n",
    "Let's compare how different indexes respond to the same query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison query\n",
    "test_query = \"What are the applications of artificial intelligence?\"\n",
    "\n",
    "indexes = {\n",
    "    \"VectorStoreIndex\": vector_query_engine,\n",
    "    \"SummaryIndex\": summary_query_engine,\n",
    "    \"TreeIndex\": tree_query_engine,\n",
    "    \"KeywordTableIndex\": keyword_query_engine,\n",
    "}\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, engine in indexes.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 70)\n",
    "    try:\n",
    "        response = engine.query(test_query)\n",
    "        print(f\"{str(response)[:400]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combining Multiple Indexes (ComposableGraph)\n",
    "\n",
    "You can combine multiple indexes for more sophisticated retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# Create tools for each index type\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=\"Useful for semantic similarity questions about AI, Python, and general topics. \"\n",
    "                \"Use when the question requires understanding concepts or finding related information.\",\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=\"Useful for summarization requests or when you need a comprehensive overview \"\n",
    "                \"of all the content. Use for questions like 'summarize' or 'give an overview'.\",\n",
    ")\n",
    "\n",
    "keyword_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=keyword_query_engine,\n",
    "    description=\"Useful for looking up specific technical terms or definitions. \"\n",
    "                \"Use when the question asks about a specific concept by name.\",\n",
    ")\n",
    "\n",
    "print(\"✓ Query tools created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Router Query Engine that automatically selects the best tool\n",
    "router_query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        vector_tool,\n",
    "        summary_tool,\n",
    "        keyword_tool,\n",
    "    ],\n",
    "    verbose=True,  # Show which tool was selected\n",
    ")\n",
    "\n",
    "print(\"✓ Router Query Engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the router with different query types\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",  # Should use keyword or vector\n",
    "    \"Summarize all the content about Python.\",  # Should use summary\n",
    "    \"How are neural networks related to deep learning?\",  # Should use vector\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"-\" * 70)\n",
    "    response = router_query_engine.query(query)\n",
    "    print(f\"\\nResponse: {str(response)[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sub-Question Query Engine\n",
    "\n",
    "For complex questions that span multiple topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "# Create sub-question query engine\n",
    "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        QueryEngineTool.from_defaults(\n",
    "            query_engine=vector_query_engine,\n",
    "            description=\"Contains information about AI, machine learning, Python programming, and technology.\",\n",
    "        ),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Sub-Question Query Engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex question that benefits from decomposition\n",
    "complex_query = \"Compare and contrast Python programming with AI development practices.\"\n",
    "\n",
    "print(f\"Complex Query: {complex_query}\")\n",
    "print(\"\\n(Watch how the query is decomposed into sub-questions...)\\n\")\n",
    "\n",
    "response = sub_question_engine.query(complex_query)\n",
    "print(f\"\\nFinal Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Index Selection Guidelines\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Scenario | Recommended Index | Why |\n",
    "|----------|------------------|-----|\n",
    "| General Q&A | VectorStoreIndex | Semantic matching handles paraphrasing |\n",
    "| Summarization | SummaryIndex | Considers all content |\n",
    "| Large documents | TreeIndex | Efficient hierarchical navigation |\n",
    "| Technical docs | KeywordTableIndex | Exact term matching |\n",
    "| Multi-topic | Router/SubQuestion | Combines strengths |\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "| Index | Build Time | Query Time | Cost |\n",
    "|-------|------------|------------|------|\n",
    "| VectorStoreIndex | Moderate (embeddings) | Fast | Low |\n",
    "| SummaryIndex | Fast (no embeddings) | Slow (all nodes) | High |\n",
    "| TreeIndex | Slow (summarization) | Moderate | Moderate |\n",
    "| KeywordTableIndex | Moderate (extraction) | Fast | Low |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "You've learned about different index types in LlamaIndex:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **VectorStoreIndex** is the default and works well for most semantic search use cases\n",
    "2. **SummaryIndex** guarantees complete coverage but is slower and more expensive\n",
    "3. **TreeIndex** provides hierarchical navigation for large documents\n",
    "4. **KeywordTableIndex** excels at exact term matching\n",
    "5. **Router** and **SubQuestion** engines combine multiple indexes intelligently\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Start with VectorStoreIndex for most use cases\n",
    "2. Use SummaryIndex only when complete coverage is essential\n",
    "3. Consider combining indexes for complex applications\n",
    "4. Profile performance based on your specific needs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore custom retrievers and reranking strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Index comparison**: Run 10 different queries and note which index performs best for each\n",
    "\n",
    "2. **Custom router**: Create a router with custom selection logic\n",
    "\n",
    "3. **Hybrid approach**: Combine vector and keyword search results\n",
    "\n",
    "4. **Performance test**: Measure build time and query time for each index type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Try different index configurations here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
