{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Level 1: Entry Level - LangChain Basics\n",
        "\n",
        "Welcome to the LangChain learning path! This notebook covers the fundamental concepts you need to get started with LangChain.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand what LangChain is and why it's useful\n",
        "- Learn to work with Chat Models (GPT-4o-mini & Gemini Flash)\n",
        "- Master message types and prompt templates\n",
        "- Build your first simple chain\n",
        "- Introduction to tools and agents\n",
        "p\n",
        "## Prerequisites\n",
        "- Python 3.10+\n",
        "- API keys for OpenAI and Google (set as environment variables)\n",
        "\n",
        "---\n",
        "\n",
        "**Documentation Reference:** [LangChain Quickstart](https://docs.langchain.com/oss/python/langchain/quickstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "We use a shared `config.py` that loads API keys from `.env` and defines global model instances so every notebook uses the same models consistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install langchain langchain-core langchain-openai langchain-google-genai langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPT model:    gpt-4o-mini\n",
            "Using Gemini model: gemini-3-flash-preview\n",
            "\n",
            "Available Models:\n",
            "-------------------------------------------------------\n",
            "  gpt-4o-mini          -> ChatOpenAI(gpt-4o-mini)\n",
            "  gemini-3-flash-preview -> ChatGoogleGenerativeAI(gemini-3-flash-preview)\n",
            "-------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for shared config\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import global model configuration\n",
        "from config import (\n",
        "    GPT_MODEL, GEMINI_MODEL,\n",
        "    GPT_MODEL_NAME, GEMINI_MODEL_NAME,\n",
        "    get_model, list_available_models,\n",
        ")\n",
        "\n",
        "print(f\"Using GPT model:    {GPT_MODEL_NAME}\")\n",
        "print(f\"Using Gemini model: {GEMINI_MODEL_NAME}\")\n",
        "print()\n",
        "list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding Chat Models\n",
        "\n",
        "Chat models are the foundation of LangChain. They take **messages** as input and return AI-generated responses.\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Chat Model** | Wrapper around an LLM that speaks in messages |\n",
        "| **invoke()** | Send a prompt and get a single response |\n",
        "| **AIMessage** | The response object returned by the model |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-4o-mini says:\n",
            "LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs) by providing tools for chaining together various components like prompts, memory, and APIs.\n",
            "\n",
            "Token usage: {'completion_tokens': 37, 'prompt_tokens': 15, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Simple invocation with GPT-4o-mini\n",
        "response = GPT_MODEL.invoke(\"What is LangChain in one sentence?\")\n",
        "print(\"GPT-4o-mini says:\")\n",
        "print(response.content)\n",
        "print(f\"\\nToken usage: {response.response_metadata.get('token_usage', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini Flash says:\n",
            "[{'type': 'text', 'text': 'LangChain is an open-source framework designed to simplify the creation of applications powered by large language models (LLMs) by providing tools to \"chain\" together different components like data sources, prompts, and APIs.', 'extras': {'signature': 'EugJCuUJAb4+9vvSmF8jkr28lNXAPfbCMvcjYpmQMhWgrGnHX/KV2+djKAyyBjysAFLA3QyjtpIsjaTNR3pIrS4Z0aTHP/GLyd1SkcW2kEgoObVWGlen1GvgW9OpA0dL8Ho7pibO3No1cojtC6ujKp1sZUblOkFw3q8Yv/FT8vDbAeBmsZttos0irNIT80HYYOzgtCeupM6EhquisYrMJ0aweD5kbYh6EU0Gjo3AnuzlPcaOFa8TiNZ9esYj/e1t7ig0eNGcpG5PLIRfH1ZwQvRGTH0l0GU3MBrhhGUmYaADjitIJMHTfG5ptYUT6qhA/LUarNCLhqOn0DmkFPPrjo9aiLd6QkDLBpRK3o1nXVTboVZ78Fdc7t1KVaYdn3/AM867SVR+lZIBxVHsfmCUEOnHtoLW3ZiZr+eTHZnF1KdsTFuw0LWGl+THgiTHCBb2iP1T94jKONn+g8iTHWJaDQ3Uzog6OY1QWN/+HGXXBMFM3DUC0U0a55PQtXg13HU59gL3KFxJzhUvHhRyCFL8zUi9RsKKfssHfPNoeuP5Tckk7VJoXk4MuQw7mt2DLlt8mbk8u71B2aw5MN19w14G4pQAOwvTLxTVKPj7W9Ag79RcWjL1vVmbYnsUNYlbyDNIzUpjlRDNmajbxFQE6nTekaUUUHkMFD/+qs9hhUf36SX6hRxXrAzWayWW+HJ7XPF4pW5MazSbThUAkAwRSKcboUjLcXHM+uN6gI2aDRptYKwf8jZljiGN2SWfmU1CtKrfbaxidbjfNtEIoWplM9/jpip0pR8j8K+TiCUE3OWdkbWmDbKlXM2aVfzt+9CySUx6wbnOPaFhNcYsW0t0yPfzgniHYkP/7uVkV5fRNl4SnQ9Nws0Wl65TPXcD+4QrSubuPKVmy1j+ZMds4peDw24a9Fy5t+gJf593eQuvSEV1lNn2a9+nyDkiNnegB/zkgeyA4AGwBG9LQ5wANIr2PTWMn6FPClkaNgyG7DPP9hAtlrTQ+Q6B9Xt/s2G0FAR1YT5AA3vXD7uNb+qzOyuxUMEh4vrszJwjAf//SVIn1JJcFdAmZKOloY+iki/nuvDWuEErCUUqMAjmF91D6oKF8zB5W5U+n1lJxWhR1KgFnb7dxCxs7P/vPu4SoK2+rDXaBZ+7WVYNTuw1yZ6Na7auNcX67U58PeIS9tmLIVPK2MIGrz1ZCsjcMd4FM8XOD2PpON4T8UXCTC1gggyE8oHyACmWYNVXF0E+AOLY2riK+SgPEYJWBbtdB+juWD3REuLxHMk/DOD1JY2dqU0AB4dsXZWuKnmHe1WDPZplk7udo2mH5FKWSxRtQVg9rsa0EELVEtqYn9opUu3TGRnUTb7IsdPAk+AWmQAV29UJcSSpT5R+5zuYx3GdH8W5EY2AVhcVng0c/1QbGF0OeZWwGhJX+akQQ0ZG+oP2a4j4PSRKl3J51oJoKyscXhsz5KU4ZLSXoR/jBHhN24SyZemnF+7OWKR60pQFZJD5Pp/0bK7yTngX6jSDYtpNpmxz9dLQGdWQEM99Ggrmt5DItc+F7TDldkVI7/+gd09rBw3N8BWyM0pRNtu0HYUQe2xSlEZtU60wPj9CgoQPWFtQAvhxsqi71MDmJ6McTOEt4b1sjaak/HckC7yry16uImJ833ee0yT9t5VjopH54OtNi1fqjus='}}]\n"
          ]
        }
      ],
      "source": [
        "# Same question with Gemini Flash\n",
        "response = GEMINI_MODEL.invoke(\"What is LangChain in one sentence?\")\n",
        "print(\"Gemini Flash says:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Message Types\n",
        "\n",
        "LangChain uses structured message types to communicate with models:\n",
        "\n",
        "| Message Type | Role | Purpose |\n",
        "|-------------|------|---------|\n",
        "| `SystemMessage` | system | Sets behavior/personality of the AI |\n",
        "| `HumanMessage` | user | The user's input |\n",
        "| `AIMessage` | assistant | The AI's response |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-4o-mini:\n",
            " A list comprehension is a concise way to create lists in Python. It allows you to generate a new list by applying an expression to each item in an existing iterable (like a list or range) and can include an optional condition to filter items.\n",
            "\n",
            "The basic syntax is:\n",
            "\n",
            "```python\n",
            "new_list = [expression for item in iterable if condition]\n",
            "```\n",
            "\n",
            "For example, to create a list of squares of even numbers from 0 to 9:\n",
            "\n",
            "```python\n",
            "squares = [x**2 for x in range(10) if x % 2 == 0]\n",
            "```\n",
            "\n",
            "This results in `squares` being `[0, 4, 16, 36, 64]`.\n",
            "\n",
            "============================================================\n",
            "\n",
            "Gemini Flash:\n",
            " [{'type': 'text', 'text': 'A **list comprehension** is a concise way to create a new list by iterating over an existing iterable (like a list, range, or string) in a single line of code.\\n\\n### Syntax\\n`[expression for item in iterable if condition]`\\n\\n---\\n\\n### Example: Squaring Even Numbers\\nSuppose you want to create a list of squares for even numbers between 0 and 4.\\n\\n**Traditional `for` loop:**\\n```python\\nsquares = []\\nfor x in range(5):\\n    if x % 2 == 0:\\n        squares.append(x**2)\\n# Result: [0, 4, 16]\\n```\\n\\n**List Comprehension:**\\n```python\\nsquares = [x**2 for x in range(5) if x % 2 == 0]\\n# Result: [0, 4, 16]\\n```\\n\\n### Why use it?\\n1.  **Brevity:** It turns multiple lines of code into one.\\n2.  **Readability:** It clearly shows the intent (what the list contains) rather than the mechanics of how to build it.\\n3.  **Performance:** It is often slightly faster than a standard `for` loop using `.append()`.', 'extras': {'signature': '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'}}]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Using structured messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful Python tutor. Be concise.\"),\n",
        "    HumanMessage(content=\"What is a list comprehension?\"),\n",
        "]\n",
        "\n",
        "# Try with GPT\n",
        "response = GPT_MODEL.invoke(messages)\n",
        "print(\"GPT-4o-mini:\\n\", response.content)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Try with Gemini\n",
        "response = GEMINI_MODEL.invoke(messages)\n",
        "print(\"Gemini Flash:\\n\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are plenty of delicious ingredients you can add to scrambled eggs to enhance their flavor and texture! Here are some ideas:\n",
            "\n",
            "1. **Cheese**: Add shredded cheese like cheddar, feta, or goat cheese for creaminess and flavor.\n",
            "2. **Herbs**: Fresh herbs like chives, parsley, dill, or cilantro can add a fresh taste.\n",
            "3. **Vegetables**: SautÃ©ed onions, bell peppers, spinach, tomatoes, or mushrooms can add nutrition and flavor.\n",
            "4. **Meats**: Cooked bacon, ham, or sausage can add heartiness.\n",
            "5. **Cream or Milk**: A splash of cream or milk can make the eggs creamier.\n",
            "6. **Spices**: A pinch of salt, pepper, paprika, or even a dash of hot sauce can enhance the flavor.\n",
            "7. **Avocado**: Adding diced avocado can give a creamy texture and healthy fats.\n",
            "8. **Salsa**: A spoonful of salsa can add a zesty kick.\n",
            "\n",
            "Feel free to mix and match based on your preferences! Enjoy your scrambled eggs!\n"
          ]
        }
      ],
      "source": [
        "# Multi-turn conversation\n",
        "conversation = [\n",
        "    SystemMessage(content=\"You are a friendly cooking assistant.\"),\n",
        "    HumanMessage(content=\"How do I make scrambled eggs?\"),\n",
        "    AIMessage(content=\"Crack 2-3 eggs into a bowl, whisk, cook on medium heat with butter, stir gently until just set.\"),\n",
        "    HumanMessage(content=\"What can I add to make them better?\"),\n",
        "]\n",
        "\n",
        "response = GPT_MODEL.invoke(conversation)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prompt Templates\n",
        "\n",
        "Prompt templates let you create reusable, parameterized prompts. They separate the **structure** from the **data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted messages:\n",
            "  [system]: You are an expert in Python programming. Answer in beginner-friendly style.\n",
            "  [human]: What is a decorator?\n",
            "\n",
            "--- Model Response ---\n",
            "A decorator in Python is a special type of function that allows you to modify or enhance the behavior of another function or method. You can think of a decorator as a wrapper that adds some functionality to an existing function without changing its code.\n",
            "\n",
            "### How Decorators Work\n",
            "\n",
            "1. **Function as First-Class Citizens**: In Python, functions are first-class citizens, which means you can pass them around as arguments, return them from other functions, and assign them to variables.\n",
            "\n",
            "2. **Defining a Decorator**: A decorator is usually defined as a function that takes another function as an argument and returns a new function that adds some kind of functionality.\n",
            "\n",
            "3. **Using a Decorator**: You can apply a decorator to a function using the `@decorator_name` syntax just above the function definition.\n",
            "\n",
            "### Example of a Simple Decorator\n",
            "\n",
            "Let's look at a simple example to understand how decorators work:\n",
            "\n",
            "```python\n",
            "def my_decorator(func):\n",
            "    def wrapper():\n",
            "        print(\"Something is happening before the function is called.\")\n",
            "        func()  # Call the original function\n",
            "        print(\"Something is happening after the function is called.\")\n",
            "    return wrapper\n",
            "\n",
            "@my_decorator\n",
            "def say_hello():\n",
            "    print(\"Hello!\")\n",
            "\n",
            "# When you call say_hello, it will actually call the wrapper function\n",
            "say_hello()\n",
            "```\n",
            "\n",
            "### What Happens Here?\n",
            "\n",
            "1. **Defining the Decorator**: We define a decorator called `my_decorator` that takes a function `func` as an argument.\n",
            "\n",
            "2. **Creating a Wrapper**: Inside `my_decorator`, we define a new function called `wrapper` that adds some behavior before and after calling the original function.\n",
            "\n",
            "3. **Applying the Decorator**: We use the `@my_decorator` syntax above the `say_hello` function. This means that `say_hello` is now replaced by the `wrapper` function returned by `my_decorator`.\n",
            "\n",
            "4. **Calling the Function**: When we call `say_hello()`, it actually calls the `wrapper` function, which adds the extra behavior and then calls the original `say_hello` function.\n",
            "\n",
            "### Output\n",
            "\n",
            "When you run the code, the output will be:\n",
            "\n",
            "```\n",
            "Something is happening before the function is called.\n",
            "Hello!\n",
            "Something is happening after the function is called.\n",
            "```\n",
            "\n",
            "### Summary\n",
            "\n",
            "Decorators are a powerful feature in Python that allow you to modify the behavior of functions in a clean and readable way. They are commonly used for logging, enforcing access control, instrumentation, and more.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a reusable prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert in {topic}. Answer in {style} style.\"),\n",
        "    (\"human\", \"{question}\"),\n",
        "])\n",
        "\n",
        "# Fill in the template\n",
        "formatted = prompt.invoke({\n",
        "    \"topic\": \"Python programming\",\n",
        "    \"style\": \"beginner-friendly\",\n",
        "    \"question\": \"What is a decorator?\",\n",
        "})\n",
        "\n",
        "print(\"Formatted messages:\")\n",
        "for msg in formatted.messages:\n",
        "    print(f\"  [{msg.type}]: {msg.content}\")\n",
        "\n",
        "print(\"\\n--- Model Response ---\")\n",
        "response = GPT_MODEL.invoke(formatted)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Chains with LCEL (LangChain Expression Language)\n",
        "\n",
        "LCEL lets you compose components using the **pipe operator** `|`. This is the modern way to build chains.\n",
        "\n",
        "```\n",
        "prompt | model | output_parser\n",
        "```\n",
        "\n",
        "Each component receives the output of the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT chain result:\n",
            " Imagine you're on a foggy mountain and you want to find the lowest point in the valley. You can't see far ahead, so you decide to feel the ground around you. You take a small step in the direction where the ground slopes down the most. \n",
            "\n",
            "After each step, you check again to see which way is downhill and take another step. You keep doing this until you canâ€™t go any lower. \n",
            "\n",
            "In machine learning, gradient descent is like that process. The \"mountain\" represents the error or loss of a model, and the \"lowest point\" is where the model performs best. By taking small steps (adjusting the model's parameters) in the direction that reduces the error the most, we gradually improve the model until we reach a point where it can't get any better.\n",
            "\n",
            "============================================================\n",
            "\n",
            "Gemini chain result:\n",
            " Imagine you are a hiker caught in a **thick, heavy fog** at the top of a mountain. You want to get down to the village at the very bottom of the valley, but you canâ€™t see more than a few inches in front of your face.\n",
            "\n",
            "How do you get down safely? You use **Gradient Descent.**\n",
            "\n",
            "Here is how it works:\n",
            "\n",
            "### 1. Feel the Slope (The Gradient)\n",
            "Since you can't see the path, you use your feet to feel the ground around you. You shuffle your boots to find which direction slopes **downward** the steepest. In machine learning, this \"steepness\" is the **Gradient**. It tells the computer which way to move to reduce its mistakes.\n",
            "\n",
            "### 2. Take a Step (The Update)\n",
            "Once you know which way is down, you take a single step in that direction. By doing this, you are now slightly lower than you were a moment ago. In a computer model, this is like tweaking the settings (parameters) just a little bit to make the math more accurate.\n",
            "\n",
            "### 3. Decide How Big to Step (The Learning Rate)\n",
            "This is the most important part. \n",
            "*   **If you take giant leaps:** You might accidentally leap right over the valley and land on the side of another mountain. Youâ€™ll keep bouncing back and forth and never reach the bottom.\n",
            "*   **If you take tiny baby steps:** You will eventually reach the bottom, but it will take you three days and youâ€™ll run out of snacks. \n",
            "\n",
            "In machine learning, we call this step size the **Learning Rate**. Finding the \"just right\" step size is the secret to a good model.\n",
            "\n",
            "### 4. Repeat Until Flat (Convergence)\n",
            "You keep feeling the slope and taking steps. Eventually, you reach a spot where every direction you feel with your feet seems to go *up*. Congratulations! Youâ€™ve reached the bottom of the valley. \n",
            "\n",
            "### Summary for your \"Machine Learning\" brain:\n",
            "*   **The Mountain:** The \"Cost Function\" (a map of all the mistakes the model is making).\n",
            "*   **The Bottom:** The \"Minimum Error\" (the perfect version of your model).\n",
            "*   **The Foggy Hiker:** The \"Algorithm\" trying to learn.\n",
            "*   **The Slope:** The \"Gradient\" (the direction of the steepest descent).\n",
            "*   **The Step Size:** The \"Learning Rate.\"\n",
            "\n",
            "**Gradient Descent is simply the process of \"stepping downhill\" until the mistakes are as small as possible.**\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Build a simple chain: prompt -> model -> string output\n",
        "chain_gpt = prompt | GPT_MODEL | StrOutputParser()\n",
        "chain_gemini = prompt | GEMINI_MODEL | StrOutputParser()\n",
        "\n",
        "# Invoke the chain with GPT\n",
        "result = chain_gpt.invoke({\n",
        "    \"topic\": \"machine learning\",\n",
        "    \"style\": \"simple analogy\",\n",
        "    \"question\": \"What is gradient descent?\",\n",
        "})\n",
        "print(\"GPT chain result:\\n\", result)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Same chain with Gemini\n",
        "result = chain_gemini.invoke({\n",
        "    \"topic\": \"machine learning\",\n",
        "    \"style\": \"simple analogy\",\n",
        "    \"question\": \"What is gradient descent?\",\n",
        "})\n",
        "print(\"Gemini chain result:\\n\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Tools and Basic Agents\n",
        "\n",
        "**Tools** are functions that an AI model can call. An **agent** decides which tools to use and when.\n",
        "\n",
        "LangGraph provides `create_react_agent` - the simplest way to build a tool-using agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined 3 tools:\n",
            "  - add: Add two numbers together.\n",
            "  - multiply: Multiply two numbers together.\n",
            "  - get_weather: Get the current weather for a city (mock data).\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# Define simple tools\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers together.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers together.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the current weather for a city (mock data).\"\"\"\n",
        "    weather_data = {\n",
        "        \"new york\": \"72F, Sunny\",\n",
        "        \"london\": \"58F, Cloudy\",\n",
        "        \"tokyo\": \"80F, Humid\",\n",
        "    }\n",
        "    return weather_data.get(city.lower(), f\"Weather data not available for {city}\")\n",
        "\n",
        "tools = [add, multiply, get_weather]\n",
        "print(f\"Defined {len(tools)} tools:\")\n",
        "for t in tools:\n",
        "    print(f\"  - {t.name}: {t.description}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content: \n",
            "Tool calls: [{'name': 'multiply', 'args': {'a': 15, 'b': 23}, 'id': 'call_sVRKTyVGE1CXSEmnCfRgBg5G', 'type': 'tool_call'}]\n"
          ]
        }
      ],
      "source": [
        "# Bind tools to a model and see tool calls\n",
        "model_with_tools = GPT_MODEL.bind_tools(tools)\n",
        "\n",
        "response = model_with_tools.invoke(\"What is 15 multiplied by 23?\")\n",
        "print(\"Content:\", response.content)\n",
        "print(\"Tool calls:\", response.tool_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Yunpeng.Cheng\\AppData\\Local\\Temp\\ipykernel_32712\\2815162728.py:5: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(GPT_MODEL, tools)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human] What is 15 + 23, and then multiply the result by 2?\n",
            "[ai] \n",
            "  -> Tool call: add({'a': 15, 'b': 23})\n",
            "  -> Tool call: multiply({'a': 15, 'b': 2})\n",
            "[tool] 38\n",
            "[tool] 30\n",
            "[ai] \n",
            "  -> Tool call: multiply({'a': 38, 'b': 2})\n",
            "[tool] 76\n",
            "[ai] The result of \\( 15 + 23 \\) is \\( 38 \\). When you multiply that by \\( 2 \\), you get \\( 76 \\).\n"
          ]
        }
      ],
      "source": [
        "# Build a simple ReAct agent using LangGraph's prebuilt helper\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Create an agent with GPT and our tools\n",
        "agent = create_react_agent(GPT_MODEL, tools)\n",
        "\n",
        "# Run the agent\n",
        "result = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 15 + 23, and then multiply the result by 2?\"}]}\n",
        ")\n",
        "\n",
        "# Print the conversation\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, 'type') else 'unknown'\n",
        "    content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "    print(f\"[{role}] {content[:200]}\")\n",
        "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"  -> Tool call: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Yunpeng.Cheng\\AppData\\Local\\Temp\\ipykernel_32712\\2053153068.py:2: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  gemini_agent = create_react_agent(GEMINI_MODEL, tools)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human] What's the weather in Tokyo? Also add 100 and 250.\n",
            "[ai] []\n",
            "  -> Tool call: get_weather({'city': 'Tokyo'})\n",
            "  -> Tool call: add({'b': 250, 'a': 100})\n",
            "[tool] 80F, Humid\n",
            "[tool] 350\n",
            "[ai] [{'type': 'text', 'text': 'The weather in Tokyo is currently 80Â°F and humid. Also, 100 plus 250 is 350.'}]\n"
          ]
        }
      ],
      "source": [
        "# Same agent but with Gemini\n",
        "gemini_agent = create_react_agent(GEMINI_MODEL, tools)\n",
        "\n",
        "result = gemini_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo? Also add 100 and 250.\"}]}\n",
        ")\n",
        "\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, 'type') else 'unknown'\n",
        "    content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "    print(f\"[{role}] {content[:200]}\")\n",
        "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"  -> Tool call: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Streaming Responses\n",
        "\n",
        "Instead of waiting for the full response, you can **stream** tokens as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming from GPT-4o-mini:\n",
            "Lines of code align,  \n",
            "Logic dances in the dark,  \n",
            "Dreams in syntax bloom.\n",
            "\n",
            "Streaming from Gemini Flash:\n",
            "[{'type': 'text', 'text': 'Logic in the lines,\\nSearching for a missing mark,\\nNow the program runs.', 'index': 0}][{'type': 'text', 'text': '', 'extras': {'signature': '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'}, 'index': 0}][]\n"
          ]
        }
      ],
      "source": [
        "# Stream output token by token\n",
        "print(\"Streaming from GPT-4o-mini:\")\n",
        "for chunk in GPT_MODEL.stream(\"Write a haiku about programming.\"):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Streaming from Gemini Flash:\")\n",
        "for chunk in GEMINI_MODEL.stream(\"Write a haiku about programming.\"):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Structured Output\n",
        "\n",
        "Force models to return data in a specific structure using Pydantic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Inception\n",
            "Rating: 9/10\n",
            "Summary: A mind-bending thriller that explores the complexities of dreams and reality through a heist narrative.\n",
            "Recommended: Yes\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class MovieReview(BaseModel):\n",
        "    \"\"\"A structured movie review.\"\"\"\n",
        "    title: str = Field(description=\"The movie title\")\n",
        "    rating: int = Field(description=\"Rating from 1-10\")\n",
        "    summary: str = Field(description=\"One-sentence summary\")\n",
        "    recommended: bool = Field(description=\"Whether you'd recommend it\")\n",
        "\n",
        "# Get structured output from GPT\n",
        "structured_model = GPT_MODEL.with_structured_output(MovieReview)\n",
        "review = structured_model.invoke(\"Review the movie 'Inception'\")\n",
        "\n",
        "print(f\"Title: {review.title}\")\n",
        "print(f\"Rating: {review.rating}/10\")\n",
        "print(f\"Summary: {review.summary}\")\n",
        "print(f\"Recommended: {'Yes' if review.recommended else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| Chat Models | How to invoke GPT-4o-mini and Gemini Flash |\n",
        "| Messages | SystemMessage, HumanMessage, AIMessage |\n",
        "| Prompt Templates | Reusable, parameterized prompts |\n",
        "| LCEL Chains | Composing components with the pipe operator |\n",
        "| Tools | Defining functions that models can call |\n",
        "| Agents | Using `create_react_agent` for tool-using agents |\n",
        "| Streaming | Token-by-token output |\n",
        "| Structured Output | Forcing models to return Pydantic objects |\n",
        "\n",
        "**Next:** [02 - Intermediate: LCEL Deep Dive & LangGraph Introduction](./02_intermediate_langchain_langgraph.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "my",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
