{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LlamaIndex\n",
    "\n",
    "Welcome to the first notebook in our LlamaIndex learning series! This notebook will introduce you to the fundamentals of LlamaIndex and help you understand its core concepts.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand what LlamaIndex is and why it's useful\n",
    "2. Set up your development environment\n",
    "3. Load your first document\n",
    "4. Create a simple index\n",
    "5. Query your data using natural language\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is LlamaIndex?\n",
    "\n",
    "**LlamaIndex** (formerly GPT Index) is a data framework designed to connect Large Language Models (LLMs) with external data sources. It provides:\n",
    "\n",
    "### Key Capabilities\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Data Connectors** | Load data from 100+ sources (PDFs, databases, APIs, etc.) |\n",
    "| **Data Indexes** | Structure your data for efficient LLM consumption |\n",
    "| **Query Engines** | Natural language interface to your data |\n",
    "| **Agents** | Autonomous AI that can use tools and make decisions |\n",
    "| **Workflows** | Complex multi-step orchestration |\n",
    "\n",
    "### Why LlamaIndex?\n",
    "\n",
    "LLMs like GPT-4, Claude, and Llama have a knowledge cutoff date and don't know about:\n",
    "- Your private documents\n",
    "- Recent events after their training\n",
    "- Domain-specific information\n",
    "\n",
    "**LlamaIndex bridges this gap** by enabling RAG (Retrieval-Augmented Generation):\n",
    "\n",
    "```\n",
    "User Query → Retrieve Relevant Context → Augment LLM Prompt → Generate Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Let's set up our environment with the necessary imports and API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle async in Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set (don't print the actual key!)\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"✓ OpenAI API key is configured\")\n",
    "else:\n",
    "    print(\"✗ OpenAI API key not found. Please set it in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    Document,\n",
    ")\n",
    "\n",
    "# LLM and Embedding imports\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Global Settings\n",
    "\n",
    "LlamaIndex uses a `Settings` object for global configuration. This includes:\n",
    "- Which LLM to use for generation\n",
    "- Which embedding model to use for vector representations\n",
    "- Chunk sizes and other parameters\n",
    "\n",
    "### Understanding LLMs vs Embeddings\n",
    "\n",
    "| Component | Purpose | Example |\n",
    "|-----------|---------|--------|\n",
    "| **LLM** | Generate human-like text responses | GPT-4, Claude, Llama |\n",
    "| **Embedding Model** | Convert text to numerical vectors for similarity search | text-embedding-3-small |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LLM (Language Model)\n",
    "# gpt-4o-mini is cost-effective for learning; use gpt-4o for production\n",
    "Settings.llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,  # Lower = more deterministic, Higher = more creative\n",
    ")\n",
    "\n",
    "# Configure the Embedding Model\n",
    "# This converts text into numerical vectors for semantic search\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "print(\"✓ Settings configured!\")\n",
    "print(f\"  LLM: {Settings.llm.model}\")\n",
    "print(f\"  Embedding: {Settings.embed_model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading Documents\n",
    "\n",
    "LlamaIndex provides multiple ways to load data:\n",
    "\n",
    "### Method 1: Direct Document Creation\n",
    "Create documents directly from strings - useful for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Create documents directly from text\n",
    "doc1 = Document(\n",
    "    text=\"LlamaIndex is a data framework for LLM applications. \"\n",
    "         \"It helps connect custom data sources to large language models.\",\n",
    "    metadata={\"source\": \"manual\", \"topic\": \"introduction\"}\n",
    ")\n",
    "\n",
    "doc2 = Document(\n",
    "    text=\"RAG stands for Retrieval-Augmented Generation. \"\n",
    "         \"It combines retrieval of relevant documents with LLM generation \"\n",
    "         \"to produce more accurate and contextual responses.\",\n",
    "    metadata={\"source\": \"manual\", \"topic\": \"rag\"}\n",
    ")\n",
    "\n",
    "documents_manual = [doc1, doc2]\n",
    "print(f\"Created {len(documents_manual)} documents manually\")\n",
    "\n",
    "# Inspect document structure\n",
    "print(f\"\\nDocument 1 preview:\")\n",
    "print(f\"  Text: {doc1.text[:50]}...\")\n",
    "print(f\"  Metadata: {doc1.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: SimpleDirectoryReader\n",
    "Load documents from files in a directory - the most common approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load from files using SimpleDirectoryReader\n",
    "# This automatically handles different file types (txt, pdf, docx, etc.)\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"../data/sample_docs\",\n",
    "    recursive=True,  # Include subdirectories\n",
    ")\n",
    "\n",
    "documents_from_files = reader.load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents_from_files)} documents from files\")\n",
    "for i, doc in enumerate(documents_from_files):\n",
    "    print(f\"  Document {i+1}: {doc.metadata.get('file_name', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating an Index\n",
    "\n",
    "An **Index** is the core data structure in LlamaIndex. It:\n",
    "1. Chunks documents into smaller pieces\n",
    "2. Converts chunks to embeddings (vectors)\n",
    "3. Stores vectors for efficient similarity search\n",
    "\n",
    "The most common index type is `VectorStoreIndex`:\n",
    "\n",
    "```\n",
    "Documents → Chunks (Nodes) → Embeddings → Vector Store\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VectorStoreIndex from our documents\n",
    "# This process:\n",
    "# 1. Splits documents into chunks (nodes)\n",
    "# 2. Generates embeddings for each chunk\n",
    "# 3. Stores in an in-memory vector store\n",
    "\n",
    "print(\"Creating index... (this may take a moment)\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents_from_files,\n",
    "    show_progress=True  # Show progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Index created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Nodes (Chunks)\n",
    "\n",
    "Documents are split into **Nodes** for more granular retrieval. Let's examine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the underlying nodes (chunks)\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "# Get nodes from the index's document store\n",
    "docstore = index.docstore\n",
    "nodes = list(docstore.docs.values())\n",
    "\n",
    "print(f\"Total nodes created: {len(nodes)}\")\n",
    "print(\"\\n--- Sample Node ---\")\n",
    "if nodes:\n",
    "    sample_node = nodes[0]\n",
    "    print(f\"Node ID: {sample_node.node_id[:20]}...\")\n",
    "    print(f\"Text preview: {sample_node.text[:200]}...\")\n",
    "    print(f\"Metadata: {sample_node.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Querying Your Data\n",
    "\n",
    "Now for the exciting part - querying your data using natural language!\n",
    "\n",
    "A **Query Engine** provides a simple interface:\n",
    "1. Takes your question\n",
    "2. Finds relevant chunks using similarity search\n",
    "3. Sends chunks + question to the LLM\n",
    "4. Returns a synthesized response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine from the index\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,  # Return top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"✓ Query engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ask our first question!\n",
    "query = \"What is artificial intelligence?\"\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(f\"Answer:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask more questions!\n",
    "questions = [\n",
    "    \"What are the main types of machine learning?\",\n",
    "    \"How is Python used in AI development?\",\n",
    "    \"What are the ethical considerations of AI?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(\"-\" * 60)\n",
    "    response = query_engine.query(q)\n",
    "    print(f\"A: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding the Response\n",
    "\n",
    "The response object contains more than just the text. Let's explore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a query and examine the full response\n",
    "query = \"What are the applications of deep learning?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(\"=== Response Analysis ===\")\n",
    "print(f\"\\n1. Response Text:\\n{response}\")\n",
    "\n",
    "print(f\"\\n2. Source Nodes (chunks used to generate response):\")\n",
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(f\"\\n   Source {i+1}:\")\n",
    "    print(f\"   - Score: {node.score:.4f}\")\n",
    "    print(f\"   - File: {node.metadata.get('file_name', 'N/A')}\")\n",
    "    print(f\"   - Text preview: {node.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving and Loading Indexes\n",
    "\n",
    "Creating indexes can be time-consuming for large datasets. You can save them to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index to disk\n",
    "PERSIST_DIR = \"./storage/introduction_index\"\n",
    "\n",
    "index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "print(f\"✓ Index saved to {PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the index from disk (useful when restarting your notebook)\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# Rebuild storage context from persisted data\n",
    "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "\n",
    "# Load the index\n",
    "loaded_index = load_index_from_storage(storage_context)\n",
    "print(\"✓ Index loaded from disk!\")\n",
    "\n",
    "# Verify it works\n",
    "loaded_query_engine = loaded_index.as_query_engine()\n",
    "test_response = loaded_query_engine.query(\"What is Python?\")\n",
    "print(f\"\\nTest query response: {str(test_response)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Congratulations! You've completed the introduction to LlamaIndex. Let's recap what you learned:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Document** | Container for your raw data with metadata |\n",
    "| **Node** | A chunk of a document, the basic retrieval unit |\n",
    "| **Index** | Data structure for efficient similarity search |\n",
    "| **Query Engine** | Natural language interface to query your data |\n",
    "| **Embedding** | Numerical vector representation of text |\n",
    "\n",
    "### The Basic RAG Flow\n",
    "\n",
    "```\n",
    "1. LOAD:    Documents from files/APIs/databases\n",
    "2. INDEX:   Create vector embeddings, store in index\n",
    "3. QUERY:   Find relevant chunks, generate response with LLM\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook (`02_simple_rag.ipynb`), we'll:\n",
    "- Build a complete RAG pipeline\n",
    "- Customize chunking strategies\n",
    "- Work with different document types\n",
    "- Understand retrieval in more depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "1. **Create your own documents**: Add more text files to the `data/sample_docs` folder and re-run the indexing\n",
    "\n",
    "2. **Experiment with questions**: Try asking different types of questions (factual, analytical, comparative)\n",
    "\n",
    "3. **Change the LLM**: Modify the temperature setting and observe how responses change\n",
    "\n",
    "4. **Examine source nodes**: For each query, look at which chunks were retrieved and their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space - try your own code here!\n",
    "\n",
    "# Example: Try a different question\n",
    "my_question = \"Your question here\"\n",
    "# my_response = query_engine.query(my_question)\n",
    "# print(my_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
