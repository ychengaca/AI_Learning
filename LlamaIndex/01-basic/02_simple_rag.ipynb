{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple RAG Pipeline\n",
    "\n",
    "In this notebook, we'll build a complete Retrieval-Augmented Generation (RAG) pipeline from scratch. You'll learn how to customize each component for your specific needs.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the complete RAG architecture\n",
    "2. Customize chunking strategies (node parsing)\n",
    "3. Configure retrieval parameters\n",
    "4. Understand different response synthesis modes\n",
    "5. Build a reusable RAG pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG Architecture Deep Dive\n",
    "\n",
    "RAG consists of three main phases:\n",
    "\n",
    "### Phase 1: Indexing (Offline)\n",
    "```\n",
    "Documents → Chunking → Embedding → Vector Store\n",
    "```\n",
    "\n",
    "### Phase 2: Retrieval (At Query Time)\n",
    "```\n",
    "Query → Query Embedding → Similarity Search → Top-K Relevant Chunks\n",
    "```\n",
    "\n",
    "### Phase 3: Generation (At Query Time)\n",
    "```\n",
    "Query + Retrieved Chunks → LLM → Response\n",
    "```\n",
    "\n",
    "Let's implement each phase with full control over the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Core imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    Document,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    TokenTextSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    SummaryExtractor,\n",
    ")\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "print(\"✓ Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure settings\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"✓ Settings configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading\n",
    "\n",
    "Let's load documents and explore their structure before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"../data/sample_docs\",\n",
    "    filename_as_id=True,  # Use filename as document ID\n",
    ").load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\\n\")\n",
    "\n",
    "# Examine document structure\n",
    "for doc in documents:\n",
    "    print(f\"Document: {doc.metadata.get('file_name', 'Unknown')}\")\n",
    "    print(f\"  - Characters: {len(doc.text):,}\")\n",
    "    print(f\"  - Words (approx): {len(doc.text.split()):,}\")\n",
    "    print(f\"  - Metadata keys: {list(doc.metadata.keys())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunking Strategies (Node Parsing)\n",
    "\n",
    "Chunking is **critical** for RAG performance. Poor chunking leads to:\n",
    "- Lost context (chunks too small)\n",
    "- Irrelevant retrieval (chunks too large)\n",
    "- Broken sentences or ideas\n",
    "\n",
    "### Available Chunking Strategies\n",
    "\n",
    "| Strategy | Best For | Description |\n",
    "|----------|----------|-------------|\n",
    "| `SentenceSplitter` | General use | Splits on sentences, respects chunk size |\n",
    "| `TokenTextSplitter` | Token-aware | Precise token counting for LLM limits |\n",
    "| `SemanticSplitter` | Quality-focused | Groups semantically related text |\n",
    "\n",
    "Let's compare them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: SentenceSplitter (Recommended default)\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512,       # Target chunk size in tokens\n",
    "    chunk_overlap=50,     # Overlap between chunks for context continuity\n",
    "    separator=\" \",        # Default separator\n",
    ")\n",
    "\n",
    "# Parse documents into nodes\n",
    "sentence_nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"SentenceSplitter produced {len(sentence_nodes)} nodes\")\n",
    "print(f\"\\nSample node (first 300 chars):\")\n",
    "print(f\"'{sentence_nodes[0].text[:300]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: TokenTextSplitter (Token-precise)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "token_nodes = token_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"TokenTextSplitter produced {len(token_nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunk sizes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentence_lengths = [len(n.text) for n in sentence_nodes]\n",
    "token_lengths = [len(n.text) for n in token_nodes]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(sentence_lengths, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('SentenceSplitter - Chunk Sizes')\n",
    "axes[0].set_xlabel('Characters per chunk')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].hist(token_lengths, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('TokenTextSplitter - Chunk Sizes')\n",
    "axes[1].set_xlabel('Characters per chunk')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSentenceSplitter: avg={sum(sentence_lengths)/len(sentence_lengths):.0f} chars\")\n",
    "print(f\"TokenTextSplitter: avg={sum(token_lengths)/len(token_lengths):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Size Guidelines\n",
    "\n",
    "| Use Case | Chunk Size | Overlap | Reasoning |\n",
    "|----------|------------|---------|----------|\n",
    "| Q&A on short docs | 256-512 | 20-50 | Precise retrieval |\n",
    "| Long documents | 512-1024 | 50-100 | More context per chunk |\n",
    "| Code documentation | 512-1024 | 100-200 | Preserve code blocks |\n",
    "| Legal/Technical | 1024-2048 | 200+ | Complex context |\n",
    "\n",
    "**Rule of thumb**: Start with 512 tokens, 10% overlap. Adjust based on retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metadata Extraction (Advanced Preprocessing)\n",
    "\n",
    "Extracting metadata from chunks can improve retrieval. LlamaIndex provides automatic extractors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ingestion pipeline with transformations\n",
    "# This combines parsing + metadata extraction\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=50),\n",
    "        # TitleExtractor extracts a title for each chunk\n",
    "        TitleExtractor(nodes=5),  # Use first 5 nodes to determine title\n",
    "        # SummaryExtractor creates a summary of each chunk\n",
    "        # SummaryExtractor(summaries=[\"self\"]),  # Uncomment for summaries (slower)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"Running ingestion pipeline...\")\n",
    "enriched_nodes = pipeline.run(documents=documents, show_progress=True)\n",
    "\n",
    "print(f\"\\nProduced {len(enriched_nodes)} enriched nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine enriched metadata\n",
    "print(\"Sample enriched node:\")\n",
    "sample = enriched_nodes[0]\n",
    "print(f\"  Metadata: {sample.metadata}\")\n",
    "print(f\"  Text preview: {sample.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Index\n",
    "\n",
    "Now let's create our index using the processed nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index from our enriched nodes\n",
    "index = VectorStoreIndex(nodes=enriched_nodes, show_progress=True)\n",
    "\n",
    "print(\"\\n✓ Index created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval Configuration\n",
    "\n",
    "Retrieval settings significantly impact response quality. Key parameters:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `similarity_top_k` | Number of chunks to retrieve | 2 |\n",
    "| `response_mode` | How to synthesize response | \"compact\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever with custom settings\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=5,  # Retrieve more chunks\n",
    ")\n",
    "\n",
    "# Test retrieval (without LLM generation)\n",
    "query = \"What are the types of machine learning?\"\n",
    "retrieved_nodes = retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nRetrieved {len(retrieved_nodes)} nodes:\\n\")\n",
    "\n",
    "for i, node in enumerate(retrieved_nodes):\n",
    "    print(f\"--- Node {i+1} (score: {node.score:.4f}) ---\")\n",
    "    print(f\"Source: {node.metadata.get('file_name', 'N/A')}\")\n",
    "    print(f\"Text: {node.text[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Response Synthesis Modes\n",
    "\n",
    "LlamaIndex offers different ways to synthesize responses from retrieved chunks:\n",
    "\n",
    "| Mode | Description | Best For |\n",
    "|------|-------------|----------|\n",
    "| `refine` | Iteratively refine answer with each chunk | Quality |\n",
    "| `compact` | Compact chunks into fewer LLM calls | Balance |\n",
    "| `tree_summarize` | Hierarchical summarization | Long context |\n",
    "| `simple_summarize` | Single LLM call with all chunks | Speed |\n",
    "| `no_text` | Return chunks without LLM generation | Retrieval only |\n",
    "| `accumulate` | Separate answer per chunk, then combine | Comprehensive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "# Compare different response modes\n",
    "query = \"Explain the ethical considerations of AI.\"\n",
    "\n",
    "response_modes = [\n",
    "    (\"compact\", \"Fast, combines chunks\"),\n",
    "    (\"refine\", \"Quality, iterative refinement\"),\n",
    "    (\"tree_summarize\", \"Hierarchical, good for long content\"),\n",
    "]\n",
    "\n",
    "for mode, description in response_modes:\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=3,\n",
    "        response_mode=mode,\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Mode: {mode} ({description})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Response: {str(response)[:500]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Customizing the Prompt\n",
    "\n",
    "You can customize the prompt template used for generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# Custom QA prompt\n",
    "custom_qa_prompt = PromptTemplate(\n",
    "    \"\"\"You are an expert technical assistant. \n",
    "Use the following context to answer the question.\n",
    "If you cannot find the answer in the context, say \"I don't have enough information to answer this.\"\n",
    "\n",
    "Context:\n",
    "---------\n",
    "{context_str}\n",
    "---------\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "Provide a clear, concise answer with examples where relevant:\"\"\"\n",
    ")\n",
    "\n",
    "# Create query engine with custom prompt\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    text_qa_template=custom_qa_prompt,\n",
    ")\n",
    "\n",
    "# Test\n",
    "response = query_engine.query(\"What are the main Python data types?\")\n",
    "print(\"Custom prompt response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete RAG Pipeline Class\n",
    "\n",
    "Let's wrap everything into a reusable class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "class SimpleRAGPipeline:\n",
    "    \"\"\"A reusable RAG pipeline with configurable components.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 512,\n",
    "        chunk_overlap: int = 50,\n",
    "        similarity_top_k: int = 3,\n",
    "        response_mode: str = \"compact\",\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.response_mode = response_mode\n",
    "        \n",
    "        self.index = None\n",
    "        self.query_engine = None\n",
    "        \n",
    "    def load_documents(self, input_dir: str) -> List[Document]:\n",
    "        \"\"\"Load documents from a directory.\"\"\"\n",
    "        reader = SimpleDirectoryReader(input_dir=input_dir)\n",
    "        documents = reader.load_data()\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "        return documents\n",
    "    \n",
    "    def build_index(self, documents: List[Document]) -> VectorStoreIndex:\n",
    "        \"\"\"Build index from documents with custom chunking.\"\"\"\n",
    "        # Create node parser\n",
    "        splitter = SentenceSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "        )\n",
    "        \n",
    "        # Parse into nodes\n",
    "        nodes = splitter.get_nodes_from_documents(documents)\n",
    "        print(f\"Created {len(nodes)} nodes\")\n",
    "        \n",
    "        # Build index\n",
    "        self.index = VectorStoreIndex(nodes=nodes, show_progress=True)\n",
    "        \n",
    "        # Create query engine\n",
    "        self.query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=self.similarity_top_k,\n",
    "            response_mode=self.response_mode,\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Index and query engine ready!\")\n",
    "        return self.index\n",
    "    \n",
    "    def query(self, question: str, verbose: bool = False) -> str:\n",
    "        \"\"\"Query the RAG system.\"\"\"\n",
    "        if not self.query_engine:\n",
    "            raise ValueError(\"Build index first using build_index()\")\n",
    "        \n",
    "        response = self.query_engine.query(question)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRetrieved {len(response.source_nodes)} source nodes:\")\n",
    "            for i, node in enumerate(response.source_nodes):\n",
    "                print(f\"  {i+1}. Score: {node.score:.4f} - {node.text[:100]}...\")\n",
    "        \n",
    "        return str(response)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save index to disk.\"\"\"\n",
    "        if self.index:\n",
    "            self.index.storage_context.persist(persist_dir=path)\n",
    "            print(f\"✓ Index saved to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load index from disk.\"\"\"\n",
    "        from llama_index.core import StorageContext, load_index_from_storage\n",
    "        \n",
    "        storage_context = StorageContext.from_defaults(persist_dir=path)\n",
    "        self.index = load_index_from_storage(storage_context)\n",
    "        self.query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=self.similarity_top_k,\n",
    "            response_mode=self.response_mode,\n",
    "        )\n",
    "        print(f\"✓ Index loaded from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pipeline\n",
    "rag = SimpleRAGPipeline(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# Build\n",
    "docs = rag.load_documents(\"../data/sample_docs\")\n",
    "rag.build_index(docs)\n",
    "\n",
    "# Query\n",
    "answer = rag.query(\"What is deep learning and how does it work?\", verbose=True)\n",
    "print(f\"\\n\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "rag.save(\"./storage/rag_pipeline_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "You've learned how to build a complete RAG pipeline with control over:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Component | What You Learned |\n",
    "|-----------|------------------|\n",
    "| **Chunking** | Different strategies, size/overlap tuning |\n",
    "| **Metadata** | Extractors for enriching chunks |\n",
    "| **Retrieval** | Top-K selection, similarity scoring |\n",
    "| **Synthesis** | Response modes for different use cases |\n",
    "| **Prompts** | Custom templates for better answers |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start simple**: Use defaults, then optimize based on results\n",
    "2. **Test retrieval first**: Check what chunks are retrieved before blaming the LLM\n",
    "3. **Monitor chunk quality**: Examine actual chunks during development\n",
    "4. **Iterate on prompts**: Custom prompts often improve results significantly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook (`03_querying_basics.ipynb`), we'll explore:\n",
    "- Streaming responses\n",
    "- Async queries\n",
    "- Query transformations\n",
    "- Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Chunk size experiment**: Try chunk sizes of 256, 512, 1024. Compare retrieval quality.\n",
    "\n",
    "2. **Overlap impact**: Test 0%, 10%, 20% overlap. When does it matter?\n",
    "\n",
    "3. **Response modes**: Run the same query with all response modes. Compare quality vs speed.\n",
    "\n",
    "4. **Custom prompts**: Create a prompt that makes the assistant respond in a specific style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "# Try different configurations here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
