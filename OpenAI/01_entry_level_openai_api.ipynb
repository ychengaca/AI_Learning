{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API - Entry Level Demo\n",
    "\n",
    "Welcome to the **Entry Level** OpenAI API tutorial! This notebook will guide you through:\n",
    "\n",
    "1. **Setting up your environment** - Installing dependencies and configuring API keys\n",
    "2. **Making your first API call** - Basic chat completions\n",
    "3. **Understanding the response** - Parsing and using API responses\n",
    "4. **Basic parameters** - Temperature, max_tokens, and model selection\n",
    "5. **Simple conversation** - Multi-turn chat basics\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- An OpenAI API key (get one at https://platform.openai.com/api-keys)\n",
    "\n",
    "---\n",
    "\n",
    "## Reference Documentation\n",
    "- [OpenAI Quickstart](https://platform.openai.com/docs/quickstart)\n",
    "- [Chat Completions API](https://platform.openai.com/docs/api-reference/chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the required packages and set up our API key."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T19:02:09.829624Z",
     "start_time": "2026-02-06T19:02:08.658203Z"
    }
   },
   "source": [
    "# Install the OpenAI Python SDK (run once)\n",
    "# !pip install openai python-dotenv\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if you have one)\n",
    "load_dotenv()\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# =============================================================================\n",
    "# Set the model to use throughout this notebook\n",
    "MODEL = \"gpt-4o-mini\"  # Change to \"gpt-4o\" for more capable model\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "# Option 1: Set your API key as an environment variable OPENAI_API_KEY\n",
    "# Option 2: Pass it directly (not recommended for production)\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # or replace with your key: \"sk-...\"\n",
    ")\n",
    "\n",
    "print(\"✓ OpenAI client initialized successfully!\")\n",
    "print(f\"✓ Using model: {MODEL}\")"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     16\u001B[39m MODEL = \u001B[33m\"\u001B[39m\u001B[33mgpt-4o-mini\u001B[39m\u001B[33m\"\u001B[39m  \u001B[38;5;66;03m# Change to \"gpt-4o\" for more capable model\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# =============================================================================\u001B[39;00m\n\u001B[32m     18\u001B[39m \n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Initialize the OpenAI client\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Option 1: Set your API key as an environment variable OPENAI_API_KEY\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# Option 2: Pass it directly (not recommended for production)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m client = \u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m=\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetenv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mOPENAI_API_KEY\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# or replace with your key: \"sk-...\"\u001B[39;49;00m\n\u001B[32m     24\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m✓ OpenAI client initialized successfully!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     27\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✓ Using model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mMODEL\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\YC\\Projects\\Unified\\ANT\\Automation\\OpenAIApi\\Lib\\site-packages\\openai\\_client.py:123\u001B[39m, in \u001B[36mOpenAI.__init__\u001B[39m\u001B[34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m base_url \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    121\u001B[39m     base_url = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mhttps://api.openai.com/v1\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m123\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[43m    \u001B[49m\u001B[43mversion\u001B[49m\u001B[43m=\u001B[49m\u001B[43m__version__\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    125\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_url\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_url\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhttp_client\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhttp_client\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdefault_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdefault_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_strict_response_validation\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_strict_response_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[38;5;28mself\u001B[39m._default_stream_cls = Stream\n\u001B[32m    136\u001B[39m \u001B[38;5;28mself\u001B[39m.completions = resources.Completions(\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\YC\\Projects\\Unified\\ANT\\Automation\\OpenAIApi\\Lib\\site-packages\\openai\\_base_client.py:849\u001B[39m, in \u001B[36mSyncAPIClient.__init__\u001B[39m\u001B[34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001B[39m\n\u001B[32m    832\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m    833\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(http_client)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    834\u001B[39m     )\n\u001B[32m    836\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\n\u001B[32m    837\u001B[39m     version=version,\n\u001B[32m    838\u001B[39m     limits=limits,\n\u001B[32m   (...)\u001B[39m\u001B[32m    847\u001B[39m     _strict_response_validation=_strict_response_validation,\n\u001B[32m    848\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m849\u001B[39m \u001B[38;5;28mself\u001B[39m._client = http_client \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mSyncHttpxClientWrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    850\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_url\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_url\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    851\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001B[39;49;00m\n\u001B[32m    852\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    853\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    854\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtransport\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtransport\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    855\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlimits\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlimits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    856\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    857\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\YC\\Projects\\Unified\\ANT\\Automation\\OpenAIApi\\Lib\\site-packages\\openai\\_base_client.py:747\u001B[39m, in \u001B[36m_DefaultHttpxClient.__init__\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    745\u001B[39m kwargs.setdefault(\u001B[33m\"\u001B[39m\u001B[33mlimits\u001B[39m\u001B[33m\"\u001B[39m, DEFAULT_CONNECTION_LIMITS)\n\u001B[32m    746\u001B[39m kwargs.setdefault(\u001B[33m\"\u001B[39m\u001B[33mfollow_redirects\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m747\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Your First API Call - Basic Chat Completion\n",
    "\n",
    "The Chat Completions API is the core of OpenAI's offerings. It takes a list of messages and returns a model-generated response.\n",
    "\n",
    "**Key concepts:**\n",
    "- `model`: Which AI model to use (e.g., \"gpt-4o\", \"gpt-4o-mini\")\n",
    "- `messages`: A list of conversation messages with roles (system, user, assistant)\n",
    "- `role`: Who is \"speaking\" - system sets behavior, user is you, assistant is the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `gpt-5-mini`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFoundError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Make your first API call!\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMODEL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Using the global MODEL variable\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHello! What can you help me with today?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Extract and print the response\u001B[39;00m\n\u001B[0;32m     13\u001B[0m assistant_message \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[1;32mc:\\Users\\Yunpeng.Cheng\\AppData\\Local\\anaconda3\\envs\\my\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    284\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Yunpeng.Cheng\\AppData\\Local\\anaconda3\\envs\\my\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m   1147\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1189\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m not_given,\n\u001B[0;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m   1191\u001B[0m     validate_response_format(response_format)\n\u001B[1;32m-> 1192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1193\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1195\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m   1196\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1197\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1198\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1199\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1200\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1202\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1203\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1204\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1205\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1206\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1207\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1208\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1209\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1210\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1211\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1212\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt_cache_key\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1213\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt_cache_retention\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_retention\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1214\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1215\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1216\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msafety_identifier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msafety_identifier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1217\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1218\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1219\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1220\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1221\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1222\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1223\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1225\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1226\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1227\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1228\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1229\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mverbosity\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1230\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweb_search_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1231\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1232\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[0;32m   1233\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[0;32m   1234\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1235\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1236\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1237\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m   1238\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1242\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Yunpeng.Cheng\\AppData\\Local\\anaconda3\\envs\\my\\Lib\\site-packages\\openai\\_base_client.py:1259\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1246\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1247\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1254\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1255\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1256\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1257\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1258\u001B[0m     )\n\u001B[1;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mc:\\Users\\Yunpeng.Cheng\\AppData\\Local\\anaconda3\\envs\\my\\Lib\\site-packages\\openai\\_base_client.py:1047\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1044\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1046\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mNotFoundError\u001B[0m: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `gpt-5-mini`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "# Make your first API call!\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,  # Using the global MODEL variable\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello! What can you help me with today?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "assistant_message = response.choices[0].message.content\n",
    "print(\"Assistant:\", assistant_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding the Response Object\n",
    "\n",
    "The API returns a structured response with metadata about the completion. Let's explore it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Response Object ===\n",
      "ID: chatcmpl-D5znl9W6e0bkU1SU7prLqhR0WbEnp\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Created: 1770321577\n",
      "\n",
      "=== Usage Statistics ===\n",
      "Prompt tokens: 17\n",
      "Completion tokens: 56\n",
      "Total tokens: 73\n",
      "\n",
      "=== Choice Details ===\n",
      "Finish reason: stop\n",
      "Message role: assistant\n",
      "Message content: Hello! I'm here to help you with a variety of topics. Whether you have questions about general knowl...\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the full response structure\n",
    "print(\"=== Full Response Object ===\")\n",
    "print(f\"ID: {response.id}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Created: {response.created}\")\n",
    "\n",
    "print(\"\\n=== Usage Statistics ===\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "print(\"\\n=== Choice Details ===\")\n",
    "choice = response.choices[0]\n",
    "print(f\"Finish reason: {choice.finish_reason}\")\n",
    "print(f\"Message role: {choice.message.role}\")\n",
    "print(f\"Message content: {choice.message.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using System Messages\n",
    "\n",
    "The **system message** sets the behavior and personality of the assistant. It's like giving the AI instructions before the conversation starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant (as coding tutor):\n",
      "Think of a variable in programming like a labeled box where you can store information. Just like you might label a box \"Toys\" or \"Books,\" in programming, you give a variable a name, like `age` or `score`. \n",
      "\n",
      "You can put something inside that box (like the number 10 for `age`), and you can change what's inside it whenever you need to. The variable acts as a placeholder, allowing you to refer to the information without needing to remember the exact value.\n"
     ]
    }
   ],
   "source": [
    "# Using a system message to customize assistant behavior\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful coding tutor who explains concepts simply and uses analogies. Keep responses concise.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is a variable in programming?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Assistant (as coding tutor):\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Parameters: Temperature and Max Tokens\n",
    "\n",
    "**Temperature** (0-2): Controls randomness in responses\n",
    "- Lower (0-0.3): More focused, deterministic, factual\n",
    "- Higher (0.7-1.0): More creative, varied, imaginative\n",
    "\n",
    "**Max Tokens**: Limits the length of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Temperature (0.2) - More focused:\n",
      "In a world where emotions were forbidden, a lonely robot discovered an old book of poetry and, for the first time, felt the warmth of longing in its metallic heart.\n",
      "\n",
      "High Temperature (1.0) - More creative:\n",
      "In a world where emotions were obsolete, a curious little robot discovered an ancient book of poetry and, for the first time, yearned to feel.\n"
     ]
    }
   ],
   "source": [
    "# Comparing different temperature settings\n",
    "prompt = \"Write a one-sentence story about a robot.\"\n",
    "\n",
    "# Low temperature - more predictable/focused\n",
    "low_temp_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# High temperature - more creative/varied\n",
    "high_temp_response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Low Temperature (0.2) - More focused:\")\n",
    "print(low_temp_response.choices[0].message.content)\n",
    "print(\"\\nHigh Temperature (1.0) - More creative:\")\n",
    "print(high_temp_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Turn Conversations\n",
    "\n",
    "To have a conversation with context, you include previous messages in each request. The model doesn't remember previous calls - you must provide the conversation history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: My name is Alex and I love pizza.\n",
      "Assistant: Nice to meet you, Alex! Pizza is a delicious choice. Do you have a favorite type or topping?\n",
      "\n",
      "User: What's my name?\n",
      "Assistant: Your name is Alex!\n",
      "\n",
      "User: What food did I mention?\n",
      "Assistant: You mentioned that you love pizza!\n"
     ]
    }
   ],
   "source": [
    "# Building a simple conversation with memory\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly assistant who remembers our conversation.\"}\n",
    "]\n",
    "\n",
    "def chat(user_message):\n",
    "    \"\"\"Send a message and get a response, maintaining conversation history.\"\"\"\n",
    "    # Add user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Make API call with full history\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=conversation_history\n",
    "    )\n",
    "    \n",
    "    # Extract assistant message\n",
    "    assistant_message = response.choices[0].message.content\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    \n",
    "    return assistant_message\n",
    "\n",
    "# Have a multi-turn conversation\n",
    "print(\"User: My name is Alex and I love pizza.\")\n",
    "print(\"Assistant:\", chat(\"My name is Alex and I love pizza.\"))\n",
    "\n",
    "print(\"\\nUser: What's my name?\")\n",
    "print(\"Assistant:\", chat(\"What's my name?\"))\n",
    "\n",
    "print(\"\\nUser: What food did I mention?\")\n",
    "print(\"Assistant:\", chat(\"What food did I mention?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Basic Error Handling\n",
    "\n",
    "Always handle potential API errors gracefully. Common issues include rate limits, invalid API keys, and network problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import APIError, RateLimitError, APIConnectionError\n",
    "\n",
    "def safe_chat_completion(messages, model=None):\n",
    "    \"\"\"Make an API call with proper error handling.\"\"\"\n",
    "    if model is None:\n",
    "        model = MODEL  # Use global MODEL if not specified\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except RateLimitError as e:\n",
    "        print(f\"Rate limit exceeded. Please wait and try again: {e}\")\n",
    "        return None\n",
    "    \n",
    "    except APIConnectionError as e:\n",
    "        print(f\"Connection error. Check your internet: {e}\")\n",
    "        return None\n",
    "    \n",
    "    except APIError as e:\n",
    "        print(f\"API error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the safe function\n",
    "result = safe_chat_completion([\n",
    "    {\"role\": \"user\", \"content\": \"Say hello!\"}\n",
    "])\n",
    "if result:\n",
    "    print(\"Success:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Available Models\n",
    "\n",
    "OpenAI offers several models with different capabilities and price points. Here are the main ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╔════════════════════════════════════════════════════════════════════╗\n",
      "║                     OPENAI MODELS OVERVIEW                         ║\n",
      "╠════════════════════════════════════════════════════════════════════╣\n",
      "║ GPT-4o          │ Latest flagship model, multimodal (text+vision)  ║\n",
      "║                 │ Best for: Complex tasks, vision, high accuracy    ║\n",
      "╠─────────────────┼──────────────────────────────────────────────────╣\n",
      "║ GPT-4o-mini     │ Fast and affordable, great for most tasks        ║\n",
      "║                 │ Best for: Quick responses, cost-effective usage  ║\n",
      "╠─────────────────┼──────────────────────────────────────────────────╣\n",
      "║ o1 / o1-mini    │ Reasoning models for complex problem solving     ║\n",
      "║                 │ Best for: Math, coding, logical reasoning        ║\n",
      "╠─────────────────┼──────────────────────────────────────────────────╣\n",
      "║ GPT-4-turbo     │ Previous generation flagship                     ║\n",
      "║                 │ Best for: Legacy applications, specific needs    ║\n",
      "╠─────────────────┼──────────────────────────────────────────────────╣\n",
      "║ GPT-3.5-turbo   │ Older, very fast and cheap                       ║\n",
      "║                 │ Best for: Simple tasks, high volume, low cost    ║\n",
      "╚════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "\n",
      "Your available models (first 10):\n",
      "  - gpt-4-0613\n",
      "  - gpt-4\n",
      "  - gpt-3.5-turbo\n",
      "  - gpt-5.2-codex\n",
      "  - gpt-4o-mini-tts-2025-12-15\n",
      "  - gpt-realtime-mini-2025-12-15\n",
      "  - gpt-audio-mini-2025-12-15\n",
      "  - chatgpt-image-latest\n",
      "  - davinci-002\n",
      "  - babbage-002\n"
     ]
    }
   ],
   "source": [
    "# Common models and their use cases\n",
    "models_info = \"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════╗\n",
    "║                     OPENAI MODELS OVERVIEW                         ║\n",
    "╠════════════════════════════════════════════════════════════════════╣\n",
    "║ GPT-4o          │ Latest flagship model, multimodal (text+vision)  ║\n",
    "║                 │ Best for: Complex tasks, vision, high accuracy    ║\n",
    "╠─────────────────┼──────────────────────────────────────────────────╣\n",
    "║ GPT-4o-mini     │ Fast and affordable, great for most tasks        ║\n",
    "║                 │ Best for: Quick responses, cost-effective usage  ║\n",
    "╠─────────────────┼──────────────────────────────────────────────────╣\n",
    "║ o1 / o1-mini    │ Reasoning models for complex problem solving     ║\n",
    "║                 │ Best for: Math, coding, logical reasoning        ║\n",
    "╠─────────────────┼──────────────────────────────────────────────────╣\n",
    "║ GPT-4-turbo     │ Previous generation flagship                     ║\n",
    "║                 │ Best for: Legacy applications, specific needs    ║\n",
    "╠─────────────────┼──────────────────────────────────────────────────╣\n",
    "║ GPT-3.5-turbo   │ Older, very fast and cheap                       ║\n",
    "║                 │ Best for: Simple tasks, high volume, low cost    ║\n",
    "╚════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "print(models_info)\n",
    "\n",
    "# List available models from your account\n",
    "print(\"\\nYour available models (first 10):\")\n",
    "models = client.models.list()\n",
    "for i, model in enumerate(models.data[:10]):\n",
    "    print(f\"  - {model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've learned the fundamentals of the OpenAI API:\n",
    "\n",
    "1. **Client Setup**: Initialize the OpenAI client with your API key\n",
    "2. **Chat Completions**: The core API for generating text responses\n",
    "3. **Message Roles**: system (instructions), user (your input), assistant (AI output)\n",
    "4. **Response Parsing**: Extract content and metadata from responses\n",
    "5. **Parameters**: Control output with temperature and max_tokens\n",
    "6. **Multi-turn Conversations**: Maintain context by including message history\n",
    "7. **Error Handling**: Gracefully handle API errors\n",
    "\n",
    "### Next Steps\n",
    "Move on to **02_middle_level_openai_api.ipynb** to learn:\n",
    "- Streaming responses\n",
    "- Function calling / Tools\n",
    "- Vision capabilities\n",
    "- Structured outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
