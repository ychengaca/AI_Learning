{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 4: Specialty - RAG, Multi-Agent Systems & Production Patterns\n",
        "\n",
        "This notebook covers specialty topics: Retrieval-Augmented Generation (RAG), multi-agent orchestration, and production deployment patterns.\n",
        "\n",
        "## Learning Objectives\n",
        "- Build a complete RAG pipeline with vector stores\n",
        "- Implement multi-agent systems with LangGraph\n",
        "- Use LangSmith for observability and tracing\n",
        "- Production patterns: error handling, fallbacks, rate limiting\n",
        "- Evaluation and testing strategies\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Notebooks 01-03\n",
        "- ChromaDB installed (`pip install chromadb langchain-chroma`)\n",
        "\n",
        "---\n",
        "\n",
        "**References:**\n",
        "- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)\n",
        "- [LangSmith Docs](https://docs.langchain.com/docs/langsmith/)\n",
        "- [LangGraph Multi-Agent](https://langchain-ai.github.io/langgraph/how-tos/multi-agent/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client initialized  -> model: gpt-4o-mini\n",
            "Google client initialized  -> model: gemini-3-flash-preview\n",
            "Using GPT model:    gpt-4o-mini\n",
            "Using Gemini model: gemini-3-flash-preview\n",
            "\n",
            "Available Models:\n",
            "-------------------------------------------------------\n",
            "  gpt-4o-mini          -> ChatOpenAI(gpt-4o-mini)\n",
            "  gemini-3-flash-preview -> ChatGoogleGenerativeAI(gemini-3-flash-preview)\n",
            "-------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for shared config\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import global model configuration\n",
        "from config import (\n",
        "    GPT_MODEL, GEMINI_MODEL,\n",
        "    GPT_MODEL_NAME, GEMINI_MODEL_NAME,\n",
        "    get_model, list_available_models,\n",
        ")\n",
        "\n",
        "print(f\"Using GPT model:    {GPT_MODEL_NAME}\")\n",
        "print(f\"Using Gemini model: {GEMINI_MODEL_NAME}\")\n",
        "print()\n",
        "list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "RAG combines a **retriever** (to find relevant documents) with a **generator** (LLM) to answer questions grounded in your own data.\n",
        "\n",
        "### RAG Pipeline\n",
        "```\n",
        "Documents -> Split -> Embed -> Vector Store -> Retrieve -> Generate Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split 5 documents into 10 chunks\n",
            "\n",
            "Chunk 0: LangChain is a framework for developing applications powered by\n",
            "        large la...\n",
            "\n",
            "Chunk 1: and memory. LangChain supports multiple LLM providers including OpenAI, Google,\n",
            "...\n",
            "\n",
            "Chunk 2: LangGraph is a library for building stateful, multi-actor\n",
            "        applications w...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Step 1: Create sample documents (in production, load from files/web)\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"\"\"LangChain is a framework for developing applications powered by\n",
        "        large language models. It provides tools for prompt management, chains, agents,\n",
        "        and memory. LangChain supports multiple LLM providers including OpenAI, Google,\n",
        "        Anthropic, and local models.\"\"\",\n",
        "        metadata={\"source\": \"langchain_overview\", \"topic\": \"framework\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"LangGraph is a library for building stateful, multi-actor\n",
        "        applications with LLMs. It extends LangChain with graph-based workflows,\n",
        "        allowing developers to create complex agent architectures with cycles,\n",
        "        persistence, and human-in-the-loop capabilities.\"\"\",\n",
        "        metadata={\"source\": \"langgraph_overview\", \"topic\": \"framework\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"Retrieval-Augmented Generation (RAG) is a technique that\n",
        "        combines information retrieval with text generation. The system first retrieves\n",
        "        relevant documents from a knowledge base, then uses those documents as context\n",
        "        for the LLM to generate accurate, grounded answers.\"\"\",\n",
        "        metadata={\"source\": \"rag_overview\", \"topic\": \"technique\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"Vector databases store data as high-dimensional vectors,\n",
        "        enabling similarity search. Popular options include ChromaDB, Pinecone, Weaviate,\n",
        "        and FAISS. They are essential for RAG pipelines as they allow efficient\n",
        "        retrieval of semantically similar documents.\"\"\",\n",
        "        metadata={\"source\": \"vector_db_overview\", \"topic\": \"infrastructure\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"LangSmith is a platform for debugging, testing, evaluating,\n",
        "        and monitoring LLM applications. It provides tracing for every LLM call,\n",
        "        evaluation datasets, and prompt management. LangSmith integrates natively\n",
        "        with LangChain and LangGraph.\"\"\",\n",
        "        metadata={\"source\": \"langsmith_overview\", \"topic\": \"observability\"},\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Step 2: Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=50,\n",
        ")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split {len(documents)} documents into {len(splits)} chunks\")\n",
        "for i, split in enumerate(splits[:3]):\n",
        "    print(f\"\\nChunk {i}: {split.page_content[:80]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Step 3: Create embeddings and vector store\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"langchain_learning\",\n",
        ")\n",
        "\n",
        "# Step 4: Create a retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Test retrieval\n",
        "results = retriever.invoke(\"What is RAG?\")\n",
        "print(f\"Retrieved {len(results)} documents for 'What is RAG?':\\n\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"  {i+1}. [{doc.metadata.get('source', 'unknown')}] {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Step 5: Build the RAG chain\n",
        "rag_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a helpful assistant. Answer the question based ONLY on the\n",
        "    following context. If you cannot find the answer in the context, say so.\n",
        "\n",
        "    Context:\n",
        "    {context}\"\"\"),\n",
        "    (\"human\", \"{question}\"),\n",
        "])\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# RAG chain with GPT\n",
        "rag_chain_gpt = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | GPT_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# RAG chain with Gemini\n",
        "rag_chain_gemini = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | GEMINI_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Test the RAG chain\n",
        "question = \"What is LangGraph and how does it relate to LangChain?\"\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(\"GPT Answer:\")\n",
        "print(rag_chain_gpt.invoke(question))\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "print(\"Gemini Answer:\")\n",
        "print(rag_chain_gemini.invoke(question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with more questions\n",
        "test_questions = [\n",
        "    \"What is a vector database?\",\n",
        "    \"How does LangSmith help with debugging?\",\n",
        "    \"What programming languages does LangChain support?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"Q: {q}\")\n",
        "    answer = rag_chain_gpt.invoke(q)\n",
        "    print(f\"A: {answer}\\n\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Multi-Agent System with LangGraph\n",
        "\n",
        "Build a team of specialized agents that collaborate to complete tasks.\n",
        "\n",
        "### Architecture\n",
        "```\n",
        "User Query -> Router -> Researcher Agent -+-> Writer Agent -> Final Answer\n",
        "                       Fact-Checker Agent -+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Define multi-agent state\n",
        "class MultiAgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    research_notes: str\n",
        "    draft: str\n",
        "    final_answer: str\n",
        "\n",
        "# Researcher node - gathers information\n",
        "def researcher_node(state: MultiAgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=\"\"\"You are a research assistant. Analyze the user's question\n",
        "        and provide detailed research notes with key facts and sources. Be thorough.\"\"\"),\n",
        "    ] + state[\"messages\"]\n",
        "    response = GPT_MODEL.invoke(messages)\n",
        "    return {\"research_notes\": response.content}\n",
        "\n",
        "# Writer node - creates polished response\n",
        "def writer_node(state: MultiAgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=f\"\"\"You are an expert technical writer. Based on these\n",
        "        research notes, write a clear and well-structured answer.\n",
        "\n",
        "        Research Notes:\n",
        "        {state['research_notes']}\"\"\"),\n",
        "    ] + state[\"messages\"]\n",
        "    response = GEMINI_MODEL.invoke(messages)\n",
        "    return {\"draft\": response.content}\n",
        "\n",
        "# Reviewer node - quality check\n",
        "def reviewer_node(state: MultiAgentState):\n",
        "    messages = [\n",
        "        SystemMessage(content=f\"\"\"You are a quality reviewer. Review this draft for\n",
        "        accuracy, clarity, and completeness. Provide the final polished version.\n",
        "\n",
        "        Draft:\n",
        "        {state['draft']}\n",
        "\n",
        "        Research Notes (for fact-checking):\n",
        "        {state['research_notes']}\"\"\"),\n",
        "    ] + state[\"messages\"]\n",
        "    response = GPT_MODEL.invoke(messages)\n",
        "    return {\"final_answer\": response.content}\n",
        "\n",
        "# Build the multi-agent graph\n",
        "multi_agent = StateGraph(MultiAgentState)\n",
        "multi_agent.add_node(\"researcher\", researcher_node)\n",
        "multi_agent.add_node(\"writer\", writer_node)\n",
        "multi_agent.add_node(\"reviewer\", reviewer_node)\n",
        "\n",
        "multi_agent.add_edge(START, \"researcher\")\n",
        "multi_agent.add_edge(\"researcher\", \"writer\")\n",
        "multi_agent.add_edge(\"writer\", \"reviewer\")\n",
        "multi_agent.add_edge(\"reviewer\", END)\n",
        "\n",
        "multi_agent_graph = multi_agent.compile()\n",
        "\n",
        "# Run the multi-agent system\n",
        "result = multi_agent_graph.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain how RAG works and when to use it vs fine-tuning.\"}],\n",
        "    \"research_notes\": \"\",\n",
        "    \"draft\": \"\",\n",
        "    \"final_answer\": \"\",\n",
        "})\n",
        "\n",
        "print(\"=== RESEARCH NOTES ===\")\n",
        "print(result[\"research_notes\"][:300] + \"...\")\n",
        "print(\"\\n=== DRAFT ===\")\n",
        "print(result[\"draft\"][:300] + \"...\")\n",
        "print(\"\\n=== FINAL ANSWER ===\")\n",
        "print(result[\"final_answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Production Patterns\n",
        "\n",
        "### 4a. Fallback Chains\n",
        "\n",
        "If one model fails, automatically fall back to another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fallback: if GPT fails, fall back to Gemini\n",
        "from langchain_core.runnables import RunnableWithFallbacks\n",
        "\n",
        "# Create a chain with fallback\n",
        "primary_chain = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ])\n",
        "    | GPT_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "fallback_chain = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ])\n",
        "    | GEMINI_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Chain with fallback\n",
        "robust_chain = primary_chain.with_fallbacks([fallback_chain])\n",
        "\n",
        "result = robust_chain.invoke({\"question\": \"What are the benefits of microservices?\"})\n",
        "print(\"Result (with fallback protection):\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b. Retry with Exponential Backoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retry with backoff for rate-limited APIs\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# LangChain models support max_retries natively\n",
        "retry_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    max_retries=3,  # Automatic retry with exponential backoff\n",
        "    request_timeout=30,\n",
        ")\n",
        "\n",
        "# All invocations will automatically retry on transient failures\n",
        "result = retry_model.invoke(\"What is the meaning of life?\")\n",
        "print(\"With retry protection:\", result.content[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4c. LangSmith Tracing (Observability)\n",
        "\n",
        "LangSmith provides full tracing of every LLM call, chain execution, and agent step.\n",
        "\n",
        "To enable tracing, set these environment variables:\n",
        "```bash\n",
        "LANGSMITH_TRACING=true\n",
        "LANGSMITH_API_KEY=your-key\n",
        "LANGSMITH_PROJECT=langchain-learning\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if LangSmith tracing is configured\n",
        "if os.getenv(\"LANGSMITH_API_KEY\"):\n",
        "    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "    print(\"LangSmith tracing is ENABLED\")\n",
        "    print(f\"Project: {os.getenv('LANGSMITH_PROJECT', 'default')}\")\n",
        "    print(\"View traces at: https://smith.langchain.com/\")\n",
        "\n",
        "    # All subsequent LLM calls will be automatically traced\n",
        "    traced_result = GPT_MODEL.invoke(\"What is observability in AI systems?\")\n",
        "    print(f\"\\nTraced response: {traced_result.content[:200]}\")\n",
        "else:\n",
        "    print(\"LangSmith tracing is NOT configured.\")\n",
        "    print(\"Set LANGSMITH_API_KEY in your .env to enable tracing.\")\n",
        "    print(\"Sign up at: https://smith.langchain.com/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Pattern: Comparing Model Outputs\n",
        "\n",
        "A simple pattern for evaluating model quality by comparing outputs from both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class EvalResult(BaseModel):\n",
        "    \"\"\"Evaluation of an LLM response.\"\"\"\n",
        "    relevance: int = Field(description=\"How relevant is the answer (1-10)\")\n",
        "    accuracy: int = Field(description=\"How accurate is the answer (1-10)\")\n",
        "    clarity: int = Field(description=\"How clear is the answer (1-10)\")\n",
        "    reasoning: str = Field(description=\"Brief reasoning for the scores\")\n",
        "\n",
        "eval_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert evaluator. Score the following AI response\n",
        "    on relevance, accuracy, and clarity (each 1-10).\n",
        "\n",
        "    Question: {question}\n",
        "    Response: {response}\"\"\"),\n",
        "    (\"human\", \"Evaluate this response.\"),\n",
        "])\n",
        "\n",
        "# Generate responses from both models\n",
        "test_question = \"What are the key differences between SQL and NoSQL databases?\"\n",
        "\n",
        "gpt_response = GPT_MODEL.invoke(test_question).content\n",
        "gemini_response = GEMINI_MODEL.invoke(test_question).content\n",
        "\n",
        "# Cross-evaluate: GPT evaluates Gemini's response and vice versa\n",
        "gpt_evaluator = GPT_MODEL.with_structured_output(EvalResult)\n",
        "gemini_evaluator = GEMINI_MODEL.with_structured_output(EvalResult)\n",
        "\n",
        "print(\"=== GPT Response (first 200 chars) ===\")\n",
        "print(gpt_response[:200] + \"...\\n\")\n",
        "\n",
        "print(\"=== Gemini Response (first 200 chars) ===\")\n",
        "print(gemini_response[:200] + \"...\\n\")\n",
        "\n",
        "# GPT evaluates Gemini's response\n",
        "eval_of_gemini = gpt_evaluator.invoke(\n",
        "    eval_prompt.invoke({\"question\": test_question, \"response\": gemini_response})\n",
        ")\n",
        "print(\"=== GPT's evaluation of Gemini ===\")\n",
        "print(f\"  Relevance: {eval_of_gemini.relevance}/10\")\n",
        "print(f\"  Accuracy:  {eval_of_gemini.accuracy}/10\")\n",
        "print(f\"  Clarity:   {eval_of_gemini.clarity}/10\")\n",
        "print(f\"  Reasoning: {eval_of_gemini.reasoning}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| RAG Pipeline | Documents -> Split -> Embed -> Store -> Retrieve -> Generate |\n",
        "| Vector Store | ChromaDB for similarity search |\n",
        "| Multi-Agent | Researcher -> Writer -> Reviewer graph |\n",
        "| Fallback Chains | Automatic model fallback on failure |\n",
        "| Retry Logic | Built-in retry with exponential backoff |\n",
        "| LangSmith | Observability and tracing setup |\n",
        "| Evaluation | Cross-model evaluation with structured output |\n",
        "\n",
        "## Recommended Next Steps\n",
        "\n",
        "1. **Explore [LangSmith](https://smith.langchain.com/)** - Set up tracing for your applications\n",
        "2. **Try [LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/)** - Deploy your agents\n",
        "3. **Read [LangChain Academy](https://academy.langchain.com/)** - Structured courses\n",
        "4. **Join the [Community](https://github.com/langchain-ai/langchain/discussions)** - Get help and share solutions\n",
        "\n",
        "## Full Learning Path Complete\n",
        "\n",
        "| Level | Notebook | Key Topics |\n",
        "|-------|----------|------------|\n",
        "| Entry | 01 | Chat models, messages, prompts, LCEL, tools, agents |\n",
        "| Intermediate | 02 | LCEL advanced, StateGraph, conditional edges, tool loops |\n",
        "| Advanced | 03 | Persistence, interrupts, streaming, subgraphs, reducers |\n",
        "| Specialty | 04 | RAG, multi-agent, fallbacks, observability, evaluation |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
