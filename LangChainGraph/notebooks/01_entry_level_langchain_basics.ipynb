{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Level 1: Entry Level - LangChain Basics\n",
        "\n",
        "Welcome to the LangChain learning path! This notebook covers the fundamental concepts you need to get started with LangChain.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand what LangChain is and why it's useful\n",
        "- Learn to work with Chat Models (GPT-4o-mini & Gemini Flash)\n",
        "- Master message types and prompt templates\n",
        "- Build your first simple chain\n",
        "- Introduction to tools and agents\n",
        "p\n",
        "## Prerequisites\n",
        "- Python 3.10+\n",
        "- API keys for OpenAI and Google (set as environment variables)\n",
        "\n",
        "---\n",
        "\n",
        "**Documentation Reference:** [LangChain Quickstart](https://docs.langchain.com/oss/python/langchain/quickstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "We use a shared `config.py` that loads API keys from `.env` and defines global model instances so every notebook uses the same models consistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install langchain langchain-core langchain-openai langchain-google-genai langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client initialized  -> model: gpt-4o-mini\n",
            "Google client initialized  -> model: gemini-3-flash-preview\n",
            "Using GPT model:    gpt-4o-mini\n",
            "Using Gemini model: gemini-3-flash-preview\n",
            "\n",
            "Available Models:\n",
            "-------------------------------------------------------\n",
            "  gpt-4o-mini          -> ChatOpenAI(gpt-4o-mini)\n",
            "  gemini-3-flash-preview -> ChatGoogleGenerativeAI(gemini-3-flash-preview)\n",
            "-------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for shared config\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import global model configuration\n",
        "from config import (\n",
        "    GPT_MODEL, GEMINI_MODEL,\n",
        "    GPT_MODEL_NAME, GEMINI_MODEL_NAME,\n",
        "    get_model, list_available_models,\n",
        ")\n",
        "\n",
        "print(f\"Using GPT model:    {GPT_MODEL_NAME}\")\n",
        "print(f\"Using Gemini model: {GEMINI_MODEL_NAME}\")\n",
        "print()\n",
        "list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding Chat Models\n",
        "\n",
        "Chat models are the foundation of LangChain. They take **messages** as input and return AI-generated responses.\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Chat Model** | Wrapper around an LLM that speaks in messages |\n",
        "| **invoke()** | Send a prompt and get a single response |\n",
        "| **AIMessage** | The response object returned by the model |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-4o-mini says:\n",
            "LangChain is a framework designed to facilitate the development of applications that leverage large language models (LLMs) by providing tools for chaining together various components like prompts, memory, and APIs.\n",
            "\n",
            "Token usage: {'completion_tokens': 37, 'prompt_tokens': 15, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Simple invocation with GPT-4o-mini\n",
        "response = GPT_MODEL.invoke(\"What is LangChain in one sentence?\")\n",
        "print(\"GPT-4o-mini says:\")\n",
        "print(response.content)\n",
        "print(f\"\\nToken usage: {response.response_metadata.get('token_usage', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini Flash says:\n",
            "[{'type': 'text', 'text': 'LangChain is an open-source framework designed to simplify the creation of applications powered by large language models (LLMs) by providing tools to \"chain\" together different components like data sources, prompts, and APIs.', 'extras': {'signature': 'EugJCuUJAb4+9vv2mnG0w7GuspFadlq3NKX0fJBPZ2XCzLEJGDJT0fBx07mcATdnYQlxFYUdsSUcERLeveCp5R817Hf64dfRxdr/+ZGBpuYdBHwsTJ0+hpXQpvY0d6eUnPHX09wYLsflV8dPuxEIDsgT0+OqdPFPnO3xS39ZKIQeYwI2YljuX16FZxd0PJU/wzR0MvOIFVRd1VgfN3PPfbX0NLKaFZuccJqBMPyIY8yX5dPPNDWS7tS5cSg7IxY82BRc32wF9EOfTPROjctEUjhnsxZaKFRHgaH6PHtDIpdpYLhQjF0lXQHQMLiGbKiJRVzUZTC5gN94A5BhTh8SOmV3x1GA0YcgAryf/XL5GE9STLrqGvrO44S8pGa2qq2XwZ0EMCmr1hnWQu04nMUsM5/yZGiYWt3ja5EaTmMPYIL8pa+SGXCJt5aLjN51J8pC5Ki0mJd5LbRFQ9irOYtRq3jF9kw+zI0N4L3CoNvJyQvd5MuPKZEJqSqiDB+kAr3VGxOIP+3CURWNHC7VSuclSZKvEMJsMBvsdlI3nToDXGP5aXzn040frP4vXiag4Zgfc1p/EMqJ+8UnwEf010bL/1m+buvcrwBOasLmcS0GrIwyZ+gCDyzw0ErkSpUCQXvcHTsEYsRRsjGZqry+1Zvww2FBgfESClxyQ1hQKUtVOWNEF1YJMsZMgUMZnO/UE5Fwu1wUYy0817bE07HxCAkfoXXr3gZYT2Pc6YxG3TsLBGb9DDubMLnoqcqVWz5nIy6eJkxkQvoYGt8qIgOpz4b9NjitYZaxu1WJ1TcTtQyAEMAgemeGNf7Q5foOMtijmfVQP7+tvYGMJNiPhYoyf8g8pDLCjydPpf6Vfz/DjUnYLQDgcPqyvYsXiTSRVzcQoNA4sVbeV8mSBekdtUZ1FefXazk7FJGABxIR1iZczbLWJkKVSCu6kJ1xurJTGKX+8Sp2Q/QXAhNRxPIjvhdInZmhtMbRb0vakuAqJ/SJ5aM75MO4bEO5XcCCiZNlUG0Fxa/LIO1Q1OaM6ss2KW85aYowv5fA+3/33tKrXvTq/7gtYfBvCytndaSwcqUwRBzv4octy27xC7qwigIQV3yujNj2a9o2KuAqs0k6IFoK/yDCjmTcw92h55HLqkf5X3SJyRNSOO3OPP49l9wCTBjzYQvDJw1zUTaV9RnixTYv0TKnrntAtnJjkqeIBvnRo6JokmwSIvV9dQNEqI5FvkiMsuP3fnHENBMfearg7P1CTkQFTUTpCFa1rzgy/41ifibaOJqmj5tqNWFHsYKbmBKpUvTPEwiuLZcHfLdJpZddeQavDHR5hm59UNaopcblMXrY3Cko3VFL1vqAkH29rjUujffJsjBPtBHeoMgaKyWPcsJE057tqmFRoV5SQgZ928XcbQ591kMkpoKRLO4IeYPklC05pCXEB2vUE/4CW9+a9eRpuxDPoNtRcda+hWz077iKS0/i6z03zhtbKddHefQtDUC2JzhAYxGAvi++y0cUtJdTwXHpjAVqon5Rt8tHN4aAwRIgiSlBdBWUOOYmpwwL7ujO/bugLy0AQLCPoOqJHX+OwuAw1teTVO8XI7JH6t2zBuEhpVKWy7xti3UiLm6tME9Py4//KCUy9PLsSs3yyny1GQN1+znraqZHtpqW7TKqjhVz6wRRIuPszga14GE='}}]\n"
          ]
        }
      ],
      "source": [
        "# Same question with Gemini Flash\n",
        "response = GEMINI_MODEL.invoke(\"What is LangChain in one sentence?\")\n",
        "print(\"Gemini Flash says:\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Message Types\n",
        "\n",
        "LangChain uses structured message types to communicate with models:\n",
        "\n",
        "| Message Type | Role | Purpose |\n",
        "|-------------|------|---------|\n",
        "| `SystemMessage` | system | Sets behavior/personality of the AI |\n",
        "| `HumanMessage` | user | The user's input |\n",
        "| `AIMessage` | assistant | The AI's response |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-4o-mini:\n",
            " A list comprehension is a concise way to create lists in Python. It allows you to generate a new list by applying an expression to each item in an existing iterable (like a list or range) and can include an optional condition to filter items.\n",
            "\n",
            "The basic syntax is:\n",
            "\n",
            "```python\n",
            "new_list = [expression for item in iterable if condition]\n",
            "```\n",
            "\n",
            "For example, to create a list of squares of even numbers from 0 to 9:\n",
            "\n",
            "```python\n",
            "squares = [x**2 for x in range(10) if x % 2 == 0]\n",
            "```\n",
            "\n",
            "This results in `squares` being `[0, 4, 16, 36, 64]`.\n",
            "\n",
            "============================================================\n",
            "\n",
            "Gemini Flash:\n",
            " [{'type': 'text', 'text': 'A **list comprehension** is a concise way to create a new list by iterating over an existing iterable (like a list, range, or string) in a single line of code.\\n\\n### Syntax\\n`[expression for item in iterable if condition]`\\n\\n---\\n\\n### Example: Squaring Even Numbers\\nSuppose you want to create a list of squares for even numbers between 0 and 4.\\n\\n**Traditional `for` loop:**\\n```python\\nsquares = []\\nfor x in range(5):\\n    if x % 2 == 0:\\n        squares.append(x**2)\\n# Result: [0, 4, 16]\\n```\\n\\n**List Comprehension:**\\n```python\\nsquares = [x**2 for x in range(5) if x % 2 == 0]\\n# Result: [0, 4, 16]\\n```\\n\\n### Why use it?\\n1.  **Brevity:** It turns multiple lines of code into one.\\n2.  **Readability:** It clearly shows the intent (what the list contains) rather than the mechanics of how to build it.\\n3.  **Performance:** It is often slightly faster than a standard `for` loop using `.append()`.', 'extras': {'signature': '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'}}]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Using structured messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful Python tutor. Be concise.\"),\n",
        "    HumanMessage(content=\"What is a list comprehension?\"),\n",
        "]\n",
        "\n",
        "# Try with GPT\n",
        "response = GPT_MODEL.invoke(messages)\n",
        "print(\"GPT-4o-mini:\\n\", response.content)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Try with Gemini\n",
        "response = GEMINI_MODEL.invoke(messages)\n",
        "print(\"Gemini Flash:\\n\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are plenty of delicious ingredients you can add to scrambled eggs to enhance their flavor and texture! Here are some ideas:\n",
            "\n",
            "1. **Cheese**: Add shredded cheese like cheddar, feta, or goat cheese for creaminess and flavor.\n",
            "2. **Herbs**: Fresh herbs like chives, parsley, dill, or cilantro can add a fresh taste.\n",
            "3. **Vegetables**: SautÃ©ed onions, bell peppers, spinach, tomatoes, or mushrooms can add nutrition and flavor.\n",
            "4. **Meats**: Cooked bacon, ham, or sausage can add heartiness.\n",
            "5. **Cream or Milk**: A splash of cream or milk can make the eggs creamier.\n",
            "6. **Spices**: A pinch of salt, pepper, paprika, or even a dash of hot sauce can enhance the flavor.\n",
            "7. **Avocado**: Adding diced avocado can provide creaminess and healthy fats.\n",
            "8. **Salsa**: A spoonful of salsa can add a zesty kick.\n",
            "\n",
            "Feel free to mix and match based on your preferences! Enjoy your scrambled eggs!\n"
          ]
        }
      ],
      "source": [
        "# Multi-turn conversation\n",
        "conversation = [\n",
        "    SystemMessage(content=\"You are a friendly cooking assistant.\"),\n",
        "    HumanMessage(content=\"How do I make scrambled eggs?\"),\n",
        "    AIMessage(content=\"Crack 2-3 eggs into a bowl, whisk, cook on medium heat with butter, stir gently until just set.\"),\n",
        "    HumanMessage(content=\"What can I add to make them better?\"),\n",
        "]\n",
        "\n",
        "response = GPT_MODEL.invoke(conversation)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prompt Templates\n",
        "\n",
        "Prompt templates let you create reusable, parameterized prompts. They separate the **structure** from the **data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted messages:\n",
            "  [system]: You are an expert in Python programming. Answer in beginner-friendly style.\n",
            "  [human]: What is a decorator?\n",
            "\n",
            "--- Model Response ---\n",
            "A decorator in Python is a special type of function that allows you to modify or enhance the behavior of another function or method. You can think of it as a way to \"wrap\" a function with additional functionality without changing its actual code.\n",
            "\n",
            "### How Does It Work?\n",
            "\n",
            "When you use a decorator, you define a function that takes another function as an argument, adds some kind of functionality, and then returns a new function. This is often used for tasks like logging, access control, or modifying input/output.\n",
            "\n",
            "### Basic Example\n",
            "\n",
            "Here's a simple example to illustrate how decorators work:\n",
            "\n",
            "1. **Define a simple function:**\n",
            "\n",
            "```python\n",
            "def say_hello():\n",
            "    return \"Hello!\"\n",
            "```\n",
            "\n",
            "2. **Create a decorator:**\n",
            "\n",
            "```python\n",
            "def my_decorator(func):\n",
            "    def wrapper():\n",
            "        print(\"Something is happening before the function is called.\")\n",
            "        result = func()  # Call the original function\n",
            "        print(\"Something is happening after the function is called.\")\n",
            "        return result\n",
            "    return wrapper\n",
            "```\n",
            "\n",
            "3. **Apply the decorator:**\n",
            "\n",
            "You can apply the decorator to your function using the `@` symbol:\n",
            "\n",
            "```python\n",
            "@my_decorator\n",
            "def say_hello():\n",
            "    return \"Hello!\"\n",
            "```\n",
            "\n",
            "4. **Call the decorated function:**\n",
            "\n",
            "```python\n",
            "print(say_hello())\n",
            "```\n",
            "\n",
            "### What Happens When You Call `say_hello()`?\n",
            "\n",
            "When you call `say_hello()`, the following happens:\n",
            "\n",
            "1. The `my_decorator` function is called with `say_hello` as its argument.\n",
            "2. Inside `my_decorator`, the `wrapper` function is defined and returned.\n",
            "3. When you call `say_hello()`, you're actually calling the `wrapper` function.\n",
            "4. The `wrapper` function adds some behavior before and after calling the original `say_hello` function.\n",
            "\n",
            "### Output\n",
            "\n",
            "When you run the code, the output will be:\n",
            "\n",
            "```\n",
            "Something is happening before the function is called.\n",
            "Something is happening after the function is called.\n",
            "Hello!\n",
            "```\n",
            "\n",
            "### Summary\n",
            "\n",
            "- A decorator is a function that modifies another function.\n",
            "- It allows you to add functionality to existing functions in a clean and readable way.\n",
            "- You can use the `@decorator_name` syntax to apply a decorator to a function.\n",
            "\n",
            "Decorators are a powerful feature in Python and are widely used in frameworks like Flask and Django for things like routing and middleware.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a reusable prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert in {topic}. Answer in {style} style.\"),\n",
        "    (\"human\", \"{question}\"),\n",
        "])\n",
        "\n",
        "# Fill in the template\n",
        "formatted = prompt.invoke({\n",
        "    \"topic\": \"Python programming\",\n",
        "    \"style\": \"beginner-friendly\",\n",
        "    \"question\": \"What is a decorator?\",\n",
        "})\n",
        "\n",
        "print(\"Formatted messages:\")\n",
        "for msg in formatted.messages:\n",
        "    print(f\"  [{msg.type}]: {msg.content}\")\n",
        "\n",
        "print(\"\\n--- Model Response ---\")\n",
        "response = GPT_MODEL.invoke(formatted)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Chains with LCEL (LangChain Expression Language)\n",
        "\n",
        "LCEL lets you compose components using the **pipe operator** `|`. This is the modern way to build chains.\n",
        "\n",
        "```\n",
        "prompt | model | output_parser\n",
        "```\n",
        "\n",
        "Each component receives the output of the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT chain result:\n",
            " Imagine you're on a foggy mountain and you want to find the lowest point in the valley. You can't see far ahead, so you decide to feel the ground around you to see which direction slopes down the most. \n",
            "\n",
            "Each time you take a step in that direction, you check again to see if you're still going down. If you are, you keep stepping that way. If you find yourself going up, you adjust and try a different direction. \n",
            "\n",
            "This process of feeling around and taking steps downwards is like gradient descent in machine learning. The \"mountain\" represents the error or loss of a model, and the \"lowest point\" is where the model performs best. By repeatedly adjusting the model's parameters (like taking steps), you gradually minimize the error until you reach the best possible performance.\n",
            "\n",
            "============================================================\n",
            "\n",
            "Gemini chain result:\n",
            " Imagine you are a hiker caught in a **thick, heavy fog** at the top of a mountain. You want to get down to the village at the very bottom of the valley, but you canâ€™t see more than a few inches in front of your face.\n",
            "\n",
            "How do you get down safely? You use **Gradient Descent.**\n",
            "\n",
            "Here is how it works:\n",
            "\n",
            "### 1. Feel the Slope (The Gradient)\n",
            "Since you can't see the path, you use your feet to feel the ground around you. You shuffle your boots to find which direction slopes **downward** the steepest. In machine learning, this \"steepness\" is the **Gradient**. It tells the computer which way to move to reduce its mistakes.\n",
            "\n",
            "### 2. Take a Step (The Update)\n",
            "Once you know which way is down, you take a single step in that direction. By doing this, you are now slightly lower than you were a moment ago. In a computer model, this is like tweaking the settings (parameters) just a little bit to make the math more accurate.\n",
            "\n",
            "### 3. Decide How Big to Step (The Learning Rate)\n",
            "This is the most important part. \n",
            "*   **If you take giant leaps:** You might accidentally leap right over the valley and land on the side of another mountain. Youâ€™ll keep bouncing back and forth and never reach the bottom.\n",
            "*   **If you take tiny baby steps:** You will eventually reach the bottom, but it will take you three days and youâ€™ll run out of snacks. \n",
            "\n",
            "In machine learning, we call this step size the **Learning Rate**. Finding the \"just right\" step size is the secret to a good model.\n",
            "\n",
            "### 4. Repeat Until Flat (Convergence)\n",
            "You keep feeling the slope and taking steps. Eventually, you reach a spot where every direction you feel with your feet seems to go *up*. Congratulations! Youâ€™ve reached the bottom of the valley. \n",
            "\n",
            "### Summary for your \"Machine Learning\" brain:\n",
            "*   **The Mountain:** The \"Cost Function\" (a map of all the mistakes the model is making).\n",
            "*   **The Bottom:** The \"Minimum Error\" (the perfect version of your model).\n",
            "*   **The Foggy Hiker:** The \"Algorithm\" trying to learn.\n",
            "*   **The Slope:** The \"Gradient\" (the direction of the steepest descent).\n",
            "*   **The Step Size:** The \"Learning Rate.\"\n",
            "\n",
            "**Gradient Descent is simply the process of \"stepping downhill\" until the mistakes are as small as possible.**\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Build a simple chain: prompt -> model -> string output\n",
        "chain_gpt = prompt | GPT_MODEL | StrOutputParser()\n",
        "chain_gemini = prompt | GEMINI_MODEL | StrOutputParser()\n",
        "\n",
        "# Invoke the chain with GPT\n",
        "result = chain_gpt.invoke({\n",
        "    \"topic\": \"machine learning\",\n",
        "    \"style\": \"simple analogy\",\n",
        "    \"question\": \"What is gradient descent?\",\n",
        "})\n",
        "print(\"GPT chain result:\\n\", result)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Same chain with Gemini\n",
        "result = chain_gemini.invoke({\n",
        "    \"topic\": \"machine learning\",\n",
        "    \"style\": \"simple analogy\",\n",
        "    \"question\": \"What is gradient descent?\",\n",
        "})\n",
        "print(\"Gemini chain result:\\n\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Tools and Basic Agents\n",
        "\n",
        "**Tools** are functions that an AI model can call. An **agent** decides which tools to use and when.\n",
        "\n",
        "LangGraph provides `create_react_agent` - the simplest way to build a tool-using agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined 3 tools:\n",
            "  - add: Add two numbers together.\n",
            "  - multiply: Multiply two numbers together.\n",
            "  - get_weather: Get the current weather for a city (mock data).\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# Define simple tools\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers together.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers together.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the current weather for a city (mock data).\"\"\"\n",
        "    weather_data = {\n",
        "        \"new york\": \"72F, Sunny\",\n",
        "        \"london\": \"58F, Cloudy\",\n",
        "        \"tokyo\": \"80F, Humid\",\n",
        "    }\n",
        "    return weather_data.get(city.lower(), f\"Weather data not available for {city}\")\n",
        "\n",
        "tools = [add, multiply, get_weather]\n",
        "print(f\"Defined {len(tools)} tools:\")\n",
        "for t in tools:\n",
        "    print(f\"  - {t.name}: {t.description}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content: \n",
            "Tool calls: [{'name': 'multiply', 'args': {'a': 15, 'b': 23}, 'id': 'call_utZGjjXrZSm3WDuO5iKsnNKr', 'type': 'tool_call'}]\n"
          ]
        }
      ],
      "source": [
        "# Bind tools to a model and see tool calls\n",
        "model_with_tools = GPT_MODEL.bind_tools(tools)\n",
        "\n",
        "response = model_with_tools.invoke(\"What is 15 multiplied by 23?\")\n",
        "print(\"Content:\", response.content)\n",
        "print(\"Tool calls:\", response.tool_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Yunpeng.Cheng\\AppData\\Local\\Temp\\ipykernel_28908\\2815162728.py:5: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(GPT_MODEL, tools)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human] What is 15 + 23, and then multiply the result by 2?\n",
            "[ai] \n",
            "  -> Tool call: add({'a': 15, 'b': 23})\n",
            "  -> Tool call: multiply({'a': 15, 'b': 2})\n",
            "[tool] 38\n",
            "[tool] 30\n",
            "[ai] \n",
            "  -> Tool call: multiply({'a': 38, 'b': 2})\n",
            "[tool] 76\n",
            "[ai] The result of \\( 15 + 23 \\) is \\( 38 \\). When you multiply that by \\( 2 \\), you get \\( 76 \\).\n"
          ]
        }
      ],
      "source": [
        "# Build a simple ReAct agent using LangGraph's prebuilt helper\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Create an agent with GPT and our tools\n",
        "agent = create_react_agent(GPT_MODEL, tools)\n",
        "\n",
        "# Run the agent\n",
        "result = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 15 + 23, and then multiply the result by 2?\"}]}\n",
        ")\n",
        "\n",
        "# Print the conversation\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, 'type') else 'unknown'\n",
        "    content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "    print(f\"[{role}] {content[:200]}\")\n",
        "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"  -> Tool call: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Yunpeng.Cheng\\AppData\\Local\\Temp\\ipykernel_28908\\2053153068.py:2: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  gemini_agent = create_react_agent(GEMINI_MODEL, tools)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human] What's the weather in Tokyo? Also add 100 and 250.\n",
            "[ai] []\n",
            "  -> Tool call: get_weather({'city': 'Tokyo'})\n",
            "  -> Tool call: add({'a': 100, 'b': 250})\n",
            "[tool] 80F, Humid\n",
            "[tool] 350\n",
            "[ai] [{'type': 'text', 'text': 'The weather in Tokyo is currently 80Â°F and humid. Also, 100 plus 250 is 350.'}]\n"
          ]
        }
      ],
      "source": [
        "# Same agent but with Gemini\n",
        "gemini_agent = create_react_agent(GEMINI_MODEL, tools)\n",
        "\n",
        "result = gemini_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo? Also add 100 and 250.\"}]}\n",
        ")\n",
        "\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, 'type') else 'unknown'\n",
        "    content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "    print(f\"[{role}] {content[:200]}\")\n",
        "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"  -> Tool call: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Streaming Responses\n",
        "\n",
        "Instead of waiting for the full response, you can **stream** tokens as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming from GPT-4o-mini:\n",
            "Lines of code align,  \n",
            "Logic dances in the dark,  \n",
            "Dreams in syntax bloom.\n",
            "\n",
            "Streaming from Gemini Flash:\n",
            "[{'type': 'text', 'text': 'Logic in the lines,\\nSearching for a missing mark,\\nNow the program runs.', 'index': 0}][{'type': 'text', 'text': '', 'extras': {'signature': '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'}, 'index': 0}][]\n"
          ]
        }
      ],
      "source": [
        "# Stream output token by token\n",
        "print(\"Streaming from GPT-4o-mini:\")\n",
        "for chunk in GPT_MODEL.stream(\"Write a haiku about programming.\"):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Streaming from Gemini Flash:\")\n",
        "for chunk in GEMINI_MODEL.stream(\"Write a haiku about programming.\"):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Structured Output\n",
        "\n",
        "Force models to return data in a specific structure using Pydantic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Inception\n",
            "Rating: 9/10\n",
            "Summary: A mind-bending thriller that explores the complexities of dreams and reality through a heist narrative.\n",
            "Recommended: Yes\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class MovieReview(BaseModel):\n",
        "    \"\"\"A structured movie review.\"\"\"\n",
        "    title: str = Field(description=\"The movie title\")\n",
        "    rating: int = Field(description=\"Rating from 1-10\")\n",
        "    summary: str = Field(description=\"One-sentence summary\")\n",
        "    recommended: bool = Field(description=\"Whether you'd recommend it\")\n",
        "\n",
        "# Get structured output from GPT\n",
        "structured_model = GPT_MODEL.with_structured_output(MovieReview)\n",
        "review = structured_model.invoke(\"Review the movie 'Inception'\")\n",
        "\n",
        "print(f\"Title: {review.title}\")\n",
        "print(f\"Rating: {review.rating}/10\")\n",
        "print(f\"Summary: {review.summary}\")\n",
        "print(f\"Recommended: {'Yes' if review.recommended else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| Chat Models | How to invoke GPT-4o-mini and Gemini Flash |\n",
        "| Messages | SystemMessage, HumanMessage, AIMessage |\n",
        "| Prompt Templates | Reusable, parameterized prompts |\n",
        "| LCEL Chains | Composing components with the pipe operator |\n",
        "| Tools | Defining functions that models can call |\n",
        "| Agents | Using `create_react_agent` for tool-using agents |\n",
        "| Streaming | Token-by-token output |\n",
        "| Structured Output | Forcing models to return Pydantic objects |\n",
        "\n",
        "**Next:** [02 - Intermediate: LCEL Deep Dive & LangGraph Introduction](./02_intermediate_langchain_langgraph.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
