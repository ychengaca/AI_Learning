{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 3: Advanced - LangGraph Deep Dive\n",
        "\n",
        "This notebook covers advanced LangGraph concepts for building production-ready agent systems.\n",
        "\n",
        "## Learning Objectives\n",
        "- Persistence and checkpointing (memory across sessions)\n",
        "- Human-in-the-loop with interrupts\n",
        "- Streaming events and tokens from graphs\n",
        "- Subgraphs for modular agent architectures\n",
        "- Dynamic state management and reducers\n",
        "- Error handling and retry strategies\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Notebooks 01 and 02\n",
        "\n",
        "---\n",
        "\n",
        "**References:**\n",
        "- [LangGraph Persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
        "- [LangGraph Human-in-the-Loop](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/)\n",
        "- [LangGraph Streaming](https://langchain-ai.github.io/langgraph/how-tos/streaming/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client initialized  -> model: gpt-4o-mini\n",
            "Google client initialized  -> model: gemini-3-flash-preview\n",
            "Using GPT model:    gpt-4o-mini\n",
            "Using Gemini model: gemini-3-flash-preview\n",
            "\n",
            "Available Models:\n",
            "-------------------------------------------------------\n",
            "  gpt-4o-mini          -> ChatOpenAI(gpt-4o-mini)\n",
            "  gemini-3-flash-preview -> ChatGoogleGenerativeAI(gemini-3-flash-preview)\n",
            "-------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Add parent directory to path for shared config\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import global model configuration\n",
        "from config import (\n",
        "    GPT_MODEL, GEMINI_MODEL,\n",
        "    GPT_MODEL_NAME, GEMINI_MODEL_NAME,\n",
        "    get_model, list_available_models,\n",
        ")\n",
        "\n",
        "print(f\"Using GPT model:    {GPT_MODEL_NAME}\")\n",
        "print(f\"Using Gemini model: {GEMINI_MODEL_NAME}\")\n",
        "print()\n",
        "list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Persistence and Checkpointing\n",
        "\n",
        "Persistence lets your graph **remember** state across invocations. This is essential for:\n",
        "- Multi-turn conversations\n",
        "- Long-running workflows\n",
        "- Crash recovery\n",
        "\n",
        "LangGraph uses **checkpointers** to save and restore graph state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Turn 1: Hi Alice! It's great to meet you. Hiking is such a wonderful way to connect with nature and stay active. Do you have any favorite trails or hiking spots?\n",
            "\n",
            "Turn 2: Your name is Alice, and you love hiking!\n"
          ]
        }
      ],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Define state with message history\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Build a simple chatbot graph with memory\n",
        "def chatbot_node(state: ChatState):\n",
        "    return {\"messages\": [GPT_MODEL.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder = StateGraph(ChatState)\n",
        "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Compile with memory checkpointer\n",
        "memory = MemorySaver()\n",
        "chatbot_with_memory = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "# First conversation turn\n",
        "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
        "result = chatbot_with_memory.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is Alice. I love hiking.\"}]},\n",
        "    config=config,\n",
        ")\n",
        "print(\"Turn 1:\", result[\"messages\"][-1].content)\n",
        "\n",
        "# Second turn - the bot should remember the name\n",
        "result = chatbot_with_memory.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is my name and what do I like?\"}]},\n",
        "    config=config,\n",
        ")\n",
        "print(\"\\nTurn 2:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thread 2 (no prior context): I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?\n",
            "\n",
            "Thread 1 (has context): Your name is Alice!\n"
          ]
        }
      ],
      "source": [
        "# Verify persistence: use a different thread to show isolation\n",
        "config2 = {\"configurable\": {\"thread_id\": \"demo-thread-2\"}}\n",
        "result = chatbot_with_memory.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]},\n",
        "    config=config2,\n",
        ")\n",
        "print(\"Thread 2 (no prior context):\", result[\"messages\"][-1].content)\n",
        "\n",
        "# Go back to thread 1 - memory should be intact\n",
        "result = chatbot_with_memory.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Can you remind me what my name is?\"}]},\n",
        "    config=config,\n",
        ")\n",
        "print(\"\\nThread 1 (has context):\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Human-in-the-Loop with Interrupts\n",
        "\n",
        "Interrupts allow you to **pause** a graph at any node, inspect the state, optionally modify it, and then **resume** execution. This is critical for:\n",
        "- Approval workflows\n",
        "- Content moderation\n",
        "- Manual data correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Yunpeng.Cheng\\AppData\\Local\\Temp\\ipykernel_25260\\3528947716.py:29: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  research_agent = create_react_agent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent paused. Current state:\n",
            "  Next node: ('tools',)\n",
            "  Pending tool call: search_web({'query': 'LangGraph'})\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Define tools for a research assistant\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"Search the web for information (mock).\"\"\"\n",
        "    mock_results = {\n",
        "        \"langchain\": \"LangChain is a framework for building LLM applications.\",\n",
        "        \"langgraph\": \"LangGraph provides graph-based workflows for agents.\",\n",
        "        \"python\": \"Python is a versatile programming language.\",\n",
        "    }\n",
        "    for key, val in mock_results.items():\n",
        "        if key in query.lower():\n",
        "            return val\n",
        "    return f\"No results found for: {query}\"\n",
        "\n",
        "@tool\n",
        "def save_note(note: str) -> str:\n",
        "    \"\"\"Save a research note for later reference.\"\"\"\n",
        "    print(f\"  [NOTE SAVED]: {note}\")\n",
        "    return f\"Note saved: {note}\"\n",
        "\n",
        "# Create agent with interrupt_before on save_note (requires approval)\n",
        "research_tools = [search_web, save_note]\n",
        "memory = MemorySaver()\n",
        "\n",
        "research_agent = create_react_agent(\n",
        "    GPT_MODEL,\n",
        "    research_tools,\n",
        "    checkpointer=memory,\n",
        "    interrupt_before=[\"tools\"],  # Pause before any tool execution\n",
        ")\n",
        "\n",
        "# Start the agent\n",
        "config = {\"configurable\": {\"thread_id\": \"research-1\"}}\n",
        "result = research_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Search for information about LangGraph and save a summary note.\"}]},\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "# The agent is now paused before tool execution\n",
        "print(\"Agent paused. Current state:\")\n",
        "snapshot = research_agent.get_state(config)\n",
        "print(f\"  Next node: {snapshot.next}\")\n",
        "if snapshot.values.get(\"messages\"):\n",
        "    last_msg = snapshot.values[\"messages\"][-1]\n",
        "    if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
        "        for tc in last_msg.tool_calls:\n",
        "            print(f\"  Pending tool call: {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approving tool execution and resuming...\n",
            "\n",
            "[human] Search for information about LangGraph and save a summary note.\n",
            "[tool] LangGraph provides graph-based workflows for agents.\n"
          ]
        }
      ],
      "source": [
        "# Resume execution: approve the pending tool calls and let the agent continue\n",
        "# In production, you'd show the pending tool call to a human for approval\n",
        "print(\"Approving tool execution and resuming...\\n\")\n",
        "\n",
        "# Resume by invoking with None (which continues from checkpoint)\n",
        "result = research_agent.invoke(None, config=config)\n",
        "\n",
        "# Print the results\n",
        "for msg in result[\"messages\"]:\n",
        "    role = msg.type if hasattr(msg, 'type') else 'unknown'\n",
        "    content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "    if content:\n",
        "        print(f\"[{role}] {content[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Streaming Events from Graphs\n",
        "\n",
        "Streaming lets you observe the graph's execution in real-time, seeing each node's output as it happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming graph events:\n",
            "\n",
            "--- Node: agent ---\n",
            "  Tool call: lookup_info({'topic': 'Python programming language'})\n",
            "  Tool call: lookup_info({'topic': 'Rust programming language'})\n",
            "\n",
            "--- Node: tools ---\n",
            "  tool: No info on Python programming language\n",
            "  tool: No info on Rust programming language\n",
            "\n",
            "--- Node: agent ---\n",
            "  ai: It seems that I couldn't retrieve specific information about Python and Rust at the moment. However, I can provide a brief overview based on my knowle\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def lookup_info(topic: str) -> str:\n",
        "    \"\"\"Look up information about a topic.\"\"\"\n",
        "    data = {\n",
        "        \"python\": \"Python is a high-level programming language known for readability.\",\n",
        "        \"javascript\": \"JavaScript is a dynamic language used for web development.\",\n",
        "        \"rust\": \"Rust is a systems language focused on safety and performance.\",\n",
        "    }\n",
        "    return data.get(topic.lower(), f\"No info on {topic}\")\n",
        "\n",
        "tools = [lookup_info]\n",
        "\n",
        "# Build a simple tool-calling agent for streaming demo\n",
        "class StreamState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "model_with_tools = GPT_MODEL.bind_tools(tools)\n",
        "\n",
        "def agent(state):\n",
        "    return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "def execute_tools(state):\n",
        "    from langchain_core.messages import ToolMessage\n",
        "    outputs = []\n",
        "    for tc in state[\"messages\"][-1].tool_calls:\n",
        "        result = {t.name: t for t in tools}[tc[\"name\"]].invoke(tc[\"args\"])\n",
        "        outputs.append(ToolMessage(content=str(result), tool_call_id=tc[\"id\"], name=tc[\"name\"]))\n",
        "    return {\"messages\": outputs}\n",
        "\n",
        "def route(state):\n",
        "    last = state[\"messages\"][-1]\n",
        "    if hasattr(last, \"tool_calls\") and last.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "g = StateGraph(StreamState)\n",
        "g.add_node(\"agent\", agent)\n",
        "g.add_node(\"tools\", execute_tools)\n",
        "g.add_edge(START, \"agent\")\n",
        "g.add_conditional_edges(\"agent\", route, {\"tools\": \"tools\", END: END})\n",
        "g.add_edge(\"tools\", \"agent\")\n",
        "stream_graph = g.compile()\n",
        "\n",
        "# Stream events from the graph\n",
        "print(\"Streaming graph events:\\n\")\n",
        "for event in stream_graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about Python and Rust.\"}]},\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    for node_name, node_output in event.items():\n",
        "        print(f\"--- Node: {node_name} ---\")\n",
        "        if \"messages\" in node_output:\n",
        "            for msg in node_output[\"messages\"]:\n",
        "                content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
        "                if content:\n",
        "                    print(f\"  {msg.type}: {content[:150]}\")\n",
        "                if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "                    for tc in msg.tool_calls:\n",
        "                        print(f\"  Tool call: {tc['name']}({tc['args']})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Subgraphs for Modular Architectures\n",
        "\n",
        "Subgraphs let you compose smaller graphs into larger ones, promoting reuse and modularity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "LangChain and LangGraph are both frameworks designed to facilitate the development of applications that utilize language models, but they serve different purposes and have distinct features.\n",
            "\n",
            "### LangChain\n",
            "- **Purpose**: LangChain is primarily focused on building applications that leverage large lan\n",
            "\n",
            "Summary:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Build a \"summarizer\" subgraph\n",
        "class SummarizerState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "def summarize_node(state: SummarizerState):\n",
        "    msgs = [SystemMessage(content=\"Summarize the conversation so far in 2 sentences.\")] + state[\"messages\"]\n",
        "    return {\"messages\": [GEMINI_MODEL.invoke(msgs)]}\n",
        "\n",
        "summarizer = StateGraph(SummarizerState)\n",
        "summarizer.add_node(\"summarize\", summarize_node)\n",
        "summarizer.add_edge(START, \"summarize\")\n",
        "summarizer.add_edge(\"summarize\", END)\n",
        "summarizer_graph = summarizer.compile()\n",
        "\n",
        "# Build a \"responder\" subgraph\n",
        "def responder_node(state: SummarizerState):\n",
        "    msgs = [SystemMessage(content=\"You are a helpful assistant. Respond to the user.\")] + state[\"messages\"]\n",
        "    return {\"messages\": [GPT_MODEL.invoke(msgs)]}\n",
        "\n",
        "responder = StateGraph(SummarizerState)\n",
        "responder.add_node(\"respond\", responder_node)\n",
        "responder.add_edge(START, \"respond\")\n",
        "responder.add_edge(\"respond\", END)\n",
        "responder_graph = responder.compile()\n",
        "\n",
        "# Build the parent graph that uses both subgraphs\n",
        "class ParentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "parent = StateGraph(ParentState)\n",
        "parent.add_node(\"responder\", responder_graph)\n",
        "parent.add_node(\"summarizer\", summarizer_graph)\n",
        "\n",
        "parent.add_edge(START, \"responder\")\n",
        "parent.add_edge(\"responder\", \"summarizer\")\n",
        "parent.add_edge(\"summarizer\", END)\n",
        "\n",
        "parent_graph = parent.compile()\n",
        "\n",
        "result = parent_graph.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain the difference between LangChain and LangGraph.\"}]\n",
        "})\n",
        "\n",
        "print(\"Response:\")\n",
        "print(result[\"messages\"][-2].content[:300])\n",
        "print(\"\\nSummary:\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Custom State Reducers\n",
        "\n",
        "Reducers control **how** state updates are merged. The default is to replace; `add_messages` appends. You can write custom reducers for any behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Scores: [{'judge': 'GPT', 'score': 8}, {'judge': 'Gemini', 'score': 5}]\n",
            "Total: 13\n",
            "Average: 6.5\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "from typing import Annotated\n",
        "\n",
        "# Helper to extract content from model response (handles both OpenAI and Gemini formats)\n",
        "def get_response_content(response) -> str:\n",
        "    \"\"\"Extract text content from model response, handling different formats.\"\"\"\n",
        "    if hasattr(response, 'content'):\n",
        "        content = response.content\n",
        "        # Gemini sometimes returns content as a list\n",
        "        if isinstance(content, list):\n",
        "            return str(content[0]) if content else \"\"\n",
        "        return str(content)\n",
        "    return str(response)\n",
        "\n",
        "# Custom reducer: accumulate a running total\n",
        "def add_to_total(current: int, update: int) -> int:\n",
        "    \"\"\"Custom reducer that adds values.\"\"\"\n",
        "    return current + update\n",
        "\n",
        "class ScoreState(TypedDict):\n",
        "    task: str\n",
        "    scores: Annotated[list, operator.add]  # List concatenation reducer\n",
        "    total: Annotated[int, add_to_total]    # Custom sum reducer\n",
        "\n",
        "def judge_a(state: ScoreState):\n",
        "    \"\"\"Judge A scores the task.\"\"\"\n",
        "    response = GPT_MODEL.invoke(f\"Rate this task on a scale of 1-10 (respond with just the number): {state['task']}\")\n",
        "    content = get_response_content(response)\n",
        "    try:\n",
        "        score = int(content.strip())\n",
        "    except ValueError:\n",
        "        score = 5\n",
        "    return {\"scores\": [{\"judge\": \"GPT\", \"score\": score}], \"total\": score}\n",
        "\n",
        "def judge_b(state: ScoreState):\n",
        "    \"\"\"Judge B scores the task.\"\"\"\n",
        "    response = GEMINI_MODEL.invoke(f\"Rate this task on a scale of 1-10 (respond with just the number): {state['task']}\")\n",
        "    content = get_response_content(response)\n",
        "    try:\n",
        "        score = int(content.strip())\n",
        "    except ValueError:\n",
        "        score = 5\n",
        "    return {\"scores\": [{\"judge\": \"Gemini\", \"score\": score}], \"total\": score}\n",
        "\n",
        "def final_verdict(state: ScoreState):\n",
        "    \"\"\"Compute average score.\"\"\"\n",
        "    avg = state[\"total\"] / len(state[\"scores\"]) if state[\"scores\"] else 0\n",
        "    print(f\"\\nScores: {state['scores']}\")\n",
        "    print(f\"Total: {state['total']}\")\n",
        "    print(f\"Average: {avg:.1f}\")\n",
        "    return {}\n",
        "\n",
        "scoring_graph = StateGraph(ScoreState)\n",
        "scoring_graph.add_node(\"judge_a\", judge_a)\n",
        "scoring_graph.add_node(\"judge_b\", judge_b)\n",
        "scoring_graph.add_node(\"verdict\", final_verdict)\n",
        "\n",
        "scoring_graph.add_edge(START, \"judge_a\")\n",
        "scoring_graph.add_edge(\"judge_a\", \"judge_b\")\n",
        "scoring_graph.add_edge(\"judge_b\", \"verdict\")\n",
        "scoring_graph.add_edge(\"verdict\", END)\n",
        "\n",
        "scoring_compiled = scoring_graph.compile()\n",
        "\n",
        "result = scoring_compiled.invoke({\n",
        "    \"task\": \"Write a Python function to reverse a string\",\n",
        "    \"scores\": [],\n",
        "    \"total\": 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Error Handling in Graphs\n",
        "\n",
        "Robust agents need to handle failures gracefully. Here's a pattern for error handling within graph nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: 2 + 2 equals 4.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "class ErrorHandlingState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    error: str\n",
        "    retry_count: int\n",
        "\n",
        "def safe_llm_call(state: ErrorHandlingState):\n",
        "    \"\"\"LLM call with error handling.\"\"\"\n",
        "    try:\n",
        "        response = GPT_MODEL.invoke(state[\"messages\"])\n",
        "        return {\"messages\": [response], \"error\": \"\"}\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"retry_count\": state.get(\"retry_count\", 0) + 1,\n",
        "        }\n",
        "\n",
        "def check_error(state: ErrorHandlingState) -> str:\n",
        "    \"\"\"Route based on whether there was an error.\"\"\"\n",
        "    if state.get(\"error\") and state.get(\"retry_count\", 0) < 3:\n",
        "        return \"retry\"\n",
        "    elif state.get(\"error\"):\n",
        "        return \"fallback\"\n",
        "    return END\n",
        "\n",
        "def fallback_node(state: ErrorHandlingState):\n",
        "    \"\"\"Use Gemini as fallback.\"\"\"\n",
        "    try:\n",
        "        response = GEMINI_MODEL.invoke(state[\"messages\"])\n",
        "        return {\"messages\": [response], \"error\": \"\"}\n",
        "    except Exception as e:\n",
        "        return {\"messages\": [HumanMessage(content=f\"All models failed: {e}\")]}\n",
        "\n",
        "error_graph = StateGraph(ErrorHandlingState)\n",
        "error_graph.add_node(\"llm\", safe_llm_call)\n",
        "error_graph.add_node(\"fallback\", fallback_node)\n",
        "\n",
        "error_graph.add_edge(START, \"llm\")\n",
        "error_graph.add_conditional_edges(\"llm\", check_error, {\n",
        "    \"retry\": \"llm\",\n",
        "    \"fallback\": \"fallback\",\n",
        "    END: END,\n",
        "})\n",
        "error_graph.add_edge(\"fallback\", END)\n",
        "\n",
        "error_compiled = error_graph.compile()\n",
        "\n",
        "# Test normal operation\n",
        "result = error_compiled.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n",
        "    \"error\": \"\",\n",
        "    \"retry_count\": 0,\n",
        "})\n",
        "print(\"Result:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| Persistence | MemorySaver for multi-turn conversation memory |\n",
        "| Thread Isolation | Separate memory per thread_id |\n",
        "| Human-in-the-Loop | interrupt_before to pause and resume graphs |\n",
        "| Streaming | stream_mode=\"updates\" for real-time events |\n",
        "| Subgraphs | Compose smaller graphs into larger workflows |\n",
        "| Custom Reducers | Control how state updates are merged |\n",
        "| Error Handling | Retry logic and model fallbacks in graphs |\n",
        "\n",
        "**Next:** [04 - Specialty: RAG, Multi-Agent & Production](./04_specialty_rag_and_multi_agent.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
